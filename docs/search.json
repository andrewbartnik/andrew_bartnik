[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andrew Bartnik",
    "section": "",
    "text": "Welcome!EducationHighlighted ProjectsExperience\n\n\nI’m an aspiring Environmental Data Scientist/Machine Learning Scientist recently graduated from the Masters of Environmental Data Science (MEDS) program at University of California, Santa Barbara’s Bren School of the Environmental Science & Management in June 2023. I am broadly interested in all topics across the natural world, including evolutionary biology, astrobiology, planetary science and oceanography, and biosphere and Earth interactions. I am passionate (and optimistic) about the application of machine learning algorithms and artificial intelligence to understand complex natural systems. I am comfortable in using R and Python programming languages and various other data science tools. I will be posting each of my data science projects here as I complete them. Thanks for stopping by!\n\n\n\nGraduate\nUniversity of California, Santa Barbara | Santa Barbara, CA (June 2023)\nMaster of Environmental Data Science\n\n\nUndergraduate\nUniversity of Arkansas | Fayetteville, AR (2015 - 2020)\nBachelor of Arts in Biology | Minor in Mathematics\nBachelor of Arts in Chemistry | Minor in Physics\nBachelor of Science and Arts in Environmental Science\n\n\n\n\n\nMachine Learning Engineer | MEDS Capstone Project (12/22-06/23)\nMeasuring Agricultural Adaptation to Climate Change in Zambia Using Satellite Imagery and Machine Learning\nVisiting Scholar | Blue Marble Space Institute of Science (2021 - Present) | Remote\nYSP Intern | Blue Marble Space Institute of Science (2021) | Remote\nResearch Associate | Kumar Lab (2020) | University of Arkansas, Fayetteville\nResearch Associate | Paul Lab (2018) | University of Arkansas, Fayetteville"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Andrew Bartnik",
    "section": "Education",
    "text": "Education\nMaster of Environmental Data Science (2022 - Present)\nUniversity of California, Santa Barbara | Santa Barbara, California\nBachelor of Arts in Biology | (2015 - 2020) - University of Arkansas | Fayetteville\nBachelor of Arts in Chemistry | (2015 - 2020) - University of Arkansas | Fayetteville\nBachelor of Science and Arts in Environmental Science | (2015 - 2020) - University of Arkansas | Fayetteville\nMinor in Mathematics | (2015 - 2020) - University of Arkansas | Fayetteville\nMinor in Physics | (2015 - 2020) - University of Arkansas | Fayetteville"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Andrew Bartnik",
    "section": "Experience",
    "text": "Experience\nVisiting Scholar | Blue Marble Space Institute of Science (2021 - Present) | Remote\nYSP Intern | Blue Marble Space Institute of Science (2021) | Remote\nResearch Associate | Kumar Lab (2020) | University of Arkansas, Fayetteville\nResearch Associate | Paul Lab (2018) | University of Arkansas, Fayetteville"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "BioPlaces Traveled\n\n\nI grew up in Melbourne, Australia and was exposed to the wonders of the natural world from a very young age. I quickly learned that understanding the natural world would be what I wanted to do for my career. My family moved to Dallas, Texas in 2008, where I would finish highschool as a remarkably average student. I attended the University of Arkansas for my undergraduate career, where the freedom to learn what I was interested in allowed me to explore my curiosities unbounded. I graduated with degrees in chemistry, biology, and environmental science along with minors in physics and math. I wanted a strong foundation in the hard sciences so I could eventually build up to study the Earth as a system.\nI graduated in May 2020, right as the coronavirus pandemic took off. I admittedly didn’t what my next step was after my undergraduate career, and worked as a bartender for about a year and a half while doing research with Dr. Jihua Hao from the Blue Marble Space Institute of Science (BMSIS). We worked on modeling the thermodynamics of amino acid synthesis on Saturn’s moon Enceladus - in an attempt to evaluate its potential for abiogenesis in its liquid water ocean beneath its icy crust.\nI realized that I wanted a broader set of tools to be able to analyze data from the natural world. I decided that data science would be the perfect place to leverage the power of data and statistics to any problem I’m interested in, so I decided to attend UCSB’s Master of Environmental Data Science program.\nI am broadly interested in many topics across the natural world. I enjoy learning about evolutionary biology, earth systems and planetary science, cosmology, climate science, and astrobiology"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Quarto\n\n\nR\n\n\nMEDS\n\n\n\nHere’s a short description of my first post\n\n\n\nAndrew Bartnik\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-10-24-blog-post/index.html",
    "href": "posts/2022-10-24-blog-post/index.html",
    "title": "My First Post",
    "section": "",
    "text": "CitationBibTeX citation:@online{bartnik2022,\n  author = {Andrew Bartnik},\n  title = {My {First} {Post}},\n  date = {10/24/2022},\n  url = {https://andrewbartnik.github.io/andrew_bartnik/posts/2022-10-24-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndrew Bartnik. 10AD–24AD. “My First Post.” 10AD–24AD. https://andrewbartnik.github.io/andrew_bartnik/posts/2022-10-24-blog-post/."
  },
  {
    "objectID": "Portfolio.html",
    "href": "Portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Texas Vehicle Growth and Air Quality\n\n\n\n\n\ntbd\n\n\n\nAndrew Bartnik\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Climbing Incident Outcomes from Article Text\n\n\n\nNLP\n\n\nR\n\n\nAssignments\n\n\nML\n\n\n\nI used NLP techniques and different ML models to predict the outcome of climbing accidents. This assignment was part of EDS 231 - Text and Sentiment Analysis for…\n\n\n\nAndrew Bartnik\n\n\nMay 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Climbing Incident Outcomes from Article Text\n\n\n\nNLP\n\n\nR\n\n\nAssignments\n\n\nML\n\n\n\nI used NLP techniques and different ML models to predict the outcome of climbing accidents. This assignment was part of EDS 231 - Text and Sentiment Analysis for…\n\n\n\nAndrew Bartnik\n\n\nMay 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating Biodiversity Sentiment, Performing Topic Analysis, and Analyzing Word Embeddings\n\n\n\nNLP\n\n\nR\n\n\nAssignments\n\n\nML\n\n\n\nI used the Nexis Uni database to analyze the sentiment associated with Biodiversity. This assignment was part of EDS 231 - Text and Sentiment Analysis for Environmental…\n\n\n\nAndrew Bartnik\n\n\nApr 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting dissolved inorganic carbon (DIC) concentrations using different algorithms in Python and R\n\n\n\nR\n\n\nPython\n\n\nML\n\n\nProjects\n\n\n\nThis project uses ocean chemistry data collected by CalCOFI to predict DIC using Linear Regression, KNN, Decision Trees, Random Forest Regressor, Gradient Boosting, and a…\n\n\n\nAndrew Bartnik\n\n\nMar 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Extinction\n\n\n\nProjects\n\n\nR\n\n\nStatistics\n\n\n\nI used IUCN data to explore extinction in Animalia. This project is part of Statistics for Environmental Data Science - EDS 222\n\n\n\nAndrew Bartnik\n\n\nDec 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing the Effects of the Texas February 2021 Storm on the Houston metropolitan Area\n\n\n\nSpatial\n\n\nR\n\n\nAssignments\n\n\n\nThis post is based on an EDS 223 - Spatial Analysis assignment, as part of the Masters of Environmental Data Science (MEDS) curriculum at the University of California, Santa…\n\n\n\nAndrew Bartnik\n\n\nNov 15, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Portfolio/stats_final/index.html",
    "href": "Portfolio/stats_final/index.html",
    "title": "Exploring Extinction",
    "section": "",
    "text": "Species extinction is a normal part of Earth’s natural history, as over 99.9% of species to ever have existed are now extinct1. Background extinction rates under normal conditions is expected to between 0.1 - 1 species of vertebrates to go extinct every 100 years per 10,000 species2. However, throughout Earth’s history there have been calamitous events such as asteroid impacts, volcanic eruptions, and sudden atmospheric changes that have rapidly changed the conditions on Earth to be unsuitable for life. The worst of these extinction events, the Permian extinction, is thought to have killed off 90% of all species on Earth.\nThe current species extinction rate is estimated to be between 1,000 and 10,000 times higher than the normal background extinction rate3, which is enough to consider our current time period the 6th mass extinction event - widely agreed to be caused by various human activities.\n\nFigure 1. The Big Five Mass Extinctions\n\n\n\n\nChinese River DolphinTasmanian TigerDodo\n\n\nYou may be familiar with some of the species that have recently gone extinct due to human activity. Among these are the Chinese River dolphin, which lived in the Yangtze river of China and was last seen in 2007 - thought to be driven to extinction due to heavy use of the river for fishing, transportation and hydroelectricity,\n\nFigure 2. The Chinese River Dolphin (Baiji)\n\n\nThe Tasmanian Tiger which lived on Tasmania, a small island south east of Australia, and was hunted to extinction in the 1930s\n\nFigure 3. Tasmanian Tiger (Thylacine)\n\n\nAnd the famous Dodo bird, endemic to the island of Mauritius, it lacked adaptations to prevent its own extinction from hunting by sailors and habitat destruction in the late 17th century.\n\nFigure 4. The Dodo went extinct by 1681\n\n\n\n\n\n\nSpecies provide us not only with important sources of medicine, food and various other products, but also play important roles in each of their respective ecosystems on which much of our societies’ depend. Each species also helps us elucidate the story of life’s history on Earth and contextualizes our relationship with the natural world. More importantly however, species have intrinsic value regardless of what they provide for humans, and each one lost is a tragedy in its own right.\nIts important to understand the factors that render species vulnerable to extinction, as well as what the mechanisms of extinction are and how they work. Extinction is notoriously difficult to study mainly due to our lack of data which I will expand upon in the issues section, but we can hopefully use some of these findings to identify vulnerable species, and better protect them and their ecosystems from extinction and collapse."
  },
  {
    "objectID": "Portfolio/stats_final/index.html#cleaning",
    "href": "Portfolio/stats_final/index.html#cleaning",
    "title": "Exploring Extinction",
    "section": "Cleaning",
    "text": "Cleaning\nAfter cleaning the data, each row represented one species with a unique assessment_id, and each column contained a variable that I thought might influence extinction. The variables that I focused on were: species endemism (endemic), habitat type (habitat), the type of threat faced (threat), human use (use), and taxonomy (class).\n\n\nCode\nhead(predictors) |> \n  datatable()\n\n\n\n\n\n\n\nTo clarify some nomenclature before we start modeling:\nWhen I use the term variable, I’m referencing the column names that we’re going to use in our analysis. When I use the term level, I’m referring to the different values that each column name can take on. For example, one of our variables is habitat. The different levels it can take on are below.\n\n\nCode\nlevels(predictors$habitat) \n\n\n[1] \"Caves\"          \"Desert\"         \"Forest\"         \"Grassland\"     \n[5] \"Marine Neritic\" \"Rocky Areas\"    \"Savanna\"        \"Shrubland\"     \n[9] \"Wetlands\""
  },
  {
    "objectID": "Portfolio/stats_final/index.html#modeling",
    "href": "Portfolio/stats_final/index.html#modeling",
    "title": "Exploring Extinction",
    "section": "Modeling",
    "text": "Modeling\nI ran a logistic linear regression on each of these variables individually to get a feel for which levels might be significant. I then used an AIC function that added these variables stepwise into 5 different models, one for each added variable. It then scored each model using AIC, a relative way of evaluating model performance, to see which one did the best. I then used the best model to make some predictions. Comparing the coefficients of the variables from their individual models to their coefficients from the consolidated model, I was able evaluate the robustness of each variable. Since we’re testing a large number of levels in the variables, it is likely we will find significance regardless if there is actually an effect. Cross-checking the output of our individual models with our consolidated model allows us to see if our significant levels are robust.\nFirst, we’re going to look at using logistic regression on each variable individually. Our logistic regression uses the following logit function: \\[\\operatorname{logit}(p)=\\log \\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1  x + \\varepsilon \\]\nWhere \\(\\beta_0\\) is our intercept, and \\(\\beta_1\\) is the coefficient for a two-level factor variable that toggles on (\\(x = 1\\)) when we are not evaluating our reference category. For a variable with \\(i\\) levels, we will have \\(i - 1\\) terms of \\(\\beta\\). We will use this expression for each of our variables.\nUnfortunately, the model coefficients don’t tell us much since they’re log-transformed, and we have to re-transform to be able to interpret them. Taking endemism for example, after some rearranging we get an expression that we can use to calculate how much more likely an endemic species is to be extinct than a non-endemic species. We will use this approach for each significant level of our variables. \\[\\hat p = \\frac{e^{\\beta_0 + \\beta_1 x}}{1+ e^{\\beta_0 + \\beta_1 x}}\\]\nWe’re going to calculate the right hand side of our equation above for endemic species (\\(x = 1\\)) since its a categorical variable), and non-endemic species (\\(x = 0\\)). When we take the difference of the two, we calculate how much MORE likely it is for an endemic species to be extinct than a non-endemic species.\n\nEndemismHabitatThreatUseTaxonomy\n\n\nI first investigated endemism - a species endemic to Santa Barbra means it is found nowhere else in the world outside of Santa Barbara. This seemed like a good place to start, since an endemic species is geographically and genetically restricted to one location, which seems likely to render it more prone to extinction than a non-endemic species. Below is a mosaic plot - which uses area to visualize to categorical variables - to see if there is an obvious correlation.\n\n\n\n\n\nIt’s difficult to tell, but it looks that endemic species might be more likely to also be extinct. We’re going to take a look at the logistic regression output for regressing extinct on endemic.\nHere is the R output for the model summary:\n\n\n\n\nTable 1. Logistic Regression of Extinction on Endemism\n\n \nEndemism\n\n\nPredictors\nOdds Ratios\nConf. Int (95%)\nP-value\n\n\nIntercept\n0.0177\n0.0166 – 0.0188\n<0.001\n\n\nEndemic\n2.0405\n1.8307 – 2.2718\n<0.001\n\n\nObservations\n70067\n\n\nR2 Tjur\n0.002\n\n\n\n\n\n\nSince our p-value is far below any of the conventional significance levels, it looks like endemism on its own is significant in predicting extinction. After the transformation, our results show that an endemic species is 1.75% more likely to be extinct than a non-endemic species.\n\n\nI then investigated the type of habitat the species lives in. Running a logistic regression model only on habitat shows that the Cave habitat (our reference group, when \\(x = 0\\)) is significant - with a minimal p-value.\n\n\n\n\n\nWetlands and Forest habitats are also significant at a 0.05 significance level, and Marine Neritic habitats are significant at a significance level of 0.01. Let’s turn the coefficients into something more interpretable as we did above.\n\n\nProfiled confidence intervals may take longer time to compute.\n  Use `ci_method=\"wald\"` for faster computation of CIs.\n\n\n\n\nTable 2. Logistic Regression of Extinction on Habitat\n\n \nHabitat\n\n\nPredictors\nOdds Ratios\nP-value\n\n\nIntercept\n0.0079\n<0.001\n\n\nDesert\n2.4085\n0.130\n\n\nForest\n2.7000\n0.049\n\n\nGrassland\n1.4574\n0.482\n\n\nMarine Neritic\n0.2040\n0.006\n\n\nRocky Areas\n0.8166\n0.727\n\n\nSavanna\n0.3305\n0.118\n\n\nShrubland\n1.8463\n0.236\n\n\nWetlands\n3.2211\n0.020\n\n\nObservations\n68395\n\n\nR2 Tjur\n0.003\n\n\n\n\n\n\nSummarizing our significant results in comparison to the reference group (Caves/Subterranean Habitats):\nSpecies living in a Cave/Subterranean habitat have a 0.78% chance of also being extinct, while species living in a Marine Neritic habitat have 0.619% LESS of a chance of being extinct than species living in a Cave/Subterranean habitat.\n\n\nNext was the type of threat that the species faces.\n\n\n\n\n\nOur results show that threat types of Agriculture and Aquaculture, Pollution and Invasive species/Diseases are significant.\n\n\n\n\nTable 3. Logistic Regression of Extinction on Threat\n\n \nThreat\n\n\nPredictors\nOdds Ratios\nP-value\n\n\nIntercept\n0.0217\n<0.001\n\n\nBiological resource use\n0.7628\n0.016\n\n\nEnergy production and Mining\n0.5140\n0.005\n\n\nHuman intrusions and disturbance\n0.9570\n0.879\n\n\nInvasive species, genes and disease\n5.2889\n<0.001\n\n\nNatural system modifications\n1.3708\n0.012\n\n\nPollution\n1.5498\n<0.001\n\n\nResidential and Commercial Development\n1.1123\n0.352\n\n\nTransportation and service corridors\n0.5502\n0.081\n\n\nObservations\n37633\n\n\nR2 Tjur\n0.019\n\n\n\n\n\n\nAgain, summarizing our significant results compared to the reference group (Agriculture/Aquaculture):\nSpecies threatened by Agriculture/Aquaculture have a 2.12% chance of also being extinct. Species threatened by Invasive species/Disease and pollution have a 8.16% and 1.1% more chance of being extinct than species threatened by Agriculture/Aquaculture, respectively.\n\n\nUse seemed like another appropriate variable to investigate. Perhaps species that provide medicinal or energy uses are extracted at more unsustainable rates than a species that provides an artisinal use.\n\n\n\n\nTable 4. Logistic Regression of Extinction on Use\n\n \nUse\n\n\nPredictors\nOdds Ratios\nP-value\n\n\nIntercept\n0.0000\n0.995\n\n\nex - situ production\n1018924.8700\n0.997\n\n\nfibre\n1.0000\n1.000\n\n\nFood - animal\n1.0000\n1.000\n\n\nFood - human\n2451051.3686\n0.996\n\n\nfuels\n1.0000\n1.000\n\n\nhandicrafts, jewellery, etc\n1213090.6232\n0.997\n\n\nManufacturing chemicals\n1.0000\n1.000\n\n\nMedicine\n1120077.2179\n0.997\n\n\nOther\n2628363.0170\n0.996\n\n\nother chemicals\n21026904.1357\n0.996\n\n\nother household goods\n1.0000\n1.000\n\n\ndisplay animals, horticulture\n365417.8191\n0.997\n\n\nPoisons\n1.0000\n1.000\n\n\nResearch\n3304227.7928\n0.996\n\n\nsport hunting/Specimen collecting\n349389.6458\n0.997\n\n\nunknown\n1.0000\n1.000\n\n\nwearing apparel, accessories\n2115511.6966\n0.996\n\n\nObservations\n19133\n\n\nR2 Tjur\n0.008\n\n\n\n\n\n\nThis shows that the human use for each species is not significant for predicting extinction. A potential problem with this though, is the amount of missing data in this column. Out of our over 70,000 species observations, approximately 51,000 of these do not have associated use cases. This may be because we simply don’t have a human use for many species, or that the uses just aren’t properly documented.\n\n\nTaxonomy seemed like another interesting variable to investigate. It seems likely that more closely-related species will face similar extinction pressures. Since we’re working within the Animalia Kingdom, we will run a logistic regression using the class of each species. I’ve shortened the output below to only include a few of the classes for visual purposes.\n\n\n\n\nTable 5. Logistic Regression of Extinction on Taxa\n\n \nTaxa\n\n\nPredictors\nOdds Ratios\nP-value\n\n\nclass [AMPHIBIA]\n2.5886\n<0.001\n\n\nclass [ANTHOZOA]\n0.2351\n0.042\n\n\nclass [ARACHNIDA]\n7.1234\n<0.001\n\n\nclass [AVES]\n1.4023\n0.001\n\n\nclass [BIVALVIA]\n6.4434\n<0.001\n\n\nclass [GASTROPODA]\n6.3545\n<0.001\n\n\nclass [INSECTA]\n1.2961\n0.017\n\n\nclass [MAMMALIA]\n1.9058\n<0.001\n\n\nclass [REPTILIA]\n0.7869\n0.065\n\n\nObservations\n70067\n\n\nR2 Tjur\n0.017\n\n\n\n\n\n\nThere are quite a few classes that look to be significant here. Especially significant classes appear to be, Actinopterygii (Ray-finned fishes, our reference group) Amphibians, Arachnids, Aves, Bivalves, and Gastropods. Since we’re testing so many levels, we expect that our model will find significance regardless if there is an actual effect. We will keep an eye on the significant classes as we build our larger model. Below is a dendrogram of the evolutionary relationships between classes and their significance levels.\n\n\n\n\n\n\n\n\n\nStepwise Model\nNow, we want to see if any of these significant levels of previous variables are still significant when we start to add our variables together for a more complete model. If they are still significant, we can be comfortable concluding that the level is influencing extinction. We start with predicting extinction off of one variable, endemism, and then incrementally add our other variables of interest. We use a stepwise AIC function - which will take a look at each step, and output scores for each step of the model, indicating which model does the best job at predicting extinction.\n\n\n\n\n \n  \n      \n    K \n    AICc \n    Delta_AICc \n    AICcWt \n    LL \n  \n \n\n  \n    5 \n    56 \n    1615.0 \n    0.0 \n    1 \n    -751.2 \n  \n  \n    4 \n    47 \n    8794.1 \n    7179.1 \n    0 \n    -4350.0 \n  \n  \n    3 \n    18 \n    8985.0 \n    7370.0 \n    0 \n    -4474.5 \n  \n  \n    2 \n    10 \n    11713.7 \n    10098.7 \n    0 \n    -5846.9 \n  \n  \n    1 \n    2 \n    14210.9 \n    12595.9 \n    0 \n    -7103.5 \n  \n\n\n\n\n\nThe last step in our model has the lowest AIC score, and appears to be the best. This is slightly worrying, since it is the most complicated model - it uses 56 different parameters (1 for each level of each variable) to predict extinction. This could potentially indicate over-fitting, so we’re going to take a look at our coefficients and p-values of our significant variable levels, to see which levels remain robust. Again, I’ve shortened the model output to include only the relevant levels.\n\n\n\n\nTable 6. Generalized Logistic Regression\n\n \nGeneralized Mod\n\n\nPredictors\nOdds Ratios\nP-value\n\n\nendemic [Yes]\n4.1961\n<0.001\n\n\nhabitat [Desert]\n1.4347\n1.000\n\n\nhabitat [Forest]\n3927257.7750\n0.996\n\n\nhabitat [Marine Neritic]\n1059842.1871\n0.996\n\n\nhabitat [Wetlands]\n10420495.4785\n0.996\n\n\nthreat [Pollution]\n6.6093\n<0.001\n\n\nclass [AMPHIBIA]\n2.2075\n0.019\n\n\nclass [ARACHNIDA]\n0.0000\n0.996\n\n\nclass [AVES]\n8.3032\n<0.001\n\n\nclass [GASTROPODA]\n0.3001\n0.245\n\n\nclass [INSECTA]\n0.0000\n0.990\n\n\nuse [Medicine]\n2209540.2542\n0.999\n\n\nuse [Poisons]\n0.2001\n1.000\n\n\nuse [Research]\n4638444.2655\n0.999\n\n\nObservations\n11603\n\n\nR2 Tjur\n0.101\n\n\n\n\n\n\nThe significance level for endemism remains far below any conventional significance threshold, it’s coefficient hardly changed, so we can remain confident that Endemic species are indeed more likely to be extinct. This is a robust indicator.\nSimilarly, threats of Pollution and Invasive Species/Disease remain robust in our more complete model. The significance levels are far below the usual significance thresholds of 0.05 and 0.01, indicating that these are also a robust indicators.\nFinally, the Aves, Amphibia, and Gastropoda classes also look like they’re remaining robust - although Aves to a lesser degree. The significance level decreases quite a bit from when we evaluated taxa on their own, and the coefficient changes noticeably. This indicates that there may be an interaction effect between taxa and one of the other variables. Perhaps species in the Aves class (birds) are more prone to infectious diseases than species in other classes.\nSo, the variable levels that we are confident are associated with species extinction, at least in this dataset are endemic species threatened by pollution or disease, in the Aves, Amphibia, and Gastropoda classes. This is fairly consistent with our current knowledge of extinction.\n\n\nCode\ndendrogram_2\n\n\n\n\n\n\nPredictions\nI then used the step4 model models to make probability predictions that a species is extinct. Because the augment function that I used removes any NA values before making predictions, I used step4 model instead of step5. The use variable only contains data for ~ 20,000 of our 70,000 species - so we’d be losing well over half of our data if we used the step5 model. We’re still losing ~30,000 species with our step4 model, since not every species has an associated threat, so we expect our p-value in our T test to look slightly different.\nThe step4 model contains the variables endemic, habitat , threat and class. I then used augment to output the predicted probability that a species is extinct based on the variables used in the step4 model.\n\n\n\n\n\nWe can evaluate how accurate the model is using a confusion matrix.\n\n\n\n\n \n  \n    Prediction \n    Reference \n    Freq \n  \n \n\n  \n    0 \n    0 \n    36524 \n  \n  \n    1 \n    0 \n    1065 \n  \n  \n    0 \n    1 \n    1 \n  \n  \n    1 \n    1 \n    1 \n  \n\n\n\n\n\nAlthough the model is 97% accurate, it is pretty bad at predicting if species are extinct or not. Here’s where machine learning comes into play."
  },
  {
    "objectID": "Portfolio/stats_final/index.html#assumptions",
    "href": "Portfolio/stats_final/index.html#assumptions",
    "title": "Exploring Extinction",
    "section": "Assumptions",
    "text": "Assumptions\nI categorized species that are classified as “extinct in the wild” as extinct, since we are interested in species outside of captivity. I also took a case by case approach to classify some of the critically endangered species with values of the variable Possibly Extinct as TRUE as extinct - since many of these species have not been seen in many years (our Chinese River Dolphin friend actually falls into this category, it is listed as critically endangered despite not having been seen since 2007) and are widely agreed to be at least functionally extinct (where there are so few members of the species surviving in the wild that it is unlikely they will ever come into contact)."
  },
  {
    "objectID": "Portfolio/stats_final/index.html#simplifications",
    "href": "Portfolio/stats_final/index.html#simplifications",
    "title": "Exploring Extinction",
    "section": "Simplifications",
    "text": "Simplifications\nTo ensure that each observation was a species and that there weren’t multiple observations of the same species, I categorized each of the sub habitats into one general habitat - tropical rainforests in Costa Rica and boreal forests in Siberia are both considered forests. I also collapsed species that live in multiple habitats into one habitat. I took similar approaches with the threat type, generalizing each sub type into one general type, collapsing species facing multiple threats into one threat, as well as the use case. Each of these introduces its own oversimplification issues, and should be explored more thoroughly in future investigations. Here is the link to IUCN’s classification schemes."
  },
  {
    "objectID": "Portfolio/stats_final/index.html#limitations",
    "href": "Portfolio/stats_final/index.html#limitations",
    "title": "Exploring Extinction",
    "section": "Limitations",
    "text": "Limitations\nOn top of all this, it is extremely difficult to study extinction. We don’t have crucial data on population dynamics, geographic range, reproductive capacity, genetic diversity, and many other important factors for many species. In fact, there are probably still millions of species of plants and animals that we have yet to identify, let alone gather enough pertinent data to understand its status. The IUCN only describes species that went extinct relatively recently. When we consider the number of extinctions that have happened over geologic time scales, we are looking at an extremely minute sample. The IUCN has assessed only ~7% of its described species. Even for species that we are aware of, it is very difficult to tell if a species is actually extinct. The Amsterdam Widgeon was endemic to Île of Amsterdam in the French Southern Territories before it went extinct likely due to visiting sealers and the rats they introduced sometime between 1600 - 1800. No naturalist even visited the island until 1874, and we only know that it existed through its bones that were found in 19555. To illustrate an extreme example of how bad we are at this, we’ll take a look at the Coelecanths.\nThis ancient genus first appeared in the fossil record over 400 million years ago. It disappeared from the fossil record 66 million years ago and was presumed to have gone extinct along with the dinosaurs. In 1938, one species of Coelecanth was rediscovered in a fishing net off the South African Coast. Here it is, a living fossil, alive and swimming.\n\nFigure 8. Live Coelecanth\nSince then, another Coelecanth species has been discovered, and over 100 individual specimens have been recorded. Coelecanths are classified as critically endangered, the IUCN estimates that fewer than 500 exist in the wild, and are suffering as a result of over-fishing. This is one example of a Lazarus taxon - an evolutionary line that disappeared from the fossil record only to reappear much later."
  },
  {
    "objectID": "Facebook Papers.html",
    "href": "Facebook Papers.html",
    "title": "Data Justice and the Facebook Papers",
    "section": "",
    "text": "As we proceed towards a future increasingly dictated by data-driven decisions, we are faced with a myriad of new ethical issues that must be addressed. It’s hard to imagine that those with access to environmental data will not have to make the same decisions made by big tech companies. User data has become the life blood for the tech giants of surveillance capitalism, and their profits are consistently chosen over the well-being of their users and society as a whole. This post will examine the facebook papers, and explore the consequences for data justice in an algorithmically manipulated world."
  },
  {
    "objectID": "Facebook Papers.html#misinformation",
    "href": "Facebook Papers.html#misinformation",
    "title": "Data Justice and the Facebook Papers",
    "section": "Misinformation",
    "text": "Misinformation\nFacebook uses a strike system to manage misinformation circulated on the app. Too many strikes in a 90-day period can result in a user being suspended from posting content, and reductions in circulation and advertisement revenue. However, accounts with large followings are given special treatment, as is the case with Breitbart, a well-known right-wing conspiratorial news site. Former president Donald Trump posted a video posted by Breitbart that encouraged viewers not to wear a mask for COVID-19 and to take hydroxychloroquine instead. Despite clearly violating their misinformation policy, the video was allowed to circulate and reach millions of people before it was taken down. Meta’s CEO, Mark Zuckerberg, claimed that Breitbart wasn’t punished for the video, as it was the company’s only misinformation violation in a 90-day period2. Breitbart is a ‘managed partner’ with Facebook, where a facebook employee is assigned to watch the company’s posts and alert the company to edit posted material that contains misinformation. This system allows Breitbart to avoid strikes, as long as they edit their content after it has been spread around. Thus, Breitbart gets their content circulated, while Facebook gets to say that they’re working to prevent misinformation while profiting off of Breitbart’s material. Additionally, facebook executives also allow the removal of strikes to avoid de-listing any right wing managed partners. This is also an example of Facebook not wanting to upset its conservative users - allowing misinformation to spread instead. One employee wrote in the company’s internal chat: “We are apparently providing hate-speech-policy-consulting and consequence-mitigation services to select partners”3."
  },
  {
    "objectID": "Portfolio/stats_final/CopyOfindex.html",
    "href": "Portfolio/stats_final/CopyOfindex.html",
    "title": "Exploring Extinction",
    "section": "",
    "text": "Species extinction is a normal part of Earth’s natural history, as over 99.9% of species to ever have existed are now extinct1. Background extinction rates under normal conditions is expected to between 0.1 - 1 species of vertebrates to go extinct every 100 years per 10,000 species2. However, throughout Earth’s history there have been calamitous events such as asteroid impacts, volcanic eruptions, and sudden atmospheric changes that have rapidly changed the conditions on Earth to be unsuitable for life. The worst of these extinction events, the Permian extinction, is thought to have killed off 90% of all species on Earth.\nThe current species extinction rate is estimated to be between 1,000 and 10,000 times higher than the normal background extinction rate3, which is enough to consider our current time period the 6th mass extinction event - due to various human activities.\n\n[Figure 1. The Big Five Mass Extinctions]\n\n\n\n\n\n\nYou may be familiar with some of the species that have recently gone extinct due to human activity. Among these are the Chinese River dolphin, which lived in the Yangtze river of China and was last seen in 2007 - thought to be driven to extinction due to heavy use of the river for fishing, transportation and hydroelectricity,\n\n[Figure 2. The Chinese River Dolphin (Baiji)]\n\n\n\nThe Tasmanian Tiger which lived on Tasmania, a small island south east of Australia, and was hunted to extinction in the 1930s\n\n[Figure 3. Tasmanian Tiger (Thylacine)]\n\n\n\nAnd the famous Dodo bird, endemic to the island of Mauritius, it lacked adaptations to prevent its own extinction from hunting by sailors and habitat destruction in the late 17th century.\n\n[Figure 4. The dodo went extinct by 1681]\n\n\n\nSpecies provide us not only with important sources of medicine, food and various other products, but also play important roles in each of their respective ecosystems on which much of our societies’ depend. Each species also helps us elucidate the story of life’s history on Earth and contextualizes our relationship with the natural world. More importantly however, species have intrinsic value regardless of what they provide for humans, and each one lost is a tragedy in its own right.\nIts important to understand the factors that render species vulnerable to extinction, as well as what the mechanisms of extinction are and how they work. Extinction is notoriously difficult to study mainly due to our lack of data which I will expand upon in the issues section, but we can hopefully use some of these findings to identify vulnerable species, and better protect them and their ecosystems from extinction and collapse."
  },
  {
    "objectID": "Portfolio/stats_final/CopyOfindex.html#cleaning",
    "href": "Portfolio/stats_final/CopyOfindex.html#cleaning",
    "title": "Exploring Extinction",
    "section": "Cleaning",
    "text": "Cleaning\nAfter cleaning the data, each row represented one species with a unique assessment_id, and each column contained a variable that I thought might influence extinction. The variables that I focused on were: species endemism (endemic), habitat type (habitat), the type of threat faced (threat), human use (use), and taxonomy (class).\n\n\nCode\nhead(predictors) |> \n  datatable()\n\n\n\n\n\n\n\nTo clarify some nomenclature before we start modeling:\nWhen I use the term variable, I’m referencing the column names that we’re going to use in our analysis. When I use the term level, I’m referring to the different values that each column name can take on. For example, one of our variables is habitat. The different levels it can take on are below.\n\n\nCode\nlevels(predictors$habitat) \n\n\n[1] \"Caves\"          \"Desert\"         \"Forest\"         \"Grassland\"     \n[5] \"Marine Neritic\" \"Rocky Areas\"    \"Savanna\"        \"Shrubland\"     \n[9] \"Wetlands\""
  },
  {
    "objectID": "Portfolio/stats_final/CopyOfindex.html#modeling",
    "href": "Portfolio/stats_final/CopyOfindex.html#modeling",
    "title": "Exploring Extinction",
    "section": "Modeling",
    "text": "Modeling\nI ran a logistic linear regression on each of these variables individually to get a feel for which levels might be significant. I then used an AIC function that added these variables stepwise into 5 different models, one for each added variable. It then scored each model using AIC, a relative way of evaluating model performance, to see which one did the best. I then used the best model to make some predictions. Comparing the coefficients of the variables from their individual models to their coefficients from the consolidated model, I was able evaluate the robustness of each variable. Since we’re testing a large number of levels in the variables, it is likely we will find significance regardless if there is actually an effect. Cross-checking the output of our individual models with our consolidated model allows us to see if our significant levels are robust.\nFirst, we’re going to look at using logistic regression on each variable individually. Our logistic regression uses the following logit function: \\[\\operatorname{logit}(p)=\\log \\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1  x + \\varepsilon \\]\nWhere \\(\\beta_0\\) is our intercept, and \\(\\beta_1\\) is the coefficient for a two-level factor variable that toggles on (\\(x = 1\\)) when we are not evaluating our reference category. For a variable with \\(i\\) levels, we will have \\(i - 1\\) terms of \\(\\beta\\). We will use this expression for each of our variables.\nUnfortunately, the model coefficients don’t tell us much since they’re log-transformed, and we have to re-transform to be able to interpret them. Taking endemism for example, after some rearranging we get an expression that we can use to calculate how much more likely an endemic species is to be extinct than a non-endemic species. We will use this approach for each significant level of our variables. \\[\\hat p = \\frac{e^{\\beta_0 + \\beta_1 x}}{1+ e^{\\beta_0 + \\beta_1 x}}\\]\nWe’re going to calculate the right hand side of our equation above for endemic species (\\(x = 1\\)) since its a categorical variable), and non-endemic species (\\(x = 0\\)). When we take the difference of the two, we calculate how much MORE likely it is for an endemic species to be extinct than a non-endemic species.\n\nEndemism\nI first investigated endemism - a species endemic to Santa Barbra means it is found nowhere else in the world outside of Santa Barbara. This seemed like a good place to start, since an endemic species is geographically and genetically restricted to one location, which seems likely to render it more prone to extinction than a non-endemic species. Below is a mosaic plot - which uses area to visualize to categorical variables - to see if there is an obvious correlation.\n\n\n\n\n\nIt’s difficult to tell, but it looks that endemic species might be more likely to also be extinct. We’re going to take a look at the logistic regression output for regressing extinct on endemic.\nHere is the R output for the model summary:\n\n\n\n\nTable 1. Logistic Regression of Extinction on Endemism\n\n \nEndemism\n\n\nPredictors\nOdds Ratios\nConf. Int (95%)\nP-value\n\n\nIntercept\n0.0177\n0.0166 – 0.0188\n<0.001\n\n\nEndemic\n2.0405\n1.8307 – 2.2718\n<0.001\n\n\nObservations\n70067\n\n\nR2 Tjur\n0.002\n\n\n\n\n\n\nSince our p-value is far below any of the conventional significance levels, it looks like endemism on its own is significant in predicting extinction. After the transformation, our results show that an endemic species is 1.75% more likely to be extinct than a non-endemic species.\n\n\nHabitat\nI then investigated the type of habitat the species lives in. Running a logistic regression model only on habitat shows that the Cave habitat (our reference group, when \\(x = 0\\)) is significant - with a minimal p-value.\n\n\n\n\n\nWetlands and Forest habitats are also significant at a 0.05 significance level, and Marine Neritic habitats are significant at a significance level of 0.01. Let’s turn the coefficients into something more interpretable as we did above.\n\n\nProfiled confidence intervals may take longer time to compute.\n  Use `ci_method=\"wald\"` for faster computation of CIs.\n\n\n\n\nTable 2. Logistic Regression of Extinction on Habitat\n\n \nHabitat\n\n\nPredictors\nOdds Ratios\nP-value\n\n\nIntercept\n0.0079\n<0.001\n\n\nDesert\n2.4085\n0.130\n\n\nForest\n2.7000\n0.049\n\n\nGrassland\n1.4574\n0.482\n\n\nMarine Neritic\n0.2040\n0.006\n\n\nRocky Areas\n0.8166\n0.727\n\n\nSavanna\n0.3305\n0.118\n\n\nShrubland\n1.8463\n0.236\n\n\nWetlands\n3.2211\n0.020\n\n\nObservations\n68395\n\n\nR2 Tjur\n0.003\n\n\n\n\n\n\nSummarizing our significant results in comparison to the reference group (Caves/Subterranean Habitats):\nSpecies living in a Cave/Subterranean habitat have a 0.78% chance of also being extinct, while species living in a Marine Neritic habitat have 0.619% LESS of a chance of being extinct than species living in a Cave/Subterranean habitat.\n\n\nThreat\nNext was the type of threat that the species faces.\n\n\n\n\n\nOur results show that threat types of Agriculture and Aquaculture, Pollution and Invasive species/Diseases are significant.\n\n\n\n\nTable 3. Logistic Regression of Extinction on Threat\n\n \nThreat\n\n\nPredictors\nOdds Ratios\nP-value\n\n\nIntercept\n0.0217\n<0.001\n\n\nBiological resource use\n0.7628\n0.016\n\n\nEnergy production and Mining\n0.5140\n0.005\n\n\nHuman intrusions and disturbance\n0.9570\n0.879\n\n\nInvasive species, genes and disease\n5.2889\n<0.001\n\n\nNatural system modifications\n1.3708\n0.012\n\n\nPollution\n1.5498\n<0.001\n\n\nResidential and Commercial Development\n1.1123\n0.352\n\n\nTransportation and service corridors\n0.5502\n0.081\n\n\nObservations\n37633\n\n\nR2 Tjur\n0.019\n\n\n\n\n\n\nAgain, summarizing our significant results compared to the reference group (Agriculture/Aquaculture):\nSpecies threatened by Agriculture/Aquaculture have a 2.12% chance of also being extinct. Species threatened by Invasive species/Disease and pollution have a 8.16% and 1.1% more chance of being extinct than species threatened by Agriculture/Aquaculture, respectively.\n\n\nUse\nUse seemed like another appropriate variable to investigate. Perhaps species that provide medicinal or energy uses are extracted at more unsustainable rates than a species that provides an artisinal use.\n\n\n\n\nTable 4. Logistic Regression of Extinction on Use\n\n \nUse\n\n\nPredictors\nOdds Ratios\nP-value\n\n\nIntercept\n0.0000\n0.995\n\n\nex - situ production\n1018924.8700\n0.997\n\n\nfibre\n1.0000\n1.000\n\n\nFood - animal\n1.0000\n1.000\n\n\nFood - human\n2451051.3686\n0.996\n\n\nfuels\n1.0000\n1.000\n\n\nhandicrafts, jewellery, etc\n1213090.6232\n0.997\n\n\nManufacturing chemicals\n1.0000\n1.000\n\n\nMedicine\n1120077.2179\n0.997\n\n\nOther\n2628363.0170\n0.996\n\n\nother chemicals\n21026904.1357\n0.996\n\n\nother household goods\n1.0000\n1.000\n\n\ndisplay animals, horticulture\n365417.8191\n0.997\n\n\nPoisons\n1.0000\n1.000\n\n\nResearch\n3304227.7928\n0.996\n\n\nsport hunting/Specimen collecting\n349389.6458\n0.997\n\n\nunknown\n1.0000\n1.000\n\n\nwearing apparel, accessories\n2115511.6966\n0.996\n\n\nObservations\n19133\n\n\nR2 Tjur\n0.008\n\n\n\n\n\n\nThis shows that the human use for each species is not significant for predicting extinction. A potential problem with this though, is the amount of missing data in this column. Out of our over 70,000 species observations, approximately 51,000 of these do not have associated use cases. This may be because we simply don’t have a human use for many species, or that the uses just aren’t properly documented.\n\n\nTaxonomy\nTaxonomy seemed like another interesting variable to investigate. It seems likely that more closely-related species will face similar extinction pressures. Since we’re working within the Animalia Kingdom, we will run a logistic regression using the class of each species. I’ve shortened the output below to only include a few of the classes for visual purposes.\n\n\n\n\nTable 5. Logistic Regression of Extinction on Taxa\n\n \nTaxa\n\n\nPredictors\nOdds Ratios\nP-value\n\n\nclass [AMPHIBIA]\n2.5886\n<0.001\n\n\nclass [ANTHOZOA]\n0.2351\n0.042\n\n\nclass [ARACHNIDA]\n7.1234\n<0.001\n\n\nclass [AVES]\n1.4023\n0.001\n\n\nclass [BIVALVIA]\n6.4434\n<0.001\n\n\nclass [GASTROPODA]\n6.3545\n<0.001\n\n\nclass [INSECTA]\n1.2961\n0.017\n\n\nclass [MAMMALIA]\n1.9058\n<0.001\n\n\nclass [REPTILIA]\n0.7869\n0.065\n\n\nObservations\n70067\n\n\nR2 Tjur\n0.017\n\n\n\n\n\n\nThere are quite a few classes that look to be significant here. Especially significant classes appear to be, Actinopterygii (Ray-finned fishes, our reference group) Amphibians, Arachnids, Aves, Bivalves, and Gastropods. Since we’re testing so many levels, we expect that our model will find significance regardless if there is an actual effect. We will keep an eye on the significant classes as we build our larger model. Below is a dendrogram of the evolutionary relationships between classes and their significance levels.\n\n\n\n\n\n\n\nStepwise Model\nNow, we want to see if any of these significant levels of previous variables are still significant when we start to add our variables together for a more complete model. If they are still significant, we can be comfortable concluding that the level is influencing extinction. We start with predicting extinction off of one variable, endemism, and then incrementally add our other variables of interest. We use a stepwise AIC function - which will take a look at each step, and output scores for each step of the model, indicating which model does the best job at predicting extinction.\n\n\n\n\n \n  \n      \n    K \n    AICc \n    Delta_AICc \n    AICcWt \n    LL \n  \n \n\n  \n    5 \n    56 \n    1615.0 \n    0.0 \n    1 \n    -751.2 \n  \n  \n    4 \n    47 \n    8794.1 \n    7179.1 \n    0 \n    -4350.0 \n  \n  \n    3 \n    18 \n    8985.0 \n    7370.0 \n    0 \n    -4474.5 \n  \n  \n    2 \n    10 \n    11713.7 \n    10098.7 \n    0 \n    -5846.9 \n  \n  \n    1 \n    2 \n    14210.9 \n    12595.9 \n    0 \n    -7103.5 \n  \n\n\n\n\n\nThe last step in our model has the lowest AIC score, and appears to be the best. This is slightly worrying, since it is the most complicated model - it uses 56 different parameters (1 for each level of each variable) to predict extinction. This could potentially indicate over-fitting, so we’re going to take a look at our coefficients and p-values of our significant variable levels, to see which levels remain robust. Again, I’ve shortened the model output to include only the relevant levels.\n\n\n\n\nTable 6. Generalized Logistic Regression\n\n \nGeneralized Mod\n\n\nPredictors\nOdds Ratios\nP-value\n\n\nhabitat [Desert]\n1.4347\n1.000\n\n\nhabitat [Forest]\n3927257.7750\n0.996\n\n\nhabitat [Marine Neritic]\n1059842.1871\n0.996\n\n\nhabitat [Wetlands]\n10420495.4785\n0.996\n\n\nthreat [Pollution]\n6.6093\n<0.001\n\n\nclass [AMPHIBIA]\n2.2075\n0.019\n\n\nclass [ARACHNIDA]\n0.0000\n0.996\n\n\nclass [AVES]\n8.3032\n<0.001\n\n\nclass [GASTROPODA]\n0.3001\n0.245\n\n\nclass [INSECTA]\n0.0000\n0.990\n\n\nuse [Medicine]\n2209540.2542\n0.999\n\n\nuse [Poisons]\n0.2001\n1.000\n\n\nuse [Research]\n4638444.2655\n0.999\n\n\nObservations\n11603\n\n\nR2 Tjur\n0.101\n\n\n\n\n\n\nThe significance level for endemism remains far below any conventional significance threshold, it’s coefficient hardly changed, so we can remain confident that Endemic species are indeed more likely to be extinct. This is a robust indicator.\nSimilarly, threats of Pollution and Invasive Species/Disease remain robust in our more complete model. The significance levels are far below the usual significance thresholds of 0.05 and 0.01, indicating that these are also a robust indicators.\nFinally, the Aves, Amphibia, and Gastropoda classes also look like they’re remaining robust - although Aves to a lesser degree. The significance level decreases quite a bit from when we evaluated taxa on their own, and the coefficient changes noticeably. This indicates that there may be an interaction effect between taxa and one of the other variables. Perhaps species in the Aves class (birds) are more prone to infectious diseases than species in other classes.\nSo, the variable levels that we are confident are associated with species extinction, at least in this dataset are endemic species threatened by pollution or disease, in the Aves, Amphibia, and Gastropoda classes. This is fairly consistent with our current knowledge of extinction.\n\n\nCode\ndendrogram_2\n\n\n\n\n\n\nPredictions\nI then used the step4 model models to make probability predictions that a species is extinct. Because the augment function that I used removes any NA values before making predictions, I used step4 model instead of step5. The use variable only contains data for ~ 20,000 of our 70,000 species - so we’d be losing well over half of our data if we used the step5 model. We’re still losing ~30,000 species with our step4 model, since not every species has an associated threat, so we expect our p-value in our T test to look slightly different.\nThe step4 model contains the variables endemic, habitat , threat and class. I then used augment to output the predicted probability that a species is extinct based on the variables used in the step4 model.\n\n\n\n\n\nWe can evaluate how accurate the model is using a confusion matrix.\n\n\n\n\n \n  \n      \n    0 \n    1 \n  \n \n\n  \n    0 \n    36524 \n    1 \n  \n  \n    1 \n    1065 \n    1 \n  \n\n\n\n\n\nAlthough the model is 97% accurate, it is pretty bad at predicting if species are extinct or not. Here’s where machine learning comes into play."
  },
  {
    "objectID": "Portfolio/stats_final/CopyOfindex.html#assumptions",
    "href": "Portfolio/stats_final/CopyOfindex.html#assumptions",
    "title": "Exploring Extinction",
    "section": "Assumptions",
    "text": "Assumptions\nI categorized species that are classified as “extinct in the wild” as extinct, since we are interested in species outside of captivity. I also took a case by case approach to classify some of the critically endangered species with values of the variable Possibly Extinct as TRUE as extinct - since many of these species have not been seen in many years (our Chinese River Dolphin friend actually falls into this category, it is listed as critically endangered despite not having been seen since 2007) and are widely agreed to be at least functionally extinct (where there are so few members of the species surviving in the wild that it is unlikely they will ever come into contact)."
  },
  {
    "objectID": "Portfolio/stats_final/CopyOfindex.html#simplifications",
    "href": "Portfolio/stats_final/CopyOfindex.html#simplifications",
    "title": "Exploring Extinction",
    "section": "Simplifications",
    "text": "Simplifications\nTo ensure that each observation was a species and that there weren’t multiple observations of the same species, I categorized each of the sub habitats into one general habitat - tropical rainforests in Costa Rica and boreal forests in Siberia are both considered forests. I also collapsed species that live in multiple habitats into one habitat. I took similar approaches with the threat type, generalizing each sub type into one general type, collapsing species facing multiple threats into one threat, as well as the use case. Each of these introduces its own oversimplification issues, and should be explored more thoroughly in future investigations."
  },
  {
    "objectID": "Portfolio/stats_final/CopyOfindex.html#limitations",
    "href": "Portfolio/stats_final/CopyOfindex.html#limitations",
    "title": "Exploring Extinction",
    "section": "Limitations",
    "text": "Limitations\nOn top of all this, it is extremely difficult to study extinction. We don’t have crucial data on population dynamics, geographic range, reproductive capacity, genetic diversity, and many other important factors for many species. In fact, there are probably still millions of species of plants and animals that we have yet to identify, let alone gather enough pertinent data to understand its status. The IUCN only describes species that went extinct relatively recently. When we consider the number of extinctions that have happened over geologic time scales, we are looking at an extremely minute sample. The IUCN has assessed only ~7% of its described species. Even for species that we are aware of, it is very difficult to tell if a species is actually extinct. The Amsterdam Widgeon was endemic to Île of Amsterdam in the French Southern Territories before it went extinct likely due to visiting sealers and the rats they introduced sometime between 1600 - 1800. No naturalist even visited the island until 1874, and we only know that it existed through its bones that were found in 19555. To illustrate an extreme example of how bad we are at this, we’ll take a look at the Coelecanths.\nThis ancient genus first appeared in the fossil record over 400 million years ago. It disappeared from the fossil record 66 million years ago and was presumed to have gone extinct along with the dinosaurs. In 1938, one species of Coelecanth was rediscovered in a fishing net off the South African Coast. Here it is, a living fossil, alive and swimming.\n\n[Figure 10. Live Coelecanth]\nSince then, another Coelecanth species has been discovered, and over 100 individual specimens have been recorded. Coelecanths are classified as critically endangered, the IUCN estimates that fewer than 500 exist in the wild, and are suffering as a result of over-fishing. This is one example of a Lazarus taxon - an evolutionary line that disappeared from the fossil record only to reappear much later."
  },
  {
    "objectID": "Portfolio/texas_storm/index.html",
    "href": "Portfolio/texas_storm/index.html",
    "title": "Analyzing the Effects of the Texas February 2021 Storm on the Houston metropolitan Area",
    "section": "",
    "text": "“In February 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”1 For more background, check out these engineering and political perspectives.\nWe were tasked with:\n- estimating the number of homes in Houston that lost power as a result of the first two storms\n- investigating if socioeconomic factors are predictors of communities recovery from a power outage\nOur analysis was based on remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite. We used VNP46A1 to detect differences in night lights before and after the storm to identify areas that lost electric power.\nTo determine the number of homes that lost power, we linked (spatially join) these areas with OpenStreetMap data on buildings and roads.\nTo investigate potential socioeconomic factors that influenced recovery, we linked our analysis with data from the US Census Bureau.\n\n\n\n\nWe used NASA’s Worldview to explore the data around the day of the storm. There were several days with too much cloud cover to be useful, but 2021-02-07 and 2021-02-16 provide two clear, contrasting images to visualize the extent of the power outage in Texas.\nVIIRS data is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Many NASA Earth data products are distributed in 10x10 degree tiles in sinusoidal equal-area projection. Tiles are identified by their horizontal and vertical position in the grid. Houston lies on the border of tiles h08v05 and h08v06. We therefore needed to download two tiles per date.\n\n\n\nTypically highways account for a large portion of the night lights observable from space (see Google’s Earth at Night). To minimize falsely identifying areas with reduced traffic as areas without power, we used a buffer to ignore areas near highways.\nOpenStreetMap (OSM) is a collaborative project which creates publicly available geographic data of the world. Ingesting this data into a database where it can be subsetted and processed is a large undertaking. Fortunately, third party companies redistribute OSM data. We used Geofabrik’s download sites to retrieve a shapefile of all highways in Texas and prepared a Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area. \n\n\n\nWe also obtained building data from OpenStreetMap. We again downloaded from Geofabrick and prepared a GeoPackage containing only houses in the Houston metropolitan area.\n\n\n\n\nWe cannot readily get socioeconomic information for every home, so instead we obtained data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019. The folder ACS_2019_5YR_TRACT_48.gdb is an ArcGIS “file geodatabase”, a multi-file proprietary format that’s roughly analogous to a GeoPackage file. We used st_layers() to explore the contents of the geodatabase. Each layer contains a subset of the fields documents in the ACS metadata. The geodatabase contains a layer holding the geometry information, separate from the layers holding the ACS attributes. We had to combine the geometry with the attributes to get a feature layer that sf can use."
  },
  {
    "objectID": "Portfolio/texas_storm/index.html#assignment",
    "href": "Portfolio/texas_storm/index.html#assignment",
    "title": "Analyzing the Effects of the Texas February 2021 Storm on the Houston metropolitan Area",
    "section": "Assignment",
    "text": "Assignment\n\nFind locations of blackouts\nWe read the data in and converted it to a stars object, for compatibility with the sf package that we’ll use frequently in this analysis. We combined the tiles into a mosaic for each date (Feb 2, 2021, and Feb 16, 2021\n\n\nCode\n#reading in first tile from feb 7th\nfeb7_1 &lt;-  data1 |&gt; \n  st_as_stars()\n#reading in second tile from feb 7th \nfeb7_2 &lt;- data2 |&gt; \n  st_as_stars()\n\n#combining tiles from feb 7th\nfeb7_mosaic &lt;- st_mosaic(feb7_1, feb7_2)\n\n\n#doing the same with first and second tiles from feb 16th\nfeb16_1 &lt;- data3 |&gt; \n  st_as_stars()\n\nfeb16_2 &lt;- data4 |&gt; \n  st_as_stars()\n\n\n#combining them\nfeb16_mosaic &lt;- st_mosaic(feb16_1, feb16_2)\n\n#sanity check\nplot(feb7_mosaic, main = \"February 7th Satellite Image\")\n\n\n\n\n\nCode\nplot(feb16_mosaic, main = \"February 16th Satellite Image\")\n\n\n\n\n\nWe then created a blackout mask to find the difference in the night light intensity between the two dates presumably caused by the storm. We then classified any location that experienced a drop of more than 200 nW cm-2sr-1 as a location that experienced a blackout. Any location that experienced less than 200 nW cm-2sr-1 was assigned NA\nWe then vectorized the blackout mask and fixed any invalid geometries\n\n\n\nCode\n#finding the change in night lights intensity - see what we get\ndifference_lights &lt;- feb7_mosaic - feb16_mosaic\nplot(difference_lights, main = \"Difference in Light Intensity between February 7th and February 16th\")\n\n\n\n\n\nCode\n#reclassify difference raster - assign NAs to all points experiencing a drop less than 200\ndifference_lights[difference_lights &lt; 200] &lt;-  NA\nplot(difference_lights, main = \"Houston Metropolitan Areas that Experienced a Blackout\")\n\n\n\n\n\nCode\n#vectorize the blackout mask\nblackout_mask &lt;- st_as_sf(difference_lights) |&gt; \n  st_make_valid()\n\n\nWe then cropped the vectorized map to our region of interest (Houston metropolitan area) using a bounding box, and assigned to it the same coordinate reference system as our night lights data. We then cropped the blackout mask to Houston, and reprojected the cropped dataset to EPSG:3083 (NAD83 / Texas Centric Albers Equal Area).\n\n\n\nCode\n#defining houston coordinates\nhouston &lt;- cbind(x = c(-96.5, -96.5, -94.5, -94.5, -96.5), \n                         y = c(29, 30.5, 30.5, 29, 29))\n#turning coordinates into polygon\nhouston &lt;- st_sfc(st_polygon(list(houston)), crs = 4326)\n\n#subsetting and masking houston\nmask &lt;- st_intersects(blackout_mask, houston, sparse = FALSE)\nhouston_subset &lt;- blackout_mask[mask,]\n\n#reprojecting\nhouston_reproj &lt;- st_transform(houston_subset, crs = 3083)\nplot(houston_reproj, main = \"Blackouts in Houston\")\n\n\n\n\n\nThe next step was to exclude the highways from the blackout mask. We used an SQL query to load our highway data from the geopackage, and reprojected it to ESPG:3083. We then identified all areas within 200m of a highway using a buffer. After removing these areas, we were left with only the areas that experienced a blackout that are further than 200m from a highway.\n\n\nCode\n#reprojecting highways\nhighways_reproject &lt;- st_transform(highways, crs = 3083)\n\n#making undissolved buffers\nhighway_buffer &lt;- st_union(st_buffer(highways_reproject, dist = 200))\nplot(highway_buffer, main = \"Highways in Houston that we will buffer out\")\n\n\n\n\n\nCode\n#plot area outside buffer\nhighway_blackouts &lt;- houston_reproj[highway_buffer, op = st_disjoint]\nplot(highway_blackouts, main = \"Blackout Areas in Houston with Highway Removed\")\n\n\n\n\n\nTo find the homes impacted by a blackout, we loaded the buildings dataset using another SQL query and only selected residential buildings. With the buildings dataset loaded and our blackout filter ready, we then filtered to homes within blackout areas. We were then able to count the number of impacted homes.\n\n\nCode\n#removing the buffered highways from the aoi\ncrop &lt;- st_difference(houston_reproj, highway_buffer)\n\n#subsetting here gives us all buildings&gt;200m away from a highway that experienced a blackout\nbuildings_blackout &lt;- buildings[crop, op = st_intersects]\n\n#counting the number of residential buildings that got hit - 157970\nprint(paste0(nrow(buildings_blackout), ' buildings were affected by the blackouts'))\n\n\n[1] \"157970 buildings were affected by the blackouts\""
  },
  {
    "objectID": "Portfolio/texas_storm/index.html#investigate-socioeconomic-factors",
    "href": "Portfolio/texas_storm/index.html#investigate-socioeconomic-factors",
    "title": "Analyzing the Effects of the Texas February 2021 Storm on the Houston metropolitan Area",
    "section": "Investigate socioeconomic factors",
    "text": "Investigate socioeconomic factors\nTo investigate if blackouts caused by the storm were correlated with any socioeconomic factors, we first joined the income data to the census tract geometries, and find which of these tracts had blackouts. We then created a map of median income by census tract, designating which tracts had blackouts. We plotted the distribution of income in impacted and unimpacted tracts.\n\n\n\nCode\n#join\ncensus_income &lt;- left_join(census_geodata, geodata_income, by = c('GEOID_Data' = \"GEOID\")) \n\n#transform crs\ncensus_income &lt;- st_transform(census_income, crs = 3083)\n\n# #Which tracts had blackouts?\ndata_blackouts &lt;- st_join(buildings_blackout, census_income) |&gt;\n  mutate(blackout_present = if_else(is.na(osm_id), 0, 1))\n\n#Buildings that didn't have any blackouts- manually used an anti-join\nno_blackouts &lt;- sapply(st_intersects(buildings, data_blackouts), function(x){length(x) == 0})\nno_blackouts_2 &lt;- buildings[no_blackouts,]\n\n#plotting the buildings affected\nblackout_buildings &lt;- buildings_blackout[census_income,]\n\nb2 &lt;- census_income[buildings_blackout,]\n\n#using our region we defined earlier + osm function to rasterize the bounding box\nbounding_box &lt;- st_bbox(houston) \nhouston_map2 &lt;- rosm::osm.raster(bounding_box)\n\n#now for the map\ntm_shape(houston_map2) + tm_rgb() + \n  tm_shape(census_income, bbox = bounding_box) +\n  tm_polygons(col = 'income',\n              colorNA = 'white',\n              title = 'Median Income (unaffected)',\n              palette= 'Reds', \n              style = 'cont') +\n  tm_shape(b2, bbox = bounding_box) + \n  tm_style('col_blind') +\n  tm_polygons(col = 'income',\n              title = 'Median income (affected)',\n              palette = 'Blues',\n              style = 'cont') +\n  tm_layout('Houston Median Income by Census Tract and Blackouts', legend.outside = TRUE)\n\n\n\n\n\n\n\nCode\n#distributions - impacted\nimpacted &lt;- data_blackouts[buildings_blackout, op = st_intersects]\nggplot(data = impacted, aes(x = income)) + geom_histogram(bins = 100) + labs(title = 'Median income of impacted residents', x = 'Median Income', y = 'count') + theme_minimal()\n\n\n\n\n\nCode\n#income dist - unimpacted\nunimpacted &lt;- st_join(no_blackouts_2, census_income, join = st_intersects)\nggplot(data = unimpacted, aes(x = income)) + geom_histogram(bins = 100) + labs(title = 'Median income of unimpacted residents', x = 'Median income', y = 'count') + theme_minimal()\n\n\n\n\n\nOver 150,000 buildings lost power in the Houston Metropolitan area due to the Texas 2021 February Winter storm. The distribution of median incomes between impacted and unimpacted residents appear to be similar. The median income of unaffected residents may be slightly higher on average than those who were affected by the blackouts. The limitations to this study include arbitrarily choosing a threshold in the change in light intensity between Feb 7 and Feb 16th, and basing our entire analyses off of data collected before the storm had finished hitting Houston. This limitation would systematically underestimate the extent of blackouts in Houston."
  },
  {
    "objectID": "Portfolio/ml_final/ocean_chem_modeling.html",
    "href": "Portfolio/ml_final/ocean_chem_modeling.html",
    "title": "Predicting dissolved inorganic carbon (DIC) concentrations using different algorithms in Python and R",
    "section": "",
    "text": "Preprocessing\nFirst, we are going to do some pre-processing and data exploration. The process is essentially the same in both R and python. We first import the data, and make sure our columns are consistent. The actual DIC values for our test set are stored in a separate dataframe, so we’ll also join these onto the test set.\n\nPythonR\n\n\nWe first use pandas to read in our data, merge our solutions to our test set, and make some minor adjustments to column names to ensure consistency.\n\n# Reading in the data using pandas. Join the solutions onto the test set\ntrain_py = pd.read_csv(\"train.csv\")\nsols = pd.read_csv(\"solution.csv\")\ntest_py = pd.read_csv(\"test.csv\")\n\ntest_py = pd.merge(test_py, sols, on=\"id\", suffixes=(\"\", \"_actual\"))\n# Dropping the initial index column, renaming TA1 to a more appropriate format\ntest_py = test_py.drop(test_py.columns[0], axis=1).rename(columns={\"TA1.x\": \"TA1.x\"})\ntrain_py = train_py.drop([train_py.columns[0], train_py.columns[12]], axis=1).rename(columns={\"TA1.x\": \"TA1\"})\n\n# We will be clearing our environment consistently using the del function in python, just to stay organized and clear up space\ndel(sols)\n\nLets plot the data to get a feel for where off the California coast the samples were collected. We’re going to use the cartopy package:\n\n\n# First, we set up the plot with cartopy\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree()) # specifying the map projection we want to use\n\n# Next, we'll define the region of interest (California coast)\nlat_min, lat_max = 30, 37\nlon_min, lon_max = -130, -114\n\n# Set the plot extent (latitude and longitude range of the coast)\nax.set_extent([lon_min, lon_max, lat_min, lat_max])\n\n# Add higher resolution coastlines, land, and ocean features for aesthetics\nax.add_feature(cfeature.NaturalEarthFeature('physical', 'land', '10m', edgecolor='face', facecolor=cfeature.COLORS['land']))\nax.add_feature(cfeature.NaturalEarthFeature('physical', 'ocean', '10m', edgecolor='face', facecolor=cfeature.COLORS['water'], alpha = 0.5))\nax.coastlines(resolution='10m')\n\n# Add the borders of the coastline\nax.add_feature(cfeature.BORDERS, linestyle=':')\n\n# Plot the sampling points \nax.scatter(train_py['Lon_Dec'], train_py['Lat_Dec'], color='red', marker='o')\n\n# Add labels and title\nax.set_xticks(np.arange(lon_min, lon_max + 1, 2), crs=ccrs.PlateCarree())\nax.set_yticks(np.arange(lat_min, lat_max + 1, 2), crs=ccrs.PlateCarree())\nax.xaxis.set_major_formatter(LongitudeFormatter())\nax.yaxis.set_major_formatter(LatitudeFormatter())\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.title(\"CalCOFI Sampling Points along the California Coast\")\n\nplt.show()\n\n\n\n\n\n\nWe read in the data the same way in R. We use an inner_join to bind the test values to the dataframe, and filter out the columns that we’re not interested in.\n\n# Read in the solutions data\nsolutions <- read_csv(\"solution.csv\")\n# Read in the testing data, join it to our solutions dataframe. \ntesting <- read_csv(\"test.csv\") |> inner_join(solutions, by = \"id\", suffix = c(\"\", \"_actual\")) |> select(-1, TA1.x = TA1)\n\n# Read in the training data, getting rid of the redundant id column and the other blank column\ntraining <- read_csv(\"train.csv\") |> select(-1, -...13)\n\n# Using doParallel to specify the number of cores that we want to use for some of our more computationally intensive models.  We'll come back to this later\nn_cores <- detectCores() - 1 \n\n# Clearing our environment\nrm(solutions)\n\nWe can also plot the data in R, instead using leaflet to add some basic interactivity.\n\n# Create a color palette for DIC. This adds another dimension to the data, and we'll be able to clearly see if there is any sort of spatial pattern with DIC\ncolor_palette <- colorNumeric(palette = \"YlOrRd\", domain = training$DIC)\n\n# Create the leaflet map\nleaflet(training) %>%\n  addProviderTiles(\"Esri.WorldImagery\", group = \"Satellite\") %>%\n  addProviderTiles(\"OpenStreetMap\", group = \"Street Map\") %>%\n  addCircleMarkers(lng = ~Lon_Dec, lat = ~Lat_Dec,\n                   fillColor = ~color_palette(DIC),\n                   fillOpacity = 1, stroke = FALSE,\n                   radius = 3, group = \"Data\") %>%\n  addLegend(pal = color_palette, values = ~DIC,\n            title = \"DIC\", position = \"bottomright\") %>%\n  addLayersControl(baseGroups = c(\"Satellite\", \"Street Map\"),\n                   overlayGroups = \"Data\",\n                   options = layersControlOptions(collapsed = FALSE))\n\n\n\n\nrm(color_palette)\n\nThis was partially an exercise to see if DIC was correlated with distance from the coastline at all, which it doesn’t appear to be.\n\n\n\n\n\nFeature Engineering\nNow we’re going to do some feature engineering. Feature engineering transforms existing data into new data, and helps improve our model performance by capturing relationships that might not be explicit in the data as it is. There are different types of feature engineering, but we’re going to focus on using interaction terms. Interaction terms are used to capture the combined effect of two or more variables on our target variable, DIC. This is particularly important in our context because ocean chemistry is complicated, and we would like to use our domain knowledge to capture some of the relationships between our predictors.\nFor example, consider the relationship between depth R_Depth and salinity R_Sal. The interaction between depth and salinity can be explained by the fact that as water depth increases, pressure also increases, which affects the solubility of gases in seawater. Salinity also affects the solubility of gases in seawater, so the combined effect of depth and salinity may impact dissolved inorganic carbon levels.\nUsing this same approach, we are going to include interaction terms for temperature/salinity, and depth/temperature.\n\nPythonR\n\n\nIn Python, we name the new interaction column and set it equal to the product of the terms we want to interact. We repeat this process for each interaction term.\n\n# Adding interaction terms to our training set\ntrain_py['T_degC_Salnty'] = train_py['Temperature_degC'] * train_py['Salinity1']\ntrain_py['Depth_Sal'] = train_py['R_Depth'] * train_py['R_Sal']\ntrain_py['Depth_Temp'] = train_py['R_Depth'] * train_py['Temperature_degC']\n\n# Same thing for the test set\ntest_py['T_degC_Salnty'] = test_py['Temperature_degC'] * test_py['Salinity1']\ntest_py['Depth_Sal'] = test_py['R_Depth'] * test_py['R_Sal']\ntest_py['Depth_Temp'] = test_py['R_Depth'] * test_py['Temperature_degC']\n\nWe then make our training and testing objects. We split off DIC and assign it to the y_test and y_train variables below. This allows us to explicitly tell our models which columns we’re using as features, and the outcome we want to predict.\n\n# Splitting off DIC in the training data\nX_train = train_py.drop(columns=['DIC'])\ny_train = train_py['DIC']\n\n# Same thing for the test data\nX_test = test_py.drop(columns=['DIC'])\ny_test = test_py['DIC']\n\n\n\nWe can use the same approach to make our interaction terms in R with the $ operator to initialize new columns:\n\n# Adding interaction terms to our training sets\ntraining$T_degC_Salnty <- training$Temperature_degC * training$Salinity1\ntraining$Depth_Sal <- training$R_Depth * training$R_Sal\ntraining$Depth_Temp <- training$R_Depth * training$Temperature_degC\n\n# Adding interaction terms to our test sets\ntesting$T_degC_Salnty <- testing$Temperature_degC * testing$Salinity1\ntesting$Depth_Sal <- testing$R_Depth * testing$R_Sal\ntesting$Depth_Temp <- testing$R_Depth * testing$Temperature_degC\n\nIn R, the tidymodels package provides us with useful functions for splitting training and testing data. We stratify on DIC to ensure that our data is not disproportionately split on the outcome variable, which would bias our model. We also specify our cross-validation parameters using 5 folds, and create a recipe object that we will use throughout our analysis. This recipe object uses our specifications to normalize the numeric variables, and we will use this in each R model. We will also normalize in Python, using pipelines that we create separately for each model.\n\n# Creating 5-fold CV\nfolds <- vfold_cv(data = training, v = 5, strata = DIC)\n\n# Creating our recipe object\nrec <- recipe(DIC ~ ., data = training) |> \n  step_normalize(all_numeric(), -DIC) |> \n  prep() \n\n\n\n\n\n\nLinear Regression\nLinear regression is the simplest model we can use to make predictions. It is an extremely powerful yet elegant technique used to model the relationship between one or more independent variables (predictors) and dependent variables (features). The basic form of the model is:\n\\[\\operatorname{Y}=\\beta_0+\\beta_1  x +\\beta_n x_n + \\varepsilon \\] Where \\(\\beta_0\\) is our intercept, \\(\\beta_1\\), \\(\\beta_n\\) are the coefficients of our independent variables, and \\(\\varepsilon\\) is the difference between the observed values and the values predicted by our model. We’re going to be evaluating our model performance using the Root Mean Squared Error (RMSE) metric which represents the square root of the average squared differences between the predicted values and the actual observed values. This helps us quantify the difference between the predicted values generated by the model and the actual values of the target variable.\nWe’re also going to try Lasso and Ridge regression here. Both of these follow the same concepts as regular linear regression, but they implement penalties based on slightly different metrics of the cost function. Ridge regression adds an L2 penalty term, which is the squared magnitude of the \\(\\beta\\) coefficients. This penalty term is controlled by alpha, and prevents our model from overfitting the training data by shrinking the coefficients towards zero, thus reducing the variance of the model. Lasso on the other hand uses an L1 penalty term on the cost function, which is based on the absolute magnitude of the coefficients. This penalty term is controlled by lambda, and forces some of the coefficients to be exactly zero, eliminating irrelevant features from the model. Both of these may improve our models’ performance.\n\nPythonR\n\n\nIn Python, we first instantiate a linear regression model, fit it on the training data, and then predict on the holdout data.\n\n\n# instantiate a Linear Regression model\nlr_model = LinearRegression()\n\n# fit the model to the training set\nlr_model.fit(X_train, y_train)\n\n# Predict on the test set\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\ny_test_pred_lr = lr_model.predict(X_test)\n\n# Calculate RMSE on our test set\nrmse_test_lr = mean_squared_error(y_test, y_test_pred_lr, squared=False)\n\nprint(f\"RMSE on the test set: {rmse_test_lr:.2f}\")\n\nRMSE on the test set: 4.93\n\n\nAn RMSE of 4.93 is not bad at all! Lets take a look at what this actually looks like plotted through our data:\n\n# clear our plot\nplt.clf()\n# Create a scatter plot of the true DIC values vs predicted DIC values\nplt.scatter(y_test, y_test_pred_lr, alpha=0.5)\n\n# Add a diagonal line representing perfect predictions\ndiagonal_line = np.linspace(min(y_test.min(), y_test_pred_lr.min()),\n                            max(y_test.max(), y_test_pred_lr.max()))\nplt.plot(diagonal_line, diagonal_line, color='red', linestyle='--', lw=2)\n\n# Add labels, title and RMSE text\nplt.xlabel('True DIC values')\nplt.ylabel('Predicted DIC values')\nplt.title('True vs Predicted DIC values')\nplt.text(0.05, 0.95, f'RMSE: {rmse_test_lr:.2f}', transform=plt.gca().transAxes)\n\n# Show the plot\nplt.show()\n\n\n\ndel(lr_model, y_test_pred_lr)\n\nThe red line here is what we would have observed if we correctly predicted each observation. The difference between the red line and the blue dots represent the difference between our model’s prediction, and the actual value.\nNow we’ll check out Ridge and Lasso regression. We follow the same approach as with linear regression but with one slight change. We specify the alpha parameter, which controls the strength of the penalty. This would usually be a tuning hyperparameter, which I’ll explain in a second, but we’re just going to use an alpha value of one.\n\nplt.clf()\n\n# Ridge Regression\nridge = Ridge(alpha=1)\nridge.fit(X_train, y_train)\n\nRidge(alpha=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=1)\n\nridge_preds = ridge.predict(X_test)\nridge_rmse = mean_squared_error(y_test, ridge_preds, squared=False)\nprint(\"Ridge RMSE:\", ridge_rmse)\n\n# Lasso Regression\n\nRidge RMSE: 4.928343737671815\n\nlasso = Lasso(alpha=1)\nlasso.fit(X_train, y_train)\n\nLasso(alpha=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso(alpha=1)\n\nlasso_preds = lasso.predict(X_test)\nlasso_rmse = mean_squared_error(y_test, lasso_preds, squared=False)\nprint(\"Lasso RMSE:\", lasso_rmse)\n\nLasso RMSE: 6.5330738340489605\n\ndel(ridge, ridge_preds, lasso, lasso_preds)\n\nWhile our ridge model achieved a comparable RMSE to our linear regression, our lasso model performed worse. This could be because the lasso model is shrinking important coefficients to zero.\n\n\nIn R, we use our recipe that we defined earlier, then create a linear regression model and a workflow. Our recipe object specifies the preprocessing steps required to transform the data, while the workflow includes both the preprocessing and the modeling steps. We add our recipe and model to our workflow, fit on the training data, and predict on the test data.\n\n# First we create the model\nlm_model <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  set_mode(\"regression\")\n\n\n# we then make a workflow, analogous to a pipe in python\nlm_workflow <- workflow() |> \n  add_recipe(rec) |> \n  add_model(lm_model)\n\n# fit to training\nlm_fit <- fit(lm_workflow, data = training)\n\n# predict on the test data\ntesting_preds_lr <- predict(lm_fit, testing)\n\n# Calculate and store the RMSE\nrmse_test_lr <- rmse(testing_preds_lr, truth = testing$DIC, estimate = .pred)\nrmse_lr <- rmse_test_lr$.estimate\n\nprint(paste0(\"RMSE on the holdout set: \", round(rmse_lr, 3)))\n\n[1] \"RMSE on the holdout set: 4.933\"\n\n# Create a scatter plot of the true DIC values vs predicted DIC values\nggplot(testing, aes(x = DIC, y = testing_preds_lr$.pred)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(\n    x = \"True DIC values\",\n    y = \"Predicted DIC values\",\n    title = \"True vs Predicted DIC values\"\n  ) +\n  annotate(\"text\", x = min(testing$DIC), y = max(testing$DIC), label = paste(\"RMSE:\", round(rmse_lr, 2)), hjust = 0, vjust = 1)\n\n\n\nrm(lm_model, lm_workflow, lm_fit, testing_preds_lr, rmse_test_lr)\n\nWe achieved a similar RMSE to our linear regression in python, which is reassuring. Now, lets try ridge and lasso penalties in R:\n\n# Ridge Regression, mixture=0 specifies that this is a ridge model\nridge_model <- linear_reg(penalty = 1, mixture = 0) %>%\n  set_engine(\"glmnet\") %>%\n  set_mode(\"regression\")\n\n# Ridge workflow, add our recipe and model\nridge_workflow <- workflow() |> \n  add_recipe(rec) |> \n  add_model(ridge_model)\n\n# Fit on the training, predict on the test data, store the RMSEs\nridge_fit <- fit(ridge_workflow, data = training)\nridge_testing_preds <- predict(ridge_fit, testing)\nridge_rmse <- as.numeric(rmse(ridge_testing_preds, truth = testing$DIC, estimate = .pred)[,3])\nprint(paste(\"Ridge RMSE:\", round(ridge_rmse, 2)))\n\n[1] \"Ridge RMSE: 7.89\"\n\n# Same approach for lasso regression, this time using mixture=1 for lasso\nlasso_model <- linear_reg(penalty = 1, mixture = 1) %>%\n  set_engine(\"glmnet\") %>%\n  set_mode(\"regression\")\n\n# workflow, same as before\nlasso_workflow <- workflow() |> \n  add_recipe(rec) |> \n  add_model(lasso_model)\n# fit on the training and predict on the test\nlasso_fit <- fit(lasso_workflow, data = training)\nlasso_testing_preds <- predict(lasso_fit, testing)\nlasso_rmse <- as.numeric(rmse(lasso_testing_preds, truth = testing$DIC, estimate = .pred)[,3])\nprint(paste(\"Lasso RMSE:\", round(lasso_rmse, 2)))\n\n[1] \"Lasso RMSE: 5.39\"\n\nrm(ridge_model, ridge_workflow, ridge_fit, lasso_model, lasso_workflow, lasso_fit, ridge_testing_preds, lasso_testing_preds)\n\nBoth of these models in R performed worse than our regular linear regression model.\n\n\n\nOur ridge and lasso models both performed slightly worse than our regular linear regression. This could be because we chose poor values of penalty, which we could address by tuning this hyperparameter. This could also simply be because our penalties might be penalizing coefficients that are actually important for predicting the outcome variable. This can happen when there is high correlation between the predictors.\n\n\nKNN\nThe next model we want to try is K-Nearest Neighbors. Usually, this algorithm is used for classification, and would not be my first choice for a regression task. In the context of classification, KNN is used to predict the category of an unknown data point based on the categories of its neighboring data points. Given a scatterplot of data points belonging to different classes (for example, measurements of tree height and leaf width of two distinct species), the algorithm considers a specified number of the closest points (neighbors) to the unknown data point. The unknown point is then assigned a category based on the majority class of these neighbors. KNN can also be applied to regression tasks, where the goal is to predict a continuous value instead of a discrete category. Here, the algorithm again looks at the specified number of nearest neighbors to the unknown data point. However, instead of determining the majority class, it computes the average of the target values of these neighbors. The unknown point is then assigned this average value as its prediction.\nWe are also tuning our first hyperparameter here: the number of neighbors we want to use. We’re going to use grid searching to find the optimal k, essentially making a grid of a range of values that we expect our optimal value for k to fall within. We search over this grid, test each of the values, and find the value that performs best on the training set. It is important to find the right k-value, as too small of a k will result in overfitting while too large of a k will result in underfitting. We will be using this approach with all models going forward.\n\nPythonR\n\n\nWe follow a slightly different workflow to our linear regression model in python now that we have a tuning parameter. We instantiate the model, specify the number of neighbors we want to test (typically the square root of the total number of observations in the dataset is a good rule of thumb)\n\n# Instantiate our knn regressor\nknn_regressor = KNeighborsRegressor()\n\n# Define the parameter grid for hyperparameter tuning - we want to test all numbers of n_neighbors from 1 to 34\nparam_grid = {'knn__n_neighbors': list(range(1, 35))}\n\n# Create a pipeline with StandardScaler and KNeighborsRegressor\nknn_pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('knn', knn_regressor)\n])\n\n# Perform GridSearchCV for hyperparameter tuning\ngrid_search = GridSearchCV(knn_pipeline, param_grid, cv=5, scoring='neg_root_mean_squared_error')\n\n# fit on the training\ngrid_search.fit(X_train, y_train)  \n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('knn', KNeighborsRegressor())]),\n             param_grid={'knn__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,\n                                              12, 13, 14, 15, 16, 17, 18, 19,\n                                              20, 21, 22, 23, 24, 25, 26, 27,\n                                              28, 29, 30, ...]},\n             scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('knn', KNeighborsRegressor())]),\n             param_grid={'knn__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,\n                                              12, 13, 14, 15, 16, 17, 18, 19,\n                                              20, 21, 22, 23, 24, 25, 26, 27,\n                                              28, 29, 30, ...]},\n             scoring='neg_root_mean_squared_error')estimator: PipelinePipeline(steps=[('scaler', StandardScaler()), ('knn', KNeighborsRegressor())])StandardScalerStandardScaler()KNeighborsRegressorKNeighborsRegressor()\n\n\nWe can take a look at how our RMSE changes with different values of k:\n\n# first, clear the figure\nplt.clf()\n# Get the mean cross-validated scores for each n_neighbors\ncv_scores = grid_search.cv_results_['mean_test_score']  \n\ncv_scores = -cv_scores\n\n# Plot RMSE vs. n_neighbors\nplt.figure(figsize=(8, 6))\nplt.plot(list(range(1, 35)), cv_scores, marker='o')\nplt.xlabel('Number of Neighbors (n_neighbors)')\nplt.ylabel('Mean Cross-Validated RMSE')\nplt.title('RMSE vs. n_neighbors')\nplt.grid()\nplt.show()\n\n\n\n\nIt looks like our best value for k is 3.\n\n# fit on the training\ngrid_search.fit(X_train, y_train)\n\n# Retrieve the best KNN model\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('knn', KNeighborsRegressor())]),\n             param_grid={'knn__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,\n                                              12, 13, 14, 15, 16, 17, 18, 19,\n                                              20, 21, 22, 23, 24, 25, 26, 27,\n                                              28, 29, 30, ...]},\n             scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('knn', KNeighborsRegressor())]),\n             param_grid={'knn__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,\n                                              12, 13, 14, 15, 16, 17, 18, 19,\n                                              20, 21, 22, 23, 24, 25, 26, 27,\n                                              28, 29, 30, ...]},\n             scoring='neg_root_mean_squared_error')estimator: PipelinePipeline(steps=[('scaler', StandardScaler()), ('knn', KNeighborsRegressor())])StandardScalerStandardScaler()KNeighborsRegressorKNeighborsRegressor()\n\nbest_knn = grid_search.best_estimator_\n\n# predict on the test set\ny_pred = grid_search.predict(X_test)\n\n# get our RMSE\nrmse_knn = mean_squared_error(y_test, y_pred, squared=False)\n\nprint(\"Root Mean Squared Error: \", rmse_knn)\n\nRoot Mean Squared Error:  11.332377692549752\n\ndel(knn_regressor, param_grid, grid_search, best_knn, y_pred)\n\nRMSE is noticeably worse than our regularized and ordinary linear regression models.\n\n\nWe follow the same approach in R, this time specifying in the model specification that we want to tune the number of neighbors. We make a grid of possible values for our number of neighbors parameter, find the optimal value, then use this value to fit on the training and predict on the test set.\n\n# Make our model\nknn_spec <- nearest_neighbor(neighbors = tune()) |> \n  set_engine(\"kknn\") |> \n  set_mode(\"regression\")\n\n# Create a grid\nk_grid <- grid_regular(\n  neighbors(range = c(1, 34)),\n  levels = 30\n)\n\n# make our workflow\nknn_workflow <- workflow() |> \n  add_model(knn_spec) |> \n  add_recipe(rec)\n\n# tune it!\nknn_tuned <- tune_grid(\n  knn_workflow,\n  resamples = folds,\n  grid = k_grid,\n  metrics = metric_set(rmse)\n)\n\n# Get our optimal value for k\nbest_k <- knn_tuned |> \n  collect_metrics() |> \n  filter(.metric == \"rmse\") |> \n  arrange(mean) |> \n  slice(1) |> \n  pull(neighbors)\n\nWe can also check how our RMSE changes with different values of our n_neighbors hyperparameter in R.\n\n# Get the RMSE for each n_neighbors from the tuning results\nknn_tuning_metrics <- knn_tuned |> \n  collect_metrics() |> \n  filter(.metric == \"rmse\")\n\n# Plot RMSE vs. n_neighbors\nggplot(knn_tuning_metrics, aes(x = neighbors, y = mean)) +\n  geom_point() +\n  geom_line() +\n  theme_minimal() +\n  labs(\n    title = \"RMSE vs. n_neighbors\",\n    x = \"Number of Neighbors (n_neighbors)\",\n    y = \"Mean Cross-Validated RMSE\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\nOur best value for K looks to be 5 in R.\n\n# Update our model using our best k\nknn_spec_best <- nearest_neighbor(neighbors = best_k) |> \n  set_engine(\"kknn\") |> \n  set_mode(\"regression\")\n\n# Fit the final model on the whole training set\nknn_final <- knn_workflow |> \n  update_model(knn_spec_best) |> \n  fit(data = training)\n\n# Make predictions on the test set\nknn_testing_preds <- predict(knn_final, testing) |> \n  bind_cols(testing) |> \n  mutate(truth = as.double(DIC), estimate = as.double(.pred)) |> \n  metrics(truth = truth, estimate = estimate)\n\n# RMSE\nrmse_knn <- knn_testing_preds |> \n  filter(.metric == \"rmse\") |> \n  pull(.estimate)\n\nprint(paste0(\"Our KNN RMSE is: \", rmse_knn))\n\n[1] \"Our KNN RMSE is: 10.8203934359629\"\n\nrm(knn_spec, knn_spec_best, best_k, knn_final, knn_testing_preds, knn_tuned, k_grid, knn_workflow)\n\nAgain, our KNN model performs poorly here.\n\n\n\nOur RMSE in both of these KNN models were noticeably worse than any of the ordinary linear regression models. No surprises here, this is not a scenario in which we would typically use KNN. Now, we’re going to move on to the tree-based models.\n\n\nDecision Tree\nThe next algorithm we’re going to try is the decision tree. Decision trees are a popular algorithm used for both classification and regression tasks. They work by recursively splitting the input data into subsets based on the values of the input features, giving you a tree like structure. Each node in the tree represents a decision rule or condition based on the feature values, while the leaf/terminal nodes represent the final predicted class or value. The process of building a decision tree involves selecting the best feature to split the data at each node, usually based on a criterion such as information gain (for classification) or mean squared error (for regression). The tree continues to grow by making further splits until a stopping criterion is met, such as a minimum number of samples per leaf, a maximum depth of the tree, or a threshold for the improvement in the splitting criterion. A much more comprehensive explanation is here.\nDecision trees are great, but can be prone to overfitting. Fortunately, we have hyperparameters to combat this. We’re tuning 3 hyperparameters in this model. The cost complexity penalty, the maximum tree depth, and the minimum leaf samples. The cost complexity is used to “prune” the trees - reducing overfitting by removing branches of the decision tree that do not improve the accuracy of the model. This is similar to our alpha parameter in Lasso/Ridge regression, but penalizes the tree for having too many nodes. The maximum tree depth hyperparameter sets the maximum depth of the decision tree, or the number of levels the tree can have. A deeper tree is more likely to be overfitting the training data. Finally, the minimum number of samples required to be in a leaf node hyperparameter also helps us prevent overfitting and controlling tree depth by requiring a minimum number of samples in each leaf node.\nIt should be noted, unlike our KNN model, decision trees and their ensembles like random forests and bagged trees are generally not very sensitive to the scale of the input features because they make decisions based on the relative ordering of feature values, rather than the magnitudes of those values. But since we’re using the same recipe object rec for each model in R, all of our features are automatically scaled. We will use the same approach in python, by explicitly creating a Pipeline object with a scaler inside of it for each model.\n\nPythonR\n\n\nWe’re introducting our first pipeline object here. This object is analogous to a workflow in R. We pass it the preprocessing steps we want to use on our data, and our model. We fit this pipeline object to the training data, and predict on the test data.\n\n\n# Create a pipeline with a scaler and the decision tree regressor\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('dt_regressor', DecisionTreeRegressor(random_state=4))\n])\n\n# Define the parameter grid for hyperparameter tuning\nparam_grid = {\n    'dt_regressor__ccp_alpha': np.logspace(-4, 0, 20), # np.logspace(-4, 0, 5)\n    'dt_regressor__max_depth': list(range(1, 13)), # list(range(1, 21))\n    'dt_regressor__min_samples_leaf': list(range(1, 13)) # list(range(1, 21))\n}\n\n# Perform GridSearchCV for hyperparameter tuning using the pipeline\ngrid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('dt_regressor',\n                                        DecisionTreeRegressor(random_state=4))]),\n             n_jobs=-1,\n             param_grid={'dt_regressor__ccp_alpha': array([1.00000000e-04, 1.62377674e-04, 2.63665090e-04, 4.28133240e-04,\n       6.95192796e-04, 1.12883789e-03, 1.83298071e-03, 2.97635144e-03,\n       4.83293024e-03, 7.84759970e-03, 1.27427499e-02, 2.06913808e-02,\n       3.35981829e-02, 5.45559478e-02, 8.85866790e-02, 1.43844989e-01,\n       2.33572147e-01, 3.79269019e-01, 6.15848211e-01, 1.00000000e+00]),\n                         'dt_regressor__max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n                                                     10, 11, 12],\n                         'dt_regressor__min_samples_leaf': [1, 2, 3, 4, 5, 6, 7,\n                                                            8, 9, 10, 11, 12]},\n             scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('dt_regressor',\n                                        DecisionTreeRegressor(random_state=4))]),\n             n_jobs=-1,\n             param_grid={'dt_regressor__ccp_alpha': array([1.00000000e-04, 1.62377674e-04, 2.63665090e-04, 4.28133240e-04,\n       6.95192796e-04, 1.12883789e-03, 1.83298071e-03, 2.97635144e-03,\n       4.83293024e-03, 7.84759970e-03, 1.27427499e-02, 2.06913808e-02,\n       3.35981829e-02, 5.45559478e-02, 8.85866790e-02, 1.43844989e-01,\n       2.33572147e-01, 3.79269019e-01, 6.15848211e-01, 1.00000000e+00]),\n                         'dt_regressor__max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n                                                     10, 11, 12],\n                         'dt_regressor__min_samples_leaf': [1, 2, 3, 4, 5, 6, 7,\n                                                            8, 9, 10, 11, 12]},\n             scoring='neg_root_mean_squared_error')estimator: PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('dt_regressor', DecisionTreeRegressor(random_state=4))])StandardScalerStandardScaler()DecisionTreeRegressorDecisionTreeRegressor(random_state=4)\n\n\n\n# Make predictions and evaluate the model\ny_pred = grid_search.predict(X_test)\ndt_rmse = mean_squared_error(y_test, y_pred, squared=False)\nprint(\"Best Decision Tree parameters: \", grid_search.best_params_)\n\nBest Decision Tree parameters:  {'dt_regressor__ccp_alpha': 0.03359818286283781, 'dt_regressor__max_depth': 11, 'dt_regressor__min_samples_leaf': 5}\n\nprint(\"RMSE: \", dt_rmse)\n\n\n# Clear our environment\n\nRMSE:  6.711271577254955\n\ndel(pipe, param_grid, grid_search, y_pred)\n\nOur RMSE for this decision tree algorithm is 6.7. Much better than the KNN regressor!\n\n\nWe follow the same approach as we did for KNN in R. We specify the parameters we want to tune in the model specification, make a grid of values to try, and select the best values for our final model.\n\n# decision tree model specification, specifying our hyperparameters to tune. \ndt_spec <- decision_tree(\n    mode = \"regression\",\n    cost_complexity = tune(),\n    tree_depth = tune(),\n    min_n = tune()\n  ) |> \n  set_engine(\"rpart\") |> \n  set_mode(\"regression\")\n\n# Define the parameter grid for hyperparameter tuning\nparam_grid <- grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 5)\n\n# workflow\ndt_workflow <- workflow() |> \n  add_model(dt_spec) |> \n  add_recipe(rec)\n\n\n# tune our parameters\nregisterDoParallel(cores = n_cores)\ndt_tuned <- tune_grid(\n  dt_workflow,\n  resamples = folds,\n  grid = param_grid,\n  metrics = metric_set(rmse),\n  control = control_grid(save_pred = TRUE, parallel_over = \"everything\")\n)\n\n# extract the best values\nbest_params <- dt_tuned |> \n  show_best(\"rmse\") |> \n  slice(1) |> \n  select(cost_complexity, tree_depth, min_n) \n\nbest_cc <- best_params$cost_complexity\nbest_depth <- best_params$tree_depth\nbest_min_n <- best_params$min_n\n\nprint(paste0(\"Best cost complexity parameter: \", best_cc))\n\n[1] \"Best cost complexity parameter: 1e-10\"\n\nprint(paste0(\"Best maximum tree depth: \", best_depth))\n\n[1] \"Best maximum tree depth: 15\"\n\nprint(paste0(\"Best minimum number of samples in a leaf: \", best_min_n))\n\n[1] \"Best minimum number of samples in a leaf: 11\"\n\n\nWe can also use this autoplot function to visualize our best hyperparameter combinations:\n\nautoplot(dt_tuned)\n\n\n\n\nWe now use these best values in our final model:\n\n# Create the final decision tree model with the best hyperparameters\ndt_final <- decision_tree(\n  cost_complexity = best_cc,\n  tree_depth = best_depth,\n  min_n = best_min_n\n) |> \n  set_engine(\"rpart\") |> \n  set_mode(\"regression\")\n\n\n# fit to the training\ndt_final_fit <- dt_workflow |> \n  update_model(dt_final) |> \n  fit(data = training)\n\n# predict on the test set\ndt_testing_preds <- predict(dt_final_fit, testing) |> \n  bind_cols(testing) |> \n  mutate(truth = as.double(DIC), estimate = as.double(.pred)) |> \n  metrics(truth = truth, estimate = estimate)\n\ndt_rmse <- dt_testing_preds |>  filter(.metric == \"rmse\") |>  pull(.estimate)\n\nprint(best_params)\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth min_n\n            <dbl>      <int> <int>\n1    0.0000000001         15    11\n\nprint(paste0(\"RMSE:\", dt_rmse))\n\n[1] \"RMSE:6.6869703721755\"\n\nrm(dt_testing_preds, dt_tuned, dt_final_fit, dt_final, dt_spec, best_params, dt_workflow)\n\nAgain, our decision tree in R performs much better than the KNN algorithm we used.\n\n\n\nThe decision tree model will be the foundation for the next 3 models we are going to try. Bagged Trees, Random Forests, and Gradient Boosting all use decision trees as their base learner, but with slight adjustments to how the final model is constructed.\n\n\nBagged Trees\nOur normal decision tree algorithm worked well, but there are some significant improvements we can make to this algorithm. Bagging, or bootstrap aggregation is a technique that creates multiple models and then combines their predictions into a final output. Bootstrapping involves drawing samples from a dataset with replacement, to create new samples that are of the same size as the original dataset. By generating multiple samples of the data and predict the statistic of interest for each sample, we can aggregate these predictions into a final output that typically generalizes to unseen data much better than a model trained on a single dataset.\nIn the context of decision trees, this is an ensemble method that employs the bagging technique by training multiple decision trees on different subsets of the training data. In our case, the new predictions are made by averaging the predictions together from the individual base learners. This technique works particularly well on decision trees, since single decision trees are high variance learners. We’re creating multiple decision trees on different subsets of the training data. Our model’s stability and robustness are increased since the predictions are less sensitive to small changes in the training data. In other words, the variance of the model is reduced, which greatly helps us with avoiding overfitting. Bagged Trees also provide information about the importance of each feature in the model, because each tree is trained on a random subset of the features. This means that each tree will give different weights to different features, which can then be used to identify the most important ones.\nWe have an additional hyperparameter to train here: the number of trees we want to generate the final bagged model from. Aside from this, the code is much the same as regular decision trees.\n\nPythonR\n\n\nWe specify the n_estimators parameter here to be 500.\n\n\n# create a pipeline with a scaler and the bagged decision tree model\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('bagged_dt', BaggingRegressor(estimator=DecisionTreeRegressor(random_state=4), n_estimators=50, random_state=4)) #500\n])\n\n# define our grid\nparam_grid = {\n    'bagged_dt__estimator__ccp_alpha': np.logspace(-4, 0, 5), # np.logspace(-4, 0, 20),\n    'bagged_dt__estimator__max_depth': list(range(1, 13)), # list(range(1, 21)),\n    'bagged_dt__estimator__min_samples_leaf': list(range(1, 13)) # list(range(1, 21))\n}\n\n# now lets search our grid for our optimal values\ngrid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('bagged_dt',\n                                        BaggingRegressor(estimator=DecisionTreeRegressor(random_state=4),\n                                                         n_estimators=50,\n                                                         random_state=4))]),\n             n_jobs=-1,\n             param_grid={'bagged_dt__estimator__ccp_alpha': array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00]),\n                         'bagged_dt__estimator__max_depth': [1, 2, 3, 4, 5, 6,\n                                                             7, 8, 9, 10, 11,\n                                                             12],\n                         'bagged_dt__estimator__min_samples_leaf': [1, 2, 3, 4,\n                                                                    5, 6, 7, 8,\n                                                                    9, 10, 11,\n                                                                    12]},\n             scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('bagged_dt',\n                                        BaggingRegressor(estimator=DecisionTreeRegressor(random_state=4),\n                                                         n_estimators=50,\n                                                         random_state=4))]),\n             n_jobs=-1,\n             param_grid={'bagged_dt__estimator__ccp_alpha': array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00]),\n                         'bagged_dt__estimator__max_depth': [1, 2, 3, 4, 5, 6,\n                                                             7, 8, 9, 10, 11,\n                                                             12],\n                         'bagged_dt__estimator__min_samples_leaf': [1, 2, 3, 4,\n                                                                    5, 6, 7, 8,\n                                                                    9, 10, 11,\n                                                                    12]},\n             scoring='neg_root_mean_squared_error')estimator: PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('bagged_dt',\n                 BaggingRegressor(estimator=DecisionTreeRegressor(random_state=4),\n                                  n_estimators=50, random_state=4))])StandardScalerStandardScaler()bagged_dt: BaggingRegressorBaggingRegressor(estimator=DecisionTreeRegressor(random_state=4),\n                 n_estimators=50, random_state=4)estimator: DecisionTreeRegressorDecisionTreeRegressor(random_state=4)DecisionTreeRegressorDecisionTreeRegressor(random_state=4)\n\n\n\n# predict on the test set\ny_pred = grid_search.predict(X_test)\nbag_tree_rmse = mean_squared_error(y_test, y_pred, squared=False)\nprint(\"Best Bagged Decision Tree parameters: \", grid_search.best_params_)\n\nBest Bagged Decision Tree parameters:  {'bagged_dt__estimator__ccp_alpha': 0.001, 'bagged_dt__estimator__max_depth': 12, 'bagged_dt__estimator__min_samples_leaf': 2}\n\nprint(\"Bagged Decision Tree RMSE: \", bag_tree_rmse)\n\nBagged Decision Tree RMSE:  4.996459665683262\n\ndel(pipe, param_grid, grid_search, y_pred)\n\nOur RMSE is improving!\n\n\nAs usual, we follow the same approach in R. We specify the number of trees we want to use in the final model using times=500.\n\n# model specification\nbagdt_spec <- bag_tree(\n    mode = \"regression\",\n    tree_depth = tune(),\n    min_n = tune(),\n    cost_complexity = tune()\n  ) %>%\n  set_engine(\"rpart\", times = 50) |> # times = 500\n  set_mode(\"regression\")\n\n# checking out our hyperparameters\nbagdt_params <- parameters(cost_complexity(), tree_depth(), min_n())\nbagdt_grid <- grid_max_entropy(bagdt_params, size = 10, iter = 5)\n\n# make a workflow\nbagdt_wf <- workflow() |>\n  add_model(bagdt_spec) |>\n  add_recipe(rec)\n\n# Tuning\nn_cores <- detectCores() - 1 # Use all available cores except one\nregisterDoParallel(cores = n_cores)\nbagdt_rs <- tune_grid(\n  bagdt_wf,\n  resamples = folds,\n  grid = bagdt_grid,\n  metrics = metric_set(yardstick::rmse),\n  control = control_grid(save_pred = TRUE, parallel_over = \"everything\")\n)\n\n# Get the best hyperparameters\nbest_bagdt_params <- bagdt_rs |> \n  show_best(\"rmse\") |> \n  slice(1) |> \n  select(cost_complexity, tree_depth, min_n)\n\nbest_cc_bagdt <- best_bagdt_params$cost_complexity\nbest_depth_bagdt <- best_bagdt_params$tree_depth\nbest_min_n_bagdt <- best_bagdt_params$min_n\n\nshow_best(bagdt_rs)\n\n# A tibble: 5 × 9\n  cost_complexity tree_depth min_n .metric .estima…¹  mean     n std_err .config\n            <dbl>      <int> <int> <chr>   <chr>     <dbl> <int>   <dbl> <chr>  \n1        1.04e- 5          8     3 rmse    standard   6.35     5   0.588 Prepro…\n2        4.19e- 8          6     7 rmse    standard   6.41     5   0.571 Prepro…\n3        1.21e- 5          7    26 rmse    standard   6.42     5   0.581 Prepro…\n4        2.17e- 6          6     5 rmse    standard   6.47     5   0.557 Prepro…\n5        8.60e-10         12    34 rmse    standard   6.51     5   0.566 Prepro…\n# … with abbreviated variable name ¹​.estimator\n\n\nWe can again use the handy autoplot() function to visualize our best hyperparameter combinations:\n\nautoplot(bagdt_rs)\n\n\n\n\nAnd finally, we use our best hyperparameter values to construct our final model.\n\n# Create the final decision tree model with the best hyperparameters\nbagdt_final <- bag_tree(\n  cost_complexity = best_cc_bagdt,\n  tree_depth = best_depth_bagdt,\n  min_n = best_min_n_bagdt\n) %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"regression\")\n\n\n# Fit the final model on the whole training set\nbagdt_final_fit <- bagdt_wf %>%\n  update_model(bagdt_final) %>%\n  fit(data = training)\n\n# Make predictions on the holdout set and compute the RMSE\nbagdt_testing_preds <- predict(bagdt_final_fit, testing) |> \n  bind_cols(testing) |> \n  mutate(truth = as.double(DIC), estimate = as.double(.pred)) |> \n  metrics(truth = truth, estimate = estimate)\n\nbagdt_rmse <- bagdt_testing_preds %>% filter(.metric == \"rmse\") |>  pull(.estimate)\nprint(paste0(\"Bagged Tree RMSE:\", bagdt_rmse))\n\n[1] \"Bagged Tree RMSE:5.94843583170231\"\n\nrm(bagdt_final, bagdt_final_fit, bagdt_grid, bagdt_params, bagdt_rs, bagdt_spec, bagdt_testing_preds, bagdt_wf, best_bagdt_params)\n\n\n\n\nOur RMSE is getting better! Bagging our decision trees help us combat the overfitting that ordinary decision trees are prone to. However, there are still a few more ways we can improve this model!\n\n\nRandom Forest\nOur Bagged Tree RMSE was better than our normal decision tree algorithm, but we can still make this better! Bagging aggregates the predictions across all the trees, which reduces the variance of the overall procedure and results in improved predictive performance. However, simply bagging trees results in tree correlation that limits the effect of variance reduction. Random forests are built using the same fundamental principles as decision trees and bagging, but inject further randomness in the process which further reduces the variance.\nRandom forests help to reduce tree correlation by injecting more randomness into the tree-growing process. Specifically, while growing a decision tree during the bagging process, random forests specify a subset of the features to be used in each tree. This parameter is called m_try, and it controls the number of features to consider when randomly selecting a subset of features for each tree. For example, if there are 10 input features and mtry is set to 3, then for each tree in the Random Forest, a random subset of 3 features will be selected from the 10 input features. By controlling the number of features used for each tree, mtry helps to reduce the correlation between the trees and increase the diversity of the Random Forest. The optimal value for m_try depends on the problem, but a standard value for regression tasks ismtry=(p/3), where p is the number of features present in our dataset.\n\nPythonR\n\n\n\n# Create a pipeline with a scaler and the random forest regressor\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('rf_regressor', RandomForestRegressor(random_state=4))\n])\n\n# Define the parameter grid for hyperparameter tuning\nparam_grid = {\n    'rf_regressor__n_estimators': list(range(100, 1000, 450)),  # number of trees list(range(100, 1000, 50))\n    'rf_regressor__max_features': list(range(1, X_train.shape[1] - 10)),  # mtry\n    'rf_regressor__min_samples_leaf': list(range(1, 10))  # min_n\n}\n\n# Perform GridSearchCV for hyperparameter tuning\ngrid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\n# Make predictions and evaluate the model\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('rf_regressor',\n                                        RandomForestRegressor(random_state=4))]),\n             n_jobs=-1,\n             param_grid={'rf_regressor__max_features': [1, 2, 3, 4, 5, 6, 7, 8],\n                         'rf_regressor__min_samples_leaf': [1, 2, 3, 4, 5, 6, 7,\n                                                            8, 9],\n                         'rf_regressor__n_estimators': [100, 550]},\n             scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('rf_regressor',\n                                        RandomForestRegressor(random_state=4))]),\n             n_jobs=-1,\n             param_grid={'rf_regressor__max_features': [1, 2, 3, 4, 5, 6, 7, 8],\n                         'rf_regressor__min_samples_leaf': [1, 2, 3, 4, 5, 6, 7,\n                                                            8, 9],\n                         'rf_regressor__n_estimators': [100, 550]},\n             scoring='neg_root_mean_squared_error')estimator: PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('rf_regressor', RandomForestRegressor(random_state=4))])StandardScalerStandardScaler()RandomForestRegressorRandomForestRegressor(random_state=4)\n\ny_pred = grid_search.predict(X_test)\nrf_rmse = mean_squared_error(y_test, y_pred, squared=False)\nprint(\"Best Random Forest parameters: \", grid_search.best_params_)\n\nBest Random Forest parameters:  {'rf_regressor__max_features': 6, 'rf_regressor__min_samples_leaf': 1, 'rf_regressor__n_estimators': 550}\n\nprint(\"RMSE: \", rf_rmse)\n\n# Clear our environment\n\nRMSE:  5.059267757009525\n\ndel(pipe, param_grid, grid_search, y_pred)\n\nOur RMSE is still improving!\n\n\nWe follow the same approach in R:\n\n# specifying tuning parameters\nrandfor_spec <- rand_forest(\n  trees = tune(),\n  mtry = tune(),\n  min_n = tune()\n) |>\n  set_mode(\"regression\") |>\n  set_engine(\"ranger\")\n\n# Looking at hyperparameters\nrandf_grid <-grid_regular(trees(), min_n(), mtry(range(1:13)), levels = 5)\n\n# Define new workflow\nrf_workflow <- workflow() |>\n  add_model(randfor_spec) |>\n  add_recipe(rec)\n\n# Tuning\ndoParallel::registerDoParallel()\nrandf_rs <- tune_grid(\n  rf_workflow,\n  resamples = folds,\n  grid = randf_grid,\n  metrics = metric_set(yardstick::rmse),\n  control = control_grid(save_pred = TRUE, parallel_over = \"everything\")\n)\n\nAs always, lets take a look at our best hyperparameter combinations:\n\nautoplot(randf_rs)\n\n\n\n\n\n# Get the best hyperparameters\nbest_randf_params <- randf_rs |> \n  show_best(\"rmse\") |> \n  slice(1) |> \n  select(trees, mtry, min_n)\n\nbest_trees_rf <- best_randf_params$trees\nbest_mtry_rf <- best_randf_params$mtry\nbest_min_n_rf <- best_randf_params$min_n\n\nshow_best(randf_rs)\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     7  1500     2 rmse    standard    5.68     5   0.577 Preprocessor1_Model0…\n2     7  2000     2 rmse    standard    5.68     5   0.576 Preprocessor1_Model0…\n3     7  1000     2 rmse    standard    5.68     5   0.573 Preprocessor1_Model0…\n4     7   500     2 rmse    standard    5.69     5   0.573 Preprocessor1_Model0…\n5     4  1500     2 rmse    standard    5.69     5   0.556 Preprocessor1_Model0…\n\n\n\n# Create the final random forest model with the best hyperparameters\nrandf_final <- rand_forest(\n  trees = best_trees_rf,\n  mtry = best_mtry_rf,\n  min_n = best_min_n_rf\n) |>\n  set_mode(\"regression\") |>\n  set_engine(\"ranger\")\n\n\n# fit on the training\nrandf_final_fit <- rf_workflow |> \n  update_model(randf_final) |> \n  fit(data = training)\n\n# predict on the test, calculate RMSE\nrf_testing_preds <- predict(randf_final_fit, testing) |> \n  bind_cols(testing) |> \n  mutate(truth = as.double(DIC), estimate = as.double(.pred)) |> \n  metrics(truth = truth, estimate = estimate)\n\nrf_rmse <- rf_testing_preds %>% filter(.metric == \"rmse\") |>  pull(.estimate)\n\nrm(randf_grid, randf_rs, randfor_spec, rf_workflow)\nprint(paste0(\"Random Forest RMSE:\", rf_rmse))\n\n[1] \"Random Forest RMSE:5.047782925692\"\n\n\n\n\n\nOur model is still improving, but there is another powerful adjustment we can make to it.\n\n\nGradient Boosting\nWhereas random forests build an ensemble of deep independent trees, Gradient Boosting Machines (GBMS) build an ensemble of shallow trees in sequence, with each tree learning and improving on the previous one. Independently, these trees are relatively weak predictive models, but they can be boosted to produce a powerful ensemble of trees.\nThe boosting idea is to add new models to the ensemble sequentially. We start with a weak model, and sequentially boost its performance by continuing to build new trees, where each new tree in the sequence remedies where the previous one made had the largest prediction errors. To quantify the amount of information gained by each new tree, we use a loss function, which measures the difference between the predicted values and the true values for the training examples. The goal is to minimize this function, which means that we want to find the set of parameters for each new tree that best fit the data. To find the optimal set of parameters, we use gradient descent, which is an optimization algorithm that works by iteratively improving the parameters of the model to minimize the loss function. At each iteration, we calculate the gradient of the loss function with respect to the model parameters, and we use this gradient to update the parameters in a direction that minimizes the loss function. We continue this process until we reach a minimum value of the loss function or a stopping criterion is met. The important hyperparameter we use for this is known as the learning rate. The learning rate specifies how fast we want to descend down the gradient. Too large, and we might miss the minimum of the loss function. Too small, and we’ll need to many iterations to find the minimum.\nWe’re going to take a different approach to tuning this model. Instead of specifying all the parameters we want to tune in one step, we’re going to break it up. First, we’ll find the optimal learning rate. Based off the optimal learning rate, we’ll then tune the tree specific parameters (min_n/xgb__min_child_weight, tree_depth/xgb__max_depth, loss_reduction/xgb__gamma, xgb__n_estimators in python, we’ll set the number of trees manually in R). Then, based off these optimal values of the tree specific parameters, we’ll tune the stochastic parameters (sample_size/xgb__subsample, mtry/xgb__colsample_bytree).\n\nPythonR\n\n\nInstead of using sklearn for this algorithm in python, we’re going to use the XGB library. This is a library built specifically for gradient boosting, and will increase our computational efficiency.\n\n# instantiate our GBM\nxgb = XGBRegressor()\n\n# make our pipeline\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('xgb', xgb)\n])\n# First, tune the learning rate\nlearning_rates = np.linspace(0.01, 0.6, 10) # np.linspace(0.01, 0.6, 100)\nparam_grid_learning_rate = {\n    'xgb__learning_rate': learning_rates\n}\n\ngrid_search_lr = GridSearchCV(\n    pipeline, param_grid_learning_rate, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1\n)\ngrid_search_lr.fit(X_train, y_train)\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('xgb',\n                                        XGBRegressor(base_score=None,\n                                                     booster=None,\n                                                     callbacks=None,\n                                                     colsample_bylevel=None,\n                                                     colsample_bynode=None,\n                                                     colsample_bytree=None,\n                                                     early_stopping_rounds=None,\n                                                     enable_categorical=False,\n                                                     eval_metric=None,\n                                                     feature_types=None,\n                                                     gamma=None, gpu_id=None,\n                                                     grow_policy=None,\n                                                     importance_type=Non...\n                                                     max_leaves=None,\n                                                     min_child_weight=None,\n                                                     missing=nan,\n                                                     monotone_constraints=None,\n                                                     n_estimators=100,\n                                                     n_jobs=None,\n                                                     num_parallel_tree=None,\n                                                     predictor=None,\n                                                     random_state=None, ...))]),\n             n_jobs=-1,\n             param_grid={'xgb__learning_rate': array([0.01      , 0.07555556, 0.14111111, 0.20666667, 0.27222222,\n       0.33777778, 0.40333333, 0.46888889, 0.53444444, 0.6       ])},\n             scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('xgb',\n                                        XGBRegressor(base_score=None,\n                                                     booster=None,\n                                                     callbacks=None,\n                                                     colsample_bylevel=None,\n                                                     colsample_bynode=None,\n                                                     colsample_bytree=None,\n                                                     early_stopping_rounds=None,\n                                                     enable_categorical=False,\n                                                     eval_metric=None,\n                                                     feature_types=None,\n                                                     gamma=None, gpu_id=None,\n                                                     grow_policy=None,\n                                                     importance_type=Non...\n                                                     max_leaves=None,\n                                                     min_child_weight=None,\n                                                     missing=nan,\n                                                     monotone_constraints=None,\n                                                     n_estimators=100,\n                                                     n_jobs=None,\n                                                     num_parallel_tree=None,\n                                                     predictor=None,\n                                                     random_state=None, ...))]),\n             n_jobs=-1,\n             param_grid={'xgb__learning_rate': array([0.01      , 0.07555556, 0.14111111, 0.20666667, 0.27222222,\n       0.33777778, 0.40333333, 0.46888889, 0.53444444, 0.6       ])},\n             scoring='neg_root_mean_squared_error')estimator: PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('xgb',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, gamma=None, gpu_id=None,\n                              grow_policy=None, importance_type=None,\n                              interaction_constraints=None, learning_rate=None,\n                              max_bin=None, max_cat_threshold=None,\n                              max_cat_to_onehot=None, max_delta_step=None,\n                              max_depth=None, max_leaves=None,\n                              min_child_weight=None, missing=nan,\n                              monotone_constraints=None, n_estimators=100,\n                              n_jobs=None, num_parallel_tree=None,\n                              predictor=None, random_state=None, ...))])StandardScalerStandardScaler()XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=None, ...)\n\nbest_learning_rate = grid_search_lr.best_params_['xgb__learning_rate']\nprint(f\"Best learning rate: {best_learning_rate}\")\n\nBest learning rate: 0.33777777777777773\n\n\nNow, with our best learning rate, we’ll tune the tree-specific parameters. Note that the names of these hyperparameters have changed slightly since we’re using a different package.\n\n# Tree-specific parameters\nparam_grid_tree = {\n    'xgb__min_child_weight': range(1, 11, 3), # range(1, 21, 1),\n    'xgb__max_depth': range(1, 11, 3), # range(1, 21, 1),\n    'xgb__gamma': np.linspace(0, 1, 5), # np.linspace(0, 1, 20),\n    'xgb__n_estimators': range(100, 1500, 700)  # adjust the number of trees np.linspace(100, 1500, 30) \n}\n\n# Set the learning rate \npipeline.set_params(xgb__learning_rate=best_learning_rate)\n\n# find our best tree-specific parameters\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('xgb',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, gamma=None, gpu_id=None,\n                              grow_policy=None, importance_type=None,\n                              interaction_constraints=None,\n                              learning_rate=0.33777777777777773, max_bin=None,\n                              max_cat_threshold=None, max_cat_to_onehot=None,\n                              max_delta_step=None, max_depth=None,\n                              max_leaves=None, min_child_weight=None,\n                              missing=nan, monotone_constraints=None,\n                              n_estimators=100, n_jobs=None,\n                              num_parallel_tree=None, predictor=None,\n                              random_state=None, ...))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('xgb',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, gamma=None, gpu_id=None,\n                              grow_policy=None, importance_type=None,\n                              interaction_constraints=None,\n                              learning_rate=0.33777777777777773, max_bin=None,\n                              max_cat_threshold=None, max_cat_to_onehot=None,\n                              max_delta_step=None, max_depth=None,\n                              max_leaves=None, min_child_weight=None,\n                              missing=nan, monotone_constraints=None,\n                              n_estimators=100, n_jobs=None,\n                              num_parallel_tree=None, predictor=None,\n                              random_state=None, ...))])StandardScalerStandardScaler()XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.33777777777777773,\n             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=None, ...)\n\ngrid_search_tree = GridSearchCV(\n    pipeline, param_grid_tree, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1\n)\ngrid_search_tree.fit(X_train, y_train)\n\n# Get the best tree-specific parameters\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('xgb',\n                                        XGBRegressor(base_score=None,\n                                                     booster=None,\n                                                     callbacks=None,\n                                                     colsample_bylevel=None,\n                                                     colsample_bynode=None,\n                                                     colsample_bytree=None,\n                                                     early_stopping_rounds=None,\n                                                     enable_categorical=False,\n                                                     eval_metric=None,\n                                                     feature_types=None,\n                                                     gamma=None, gpu_id=None,\n                                                     grow_policy=None,\n                                                     importance_type=Non...\n                                                     min_child_weight=None,\n                                                     missing=nan,\n                                                     monotone_constraints=None,\n                                                     n_estimators=100,\n                                                     n_jobs=None,\n                                                     num_parallel_tree=None,\n                                                     predictor=None,\n                                                     random_state=None, ...))]),\n             n_jobs=-1,\n             param_grid={'xgb__gamma': array([0.  , 0.25, 0.5 , 0.75, 1.  ]),\n                         'xgb__max_depth': range(1, 11, 3),\n                         'xgb__min_child_weight': range(1, 11, 3),\n                         'xgb__n_estimators': range(100, 1500, 700)},\n             scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('xgb',\n                                        XGBRegressor(base_score=None,\n                                                     booster=None,\n                                                     callbacks=None,\n                                                     colsample_bylevel=None,\n                                                     colsample_bynode=None,\n                                                     colsample_bytree=None,\n                                                     early_stopping_rounds=None,\n                                                     enable_categorical=False,\n                                                     eval_metric=None,\n                                                     feature_types=None,\n                                                     gamma=None, gpu_id=None,\n                                                     grow_policy=None,\n                                                     importance_type=Non...\n                                                     min_child_weight=None,\n                                                     missing=nan,\n                                                     monotone_constraints=None,\n                                                     n_estimators=100,\n                                                     n_jobs=None,\n                                                     num_parallel_tree=None,\n                                                     predictor=None,\n                                                     random_state=None, ...))]),\n             n_jobs=-1,\n             param_grid={'xgb__gamma': array([0.  , 0.25, 0.5 , 0.75, 1.  ]),\n                         'xgb__max_depth': range(1, 11, 3),\n                         'xgb__min_child_weight': range(1, 11, 3),\n                         'xgb__n_estimators': range(100, 1500, 700)},\n             scoring='neg_root_mean_squared_error')estimator: PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('xgb',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, gamma=None, gpu_id=None,\n                              grow_policy=None, importance_type=None,\n                              interaction_constraints=None,\n                              learning_rate=0.33777777777777773, max_bin=None,\n                              max_cat_threshold=None, max_cat_to_onehot=None,\n                              max_delta_step=None, max_depth=None,\n                              max_leaves=None, min_child_weight=None,\n                              missing=nan, monotone_constraints=None,\n                              n_estimators=100, n_jobs=None,\n                              num_parallel_tree=None, predictor=None,\n                              random_state=None, ...))])StandardScalerStandardScaler()XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.33777777777777773,\n             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=None, ...)\n\nbest_min_child_weight = grid_search_tree.best_params_['xgb__min_child_weight']\nbest_max_depth = grid_search_tree.best_params_['xgb__max_depth']\nbest_gamma = grid_search_tree.best_params_['xgb__gamma']\nbest_n_estimators = grid_search_tree.best_params_['xgb__n_estimators']\n\nprint(f\"Best min_n: {best_min_child_weight}\")\n\nBest min_n: 1\n\nprint(f\"Best max_depth: {best_max_depth}\")\n\nBest max_depth: 4\n\nprint(f\"Best lost_reduction: {best_gamma}\")\n\nBest lost_reduction: 0.5\n\nprint(f\"Best n_trees: {best_n_estimators}\")\n\nBest n_trees: 800\n\n\nNow that we have the tree-specific parameters tuned, we can move on to the stochastic hyperparameters.\n\n# Stochastic parameters\nparam_grid_stochastic = {\n    'xgb__subsample': np.linspace(0.5, 1, 5), #np.linspace(0.5, 1, 20)\n    'xgb__colsample_bytree': np.linspace(1, 15, 5), # np.linspace(1, 15, 15)\n}\n\n# Use the best learning rate and tree-specific parameters \npipeline.set_params(\n    xgb__learning_rate=best_learning_rate,\n    xgb__min_child_weight=best_min_child_weight,\n    xgb__max_depth=best_max_depth,\n    xgb__gamma=best_gamma,\n    xgb__n_estimators=best_n_estimators\n)\n\n# Perform GridSearchCV for stochastic parameters\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('xgb',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, gamma=0.5, gpu_id=None,\n                              grow_policy=None, importance_type=None,\n                              interaction_constraints=None,\n                              learning_rate=0.33777777777777773, max_bin=None,\n                              max_cat_threshold=None, max_cat_to_onehot=None,\n                              max_delta_step=None, max_depth=4, max_leaves=None,\n                              min_child_weight=1, missing=nan,\n                              monotone_constraints=None, n_estimators=800,\n                              n_jobs=None, num_parallel_tree=None,\n                              predictor=None, random_state=None, ...))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('xgb',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, gamma=0.5, gpu_id=None,\n                              grow_policy=None, importance_type=None,\n                              interaction_constraints=None,\n                              learning_rate=0.33777777777777773, max_bin=None,\n                              max_cat_threshold=None, max_cat_to_onehot=None,\n                              max_delta_step=None, max_depth=4, max_leaves=None,\n                              min_child_weight=1, missing=nan,\n                              monotone_constraints=None, n_estimators=800,\n                              n_jobs=None, num_parallel_tree=None,\n                              predictor=None, random_state=None, ...))])StandardScalerStandardScaler()XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=0.5, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.33777777777777773,\n             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=4, max_leaves=None,\n             min_child_weight=1, missing=nan, monotone_constraints=None,\n             n_estimators=800, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=None, ...)\n\ngrid_search_stochastic = GridSearchCV(\n    pipeline, param_grid_stochastic, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1\n)\ngrid_search_stochastic.fit(X_train, y_train)\n\n# Get the best stochastic parameters\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('xgb',\n                                        XGBRegressor(base_score=None,\n                                                     booster=None,\n                                                     callbacks=None,\n                                                     colsample_bylevel=None,\n                                                     colsample_bynode=None,\n                                                     colsample_bytree=None,\n                                                     early_stopping_rounds=None,\n                                                     enable_categorical=False,\n                                                     eval_metric=None,\n                                                     feature_types=None,\n                                                     gamma=0.5, gpu_id=None,\n                                                     grow_policy=None,\n                                                     importance_type=None...\n                                                     max_delta_step=None,\n                                                     max_depth=4,\n                                                     max_leaves=None,\n                                                     min_child_weight=1,\n                                                     missing=nan,\n                                                     monotone_constraints=None,\n                                                     n_estimators=800,\n                                                     n_jobs=None,\n                                                     num_parallel_tree=None,\n                                                     predictor=None,\n                                                     random_state=None, ...))]),\n             n_jobs=-1,\n             param_grid={'xgb__colsample_bytree': array([ 1. ,  4.5,  8. , 11.5, 15. ]),\n                         'xgb__subsample': array([0.5  , 0.625, 0.75 , 0.875, 1.   ])},\n             scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('xgb',\n                                        XGBRegressor(base_score=None,\n                                                     booster=None,\n                                                     callbacks=None,\n                                                     colsample_bylevel=None,\n                                                     colsample_bynode=None,\n                                                     colsample_bytree=None,\n                                                     early_stopping_rounds=None,\n                                                     enable_categorical=False,\n                                                     eval_metric=None,\n                                                     feature_types=None,\n                                                     gamma=0.5, gpu_id=None,\n                                                     grow_policy=None,\n                                                     importance_type=None...\n                                                     max_delta_step=None,\n                                                     max_depth=4,\n                                                     max_leaves=None,\n                                                     min_child_weight=1,\n                                                     missing=nan,\n                                                     monotone_constraints=None,\n                                                     n_estimators=800,\n                                                     n_jobs=None,\n                                                     num_parallel_tree=None,\n                                                     predictor=None,\n                                                     random_state=None, ...))]),\n             n_jobs=-1,\n             param_grid={'xgb__colsample_bytree': array([ 1. ,  4.5,  8. , 11.5, 15. ]),\n                         'xgb__subsample': array([0.5  , 0.625, 0.75 , 0.875, 1.   ])},\n             scoring='neg_root_mean_squared_error')estimator: PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('xgb',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, gamma=0.5, gpu_id=None,\n                              grow_policy=None, importance_type=None,\n                              interaction_constraints=None,\n                              learning_rate=0.33777777777777773, max_bin=None,\n                              max_cat_threshold=None, max_cat_to_onehot=None,\n                              max_delta_step=None, max_depth=4, max_leaves=None,\n                              min_child_weight=1, missing=nan,\n                              monotone_constraints=None, n_estimators=800,\n                              n_jobs=None, num_parallel_tree=None,\n                              predictor=None, random_state=None, ...))])StandardScalerStandardScaler()XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=0.5, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.33777777777777773,\n             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=4, max_leaves=None,\n             min_child_weight=1, missing=nan, monotone_constraints=None,\n             n_estimators=800, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=None, ...)\n\nbest_subsample = grid_search_stochastic.best_params_['xgb__subsample']\nbest_colsample_bytree = grid_search_stochastic.best_params_['xgb__colsample_bytree']\n\nprint(f\"Best subsample: {best_subsample}\")\n\nBest subsample: 1.0\n\nprint(f\"Best colsample_bytree: {best_colsample_bytree}\")\n\nBest colsample_bytree: 1.0\n\n\nAnd finally, we can use the best values for all of these hyperparameters to build our final model.\n\n# Train the final model with the best hyperparameters on the training set (excluding the holdout set)\nfinal_pipeline = pipeline.set_params(\n    xgb__learning_rate=best_learning_rate,\n    xgb__min_child_weight=best_min_child_weight,\n    xgb__max_depth=best_max_depth,\n    xgb__gamma=best_gamma,\n    xgb__n_estimators=best_n_estimators,\n    xgb__subsample=best_subsample,\n    xgb__colsample_bytree=best_colsample_bytree\n)\nfinal_pipeline.fit(X_train, y_train)\n\n# Validate the final model on the holdout set\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('xgb',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=1.0, early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, gamma=0.5, gpu_id=None,\n                              grow_policy=None, importance_type=None,\n                              interaction_constraints=None,\n                              learning_rate=0.33777777777777773, max_bin=None,\n                              max_cat_threshold=None, max_cat_to_onehot=None,\n                              max_delta_step=None, max_depth=4, max_leaves=None,\n                              min_child_weight=1, missing=nan,\n                              monotone_constraints=None, n_estimators=800,\n                              n_jobs=None, num_parallel_tree=None,\n                              predictor=None, random_state=None, ...))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('xgb',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=1.0, early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, gamma=0.5, gpu_id=None,\n                              grow_policy=None, importance_type=None,\n                              interaction_constraints=None,\n                              learning_rate=0.33777777777777773, max_bin=None,\n                              max_cat_threshold=None, max_cat_to_onehot=None,\n                              max_delta_step=None, max_depth=4, max_leaves=None,\n                              min_child_weight=1, missing=nan,\n                              monotone_constraints=None, n_estimators=800,\n                              n_jobs=None, num_parallel_tree=None,\n                              predictor=None, random_state=None, ...))])StandardScalerStandardScaler()XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=1.0, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=0.5, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.33777777777777773,\n             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=4, max_leaves=None,\n             min_child_weight=1, missing=nan, monotone_constraints=None,\n             n_estimators=800, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=None, ...)\n\ntest_preds = final_pipeline.predict(X_test)\ngb_rmse = mean_squared_error(y_test, test_preds, squared=False)\nprint(f\"test RMSE: {gb_rmse}\")\n\ntest RMSE: 5.499163662927346\n\n\n\n\nSimilarly in R, we start by tuning the learning rate. As we have before, we use a grid of values to test to find which produces the lowest RMSE on the training data.\n\n# tune learning rate\ntune_lr <- boost_tree(learn_rate = tune()) |> \n  set_mode('regression') |> \n  set_engine(\"xgboost\") \n\n# make our cv grid\nlr_grid <- expand.grid(learn_rate = seq(0.0001, 0.5, length.out = 20)) # seq(0.0001, 0.5, length.out = 200))\n\n# now our workflow\ntune_wf_lr <- workflow() |> \n      add_model(tune_lr) |> \n      add_recipe(rec) \n\n# tuning\nfit_tune_lr <- tune_wf_lr |> \n      tune_grid(resamples = folds, grid = lr_grid)\n\nshow_best(fit_tune_lr, metric = \"rmse\")\n\n# A tibble: 5 × 7\n  learn_rate .metric .estimator  mean     n std_err .config              \n       <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1      0.5   rmse    standard    7.38     5   0.552 Preprocessor1_Model20\n2      0.474 rmse    standard    7.51     5   0.543 Preprocessor1_Model19\n3      0.421 rmse    standard    7.57     5   0.487 Preprocessor1_Model17\n4      0.447 rmse    standard    7.64     5   0.576 Preprocessor1_Model18\n5      0.395 rmse    standard    7.66     5   0.560 Preprocessor1_Model16\n\nbest_lr <- as.numeric(show_best(fit_tune_lr, metric = \"rmse\")[1,1])\n\nUsing this learning rate, we then tune the tree specific parameters. Unlike our approach in python, we’re fixing the number of trees to be 1000 in R.\n\nregisterDoParallel(cores = n_cores)\n# Specifying that we want to tune the tree-specific parameters\nlropt_tune_spec <-\n    boost_tree(\n    learn_rate = best_lr,\n    trees = 100, # 1000\n    min_n = tune(),\n    tree_depth = tune(),\n    loss_reduction = tune()\n  ) |>\n  set_engine(\"xgboost\") |>\n  set_mode('regression')\n\n# Setting up the parameters to be tuned and the grid to search over\ntree_params <- parameters(tree_depth(), min_n(), loss_reduction())\ntrees_grid <- grid_max_entropy(tree_params, size = 10, iter = 5) # size=20\n\n# Defining a new workflow, adding our models and tuning parameters\ntree_wf <- workflow() |> add_model(lropt_tune_spec) |> add_recipe(rec)\n\n#  tune our parameters\nfit_tune_trees <- tree_wf |> tune_grid(\n  resamples = folds,\n  grid = trees_grid,\n  metrics = metric_set(rmse),\n  control = control_grid(save_pred = TRUE, parallel_over = \"everything\")\n)\n\n# extract the best values\nshow_best(fit_tune_trees, metric =\"rmse\")\n\n# A tibble: 5 × 9\n  min_n tree_depth loss_reduction .metric .estimator  mean     n std_err .config\n  <int>      <int>          <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>  \n1    16          4       4.78e- 8 rmse    standard    6.89     5   0.548 Prepro…\n2    39          4       1.81e+ 0 rmse    standard    6.90     5   0.415 Prepro…\n3    17          8       1.85e- 6 rmse    standard    7.09     5   0.393 Prepro…\n4    20         10       8.64e-10 rmse    standard    7.09     5   0.500 Prepro…\n5    36          6       3.85e-10 rmse    standard    7.10     5   0.539 Prepro…\n\nopt_min_n <- as.numeric(show_best(fit_tune_trees, metric = \"rmse\")[1,1])\nopt_tree_depth <- as.numeric(show_best(fit_tune_trees, metric = \"rmse\")[1,2])\nopt_loss_red <- as.numeric(show_best(fit_tune_trees, metric = \"rmse\")[1,3])\n\nWe now use our optimal tree parameters to tune the stochastic parameters.\n\nregisterDoParallel(cores = n_cores)\n\n# Specifying that we want to tune the stoachastic-specific parameters\nstoch_tune_spec <-\n  boost_tree(\n    learn_rate = best_lr,\n    trees = 100, # 1000\n    min_n = opt_min_n,\n    tree_depth = opt_tree_depth,\n    loss_reduction = opt_loss_red,\n    mtry = tune(),\n    sample_size = tune()\n  ) |>\n  set_engine(\"xgboost\") |>\n  set_mode('regression')\n\n# Define the parameters\nstoch_params <- parameters(\n  finalize(mtry(), training),\n  sample_size= sample_prop())\n\n# Generate a grid of parameter values\nstoch_grid <- grid_max_entropy(stoch_params, size = 10, iter = 5)\n        \nstoch_tune_wf <- workflow() |> \n  add_model(stoch_tune_spec) |> \n  add_recipe(rec)\n\nfit_tune_stoch <- stoch_tune_wf |> tune_grid(\n  resamples = folds,\n  grid = stoch_grid,\n  metrics = metric_set(rmse),\n  control = control_grid(save_pred = TRUE, parallel_over = \"everything\")\n)\nshow_best(fit_tune_stoch, metric = \"rmse\")\n\n# A tibble: 5 × 8\n   mtry sample_size .metric .estimator  mean     n std_err .config              \n  <int>       <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1    13       0.925 rmse    standard    7.12     5   0.448 Preprocessor1_Model10\n2    17       0.544 rmse    standard    8.01     5   0.261 Preprocessor1_Model01\n3    14       0.575 rmse    standard    8.26     5   0.542 Preprocessor1_Model07\n4     5       0.952 rmse    standard    8.28     5   0.683 Preprocessor1_Model08\n5     9       0.483 rmse    standard    8.89     5   0.394 Preprocessor1_Model04\n\nopt_mtry <- as.numeric(show_best(fit_tune_stoch, metric = \"rmse\")[1,1])\nopt_ss <- as.numeric(show_best(fit_tune_stoch, metric = \"rmse\")[1,2])\n\nFinally, we add all of this into the final model\n\nfinal_model <- boost_tree(learn_rate = best_lr,\n                          trees = 100, # 1000\n                          min_n = opt_min_n,\n                          mtry = opt_mtry,\n                          tree_depth = opt_tree_depth,\n                          loss_reduction = opt_loss_red,\n                          sample_size = opt_ss,\n                          ) |>\n  set_mode(\"regression\") |>\n  set_engine(\"xgboost\", early_stopping_rounds = 50)\n\n# final_params <- extract_parameter_set_dials(final_model)\n# final_grid <- grid_max_entropy(final_params, size = 50, iter = 5)\n\nfinal_wf <- workflow() |>\n  add_model(final_model) |>\n  add_recipe(rec)\n\nfinal_fit <- final_wf |> fit(training)\n\n# predict on the test, calculate RMSE\ngb_testing_preds <- predict(final_fit, testing) |> \n  bind_cols(testing) |> \n  mutate(truth = as.double(DIC), estimate = as.double(.pred)) |> \n  metrics(truth = truth, estimate = estimate)\n\ngb_rmse <- gb_testing_preds %>% filter(.metric == \"rmse\") |>  pull(.estimate)\n\nprint(gb_rmse)\n\n[1] 6.291353\n\n\n\n\n\n\n\nSupport Vector Machine\nSupport Vector Machines (SVMs) are a little different than what we’ve done so far. The goal of SVMs in our case is to move our data into higher dimensions to find a hyperplane that best fits our data points, while balancing model complexity and error tolerance. SVMs accomplish this by transforming the input data into a higher-dimensional space using the kernel trick (implicitly transforming the data into a higher dimensional space without explicitly performing the transformation, this helps the algorithm capture complex relationships in the data without compromising too much computational efficiency). SVMs identify a subset of the training data, called support vectors, which are critical in determining the optimal hyperplane. Once the optimal hyperplane is found, new data points can be projected onto it to make predictions.\nThe kernel function is what maps the features to a higher-dimensional feature space. Although there are different types of kernel functions to choose from, the linear kernel function is the most appropriate for our data. The linear kernel function is particularly useful when the data is linearly separable or when a linear relationship exists between the features and the target variable.\n\nPythonR\n\n\n\n# Create the pipeline\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('svm', SVR(kernel='linear'))\n])\n\n# Define the tuning grid\nparam_grid = {\n    'svm__C': np.logspace(-5, 5, 5), # np.logspace(-5, 5, 100),\n}\n\n# Perform grid search with cross-validation\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\ngrid.fit(X_train, y_train)\n\n# Train the final model with the best parameters\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('svm', SVR(kernel='linear'))]),\n             n_jobs=-1,\n             param_grid={'svm__C': array([1.00000000e-05, 3.16227766e-03, 1.00000000e+00, 3.16227766e+02,\n       1.00000000e+05])},\n             scoring='neg_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('svm', SVR(kernel='linear'))]),\n             n_jobs=-1,\n             param_grid={'svm__C': array([1.00000000e-05, 3.16227766e-03, 1.00000000e+00, 3.16227766e+02,\n       1.00000000e+05])},\n             scoring='neg_mean_squared_error')estimator: PipelinePipeline(steps=[('scaler', StandardScaler()), ('svm', SVR(kernel='linear'))])StandardScalerStandardScaler()SVRSVR(kernel='linear')\n\nbest_params = grid.best_params_\nfinal_model = grid.best_estimator_\n\n# Make predictions on the validation set\ny_pred = final_model.predict(X_test)\n\n# Calculate the RMSE\nsvm_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f\"RMSE: {svm_rmse}\")\n\nRMSE: 4.8954038960758375\n\n\n\n\n\n# make our SVM model\nsvm_model <- svm_linear(\n  cost = tune()\n) |>\n  set_mode(\"regression\") |>\n  set_engine(\"kernlab\")\n\n# add model to workflow\nsvm_workflow <- workflow() |>\n  add_model(svm_model) |>\n  add_recipe(rec)\n\n# specify our tuning grid\ntune_svm_grid <- grid_regular(\n  cost(range = c(-10, 10)),\n  levels = 10\n)\n\n# tune our model\nfit_tune_svm <- svm_workflow |>\n  tune_grid(\n    resamples = folds,\n    grid = tune_svm_grid,\n    metrics = metric_set(rmse),\n    control = control_grid(save_pred = TRUE, parallel_over = \"everything\")\n  )\n\n# extract the best values for the hyperparameters\nbest_svm_params <- fit_tune_svm |>\n  show_best(metric = \"rmse\") |>\n  dplyr::slice(1) |>\n  select(cost)\n\n# use our best values in the final model\nfinal_svm_model <- svm_linear(\n  cost = best_svm_params$cost\n) |>\n  set_mode(\"regression\") |>\n  set_engine(\"kernlab\")\n\n# make a new workflow\nfinal_svm_wf <- workflow() |>\n  add_model(final_svm_model) |>\n  add_recipe(rec)\n\nfinal_svm_fit <- final_svm_wf |> fit(training)\n\n Setting default kernel parameters  \n\nfinal_svm_pred <- final_svm_fit |> predict(testing)\n\nsvm_rmse <- final_svm_pred |>\n  bind_cols(testing) |>\n  rmse(truth = DIC, estimate = .pred) %>%\n  pull(.estimate)\n\nprint(\"RMSE:\")\n\n[1] \"RMSE:\"\n\nprint(svm_rmse)\n\n[1] 5.643437\n\n\n\n\n\n\n\nEvaluation\n\n\n\nplt.clf()\nmodel_names = [\"LR\", \"Lasso\", \"Ridge\", \"KNN\", \"DT\", \"Bag DT\", \"RF\", \"GB\", \"SVM\"]\nrmses = [rmse_test_lr, ridge_rmse, lasso_rmse, rmse_knn, dt_rmse, bag_tree_rmse, rf_rmse, gb_rmse, svm_rmse]\n\nfig, ax = plt.subplots()\nax.bar(model_names, rmses)\n\n<BarContainer object of 9 artists>\n\nax.set_xlabel(\"Model\")\nax.set_ylabel(\"RMSE\")\nax.set_title(\"Model Performance Comparison\")\nplt.xticks(rotation = 45)\n\n([0, 1, 2, 3, 4, 5, 6, 7, 8], [Text(0, 0, 'LR'), Text(1, 0, 'Lasso'), Text(2, 0, 'Ridge'), Text(3, 0, 'KNN'), Text(4, 0, 'DT'), Text(5, 0, 'Bag DT'), Text(6, 0, 'RF'), Text(7, 0, 'GB'), Text(8, 0, 'SVM')])\n\nplt.show()\n\n\n\n\n\nmodel_names <- c(\"Linear Regression\", \"Lasso\", \"Ridge\", \"KNN\", \"Decision Tree\", \"Bagged DT\", \"Random Forest\", \"Gradient Boosting\", \"SVM\")\nrmses <- c(rmse_lr, lasso_rmse, ridge_rmse, rmse_knn, dt_rmse, bagdt_rmse, rf_rmse, gb_rmse, svm_rmse)\n\ndf_rmse <- data.frame(Model = model_names, RMSE = rmses)\n\nggplot(df_rmse, aes(x = Model, y = RMSE, fill = Model)) +\n  geom_bar(stat = \"identity\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(x = \"Model\", y = \"RMSE\", title = \"Model Performance Comparison\")\n\n\n\n\n\n\n\nConclusion\n\n\n\n\nCitationBibTeX citation:@online{bartnik2023,\n  author = {Andrew Bartnik},\n  title = {Predicting Dissolved Inorganic Carbon {(DIC)} Concentrations\n    Using Different Algorithms in {Python} and {R}},\n  date = {03/18/2023},\n  url = {https://andrewbartnik.github.io/Portfolio/ocean-modeling},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndrew Bartnik. 3AD–18AD. “Predicting Dissolved Inorganic Carbon\n(DIC) Concentrations Using Different Algorithms in Python and R.”\n3AD–18AD. https://andrewbartnik.github.io/Portfolio/ocean-modeling."
  },
  {
    "objectID": "Portfolio/ml_final/index.html",
    "href": "Portfolio/ml_final/index.html",
    "title": "Predicting dissolved inorganic carbon (DIC) concentrations using different algorithms in Python and R",
    "section": "",
    "text": "Introduction and Background\nIn this post we will be using chemistry data from seawater collected by the California Cooperative Oceanic Fisheries Investigation (CalCOFI). We will be using temperature, salinity, pH, and other chemical parameters to predict Dissolved Inorganic Carbon (DIC) concentrations. Samples are collected at CalCOFI sampling stations. We will be using different regression models to find which does the best job at predicting DIC. Our models are as follows: Linear Regression (including Ridge and Lasso penalties), K-Nearest Neighbors, Decision Trees, Bagged Decision Trees, Random Forests, Gradient Boosting, and a Support Vector Machine. We will be building each of these models in both Python and R using sklearn and tidymodels, and we will discuss how each of these models work. This is an expansion on an assignment from Machine Learning for Environmental Science as part of the University of California Santa Barbara’s Bren School of the Environment & Management Master of Environmental Data Science (MEDS) Program.\nThe full list of our predictors are below:\n\nNO2uM - Micromoles of Nitrite per liter of seawater\nNO3uM - Micromoles of Nitrate per liter of seawater\nNH3uM - Micromoles of Ammonia per liter of seawater\nR_TEMP - Reported (Potential) Temperature Celsius\nR_Depth - Reported Depth (from pressure, meters)\nR_Sal - Reported Salinity (from Specific Volume Anomoly, M³/Kg)\nR_DYNHT - Reported Dynamic Height in units of dynamic meters (work per unit mass)\nR_Nuts - Reported Ammonium concentration\nR_Oxy_micromol - Reported Oxygen concentration micromoles/kilogram\nPO4uM - Micromoles of Phosphate per liter of seawater\nSiO3uM - Micromoles of Silicate per liter of seawater\nTA1 - Total Alkalinity micromoles per kilogram solution\nSalinity1 - Salinity (Practical Scale - 1978)\nTemperature_degC - Temp (C)\n\nFirst, we import our required libraries in both languages. We use the reticulate package in R to allow us to execute python code in a quarto doc.\n\nRPython\n\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(doParallel)\nlibrary(rsample)\nlibrary(glmnet)\nlibrary(tmap)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(reticulate)\nlibrary(baguette)\nlibrary(corrplot)\n\nuse_condaenv(\"oceanchem\", required = TRUE)\n\n\n\n\n# numpy and pandas for data manipulation + processing\nimport pandas as pd\nimport numpy as np\n\n# sklearn and xgboost for our modeling\nimport xgboost\nimport sklearn\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.svm import SVR\n\n# cartopy, matplotlib for plots\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport plotly\n\n\n\n\n\n\nPreprocessing\nFirst, we are going to do some pre-processing and data exploration. The process is essentially the same in both R and python. We first import the data, and make sure our columns are consistent. The actual DIC values for our test set are stored in a separate dataframe, so we’ll also join these onto the test set.\n\nPythonR\n\n\nWe first use pandas to read in our data, merge our solutions to our test set, and make some minor adjustments to column names to ensure consistency.\n\n# Reading in the data using pandas. Join the solutions onto the test set\ntrain_py = pd.read_csv(\"train.csv\")\nsols = pd.read_csv(\"solution.csv\")\ntest_py = pd.read_csv(\"test.csv\")\n\ntest_py = pd.merge(test_py, sols, on=\"id\", suffixes=(\"\", \"_actual\"))\n# Dropping the initial index column, renaming TA1 to a more appropriate format\ntest_py = test_py.drop(test_py.columns[0], axis=1).rename(columns={\"TA1.x\": \"TA1.x\"})\ntrain_py = train_py.drop([train_py.columns[0], train_py.columns[12]], axis=1).rename(columns={\"TA1.x\": \"TA1\"})\n\n# We will be clearing our environment consistently using the del function in python, just to stay organized and clear up space\ndel(sols)\n\nLets plot the data to get a feel for where off the California coast the samples were collected. We’re going to use the cartopy package:\n\n\n# First, we set up the plot with cartopy\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree()) # specifying the map projection we want to use\n\n# Next, we'll define the region of interest (California coast)\nlat_min, lat_max = 30, 37\nlon_min, lon_max = -130, -114\n\n# Set the plot extent (latitude and longitude range of the coast)\nax.set_extent([lon_min, lon_max, lat_min, lat_max])\n\n# Add higher resolution coastlines, land, and ocean features for aesthetics\nax.add_feature(cfeature.NaturalEarthFeature('physical', 'land', '10m', edgecolor='face', facecolor=cfeature.COLORS['land']))\nax.add_feature(cfeature.NaturalEarthFeature('physical', 'ocean', '10m', edgecolor='face', facecolor=cfeature.COLORS['water'], alpha = 0.5))\nax.coastlines(resolution='10m')\n\n# Add the borders of the coastline\nax.add_feature(cfeature.BORDERS, linestyle=':')\n\n# Plot the sampling points \nax.scatter(train_py['Lon_Dec'], train_py['Lat_Dec'], color='red', marker='o')\n\n# Add labels and title\nax.set_xticks(np.arange(lon_min, lon_max + 1, 2), crs=ccrs.PlateCarree())\nax.set_yticks(np.arange(lat_min, lat_max + 1, 2), crs=ccrs.PlateCarree())\nax.xaxis.set_major_formatter(LongitudeFormatter())\nax.yaxis.set_major_formatter(LatitudeFormatter())\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.title(\"CalCOFI Sampling Points along the California Coast\")\n\nplt.show()\n\n\n\n\n\n\nWe read in the data the same way in R. We use an inner_join to bind the test values to the dataframe, and filter out the columns that we’re not interested in.\n\n# Read in the solutions data\nsolutions <- read_csv(\"solution.csv\")\n# Read in the testing data, join it to our solutions dataframe. \ntesting <- read_csv(\"test.csv\") |> inner_join(solutions, by = \"id\", suffix = c(\"\", \"_actual\")) |> select(-1, TA1.x = TA1)\n\n# Read in the training data, getting rid of the redundant id column and the other blank column\ntraining <- read_csv(\"train.csv\") |> select(-1, -...13)\n\n# Using doParallel to specify the number of cores that we want to use for some of our more computationally intensive models.  We'll come back to this later\nn_cores <- detectCores() - 1 \n\n# Clearing our environment\nrm(solutions)\n\n## Using a corrplot to visualize correlations between our variables\ntraining_cor <- cor(training)\ncorrplot(training_cor, method = \"color\")\n\n\n\n\nOur corrplot shows that there are strong correlations between DIC and some of our other variables. Specifically, DIC looks to be highly correlated with Micromoles of Nitrate per liter of seawater, Temperature, Salinity, and a few others.\nWe can also plot the data in R, instead using leaflet to add some basic interactivity.\n\n# Create a color palette for DIC. This adds another dimension to the data, and we'll be able to clearly see if there is any sort of spatial pattern with DIC\ncolor_palette <- colorNumeric(palette = \"YlOrRd\", domain = training$DIC)\n\n# Create the leaflet map\nleaflet(training) %>%\n  addProviderTiles(\"Esri.WorldImagery\", group = \"Satellite\") %>%\n  addProviderTiles(\"OpenStreetMap\", group = \"Street Map\") %>%\n  addCircleMarkers(lng = ~Lon_Dec, lat = ~Lat_Dec,\n                   fillColor = ~color_palette(DIC),\n                   fillOpacity = 1, stroke = FALSE,\n                   radius = 3, group = \"Data\") %>%\n  addLegend(pal = color_palette, values = ~DIC,\n            title = \"DIC\", position = \"bottomright\") %>%\n  addLayersControl(baseGroups = c(\"Satellite\", \"Street Map\"),\n                   overlayGroups = \"Data\",\n                   options = layersControlOptions(collapsed = FALSE))\n\n\n\n\nrm(color_palette)\n\nThis was partially an exercise to see if DIC was correlated with distance from the coastline at all, which it doesn’t appear to be.\n\n\n\n\n\nFeature Engineering\nNow we’re going to do some feature engineering. Feature engineering transforms existing data into new data, and helps improve our model performance by capturing relationships that might not be explicit in the data as it is. There are different types of feature engineering, but we’re going to focus on using interaction terms. Interaction terms are used to capture the combined effect of two or more variables on our target variable, DIC. This is particularly important in our context because ocean chemistry is complicated, and we would like to use our domain knowledge to capture some of the relationships between our predictors.\nFor example, consider the relationship between depth R_Depth and salinity R_Sal. The interaction between depth and salinity can be explained by the fact that as water depth increases, pressure also increases, which affects the solubility of gases in seawater. Salinity also affects the solubility of gases in seawater, so the combined effect of depth and salinity may impact dissolved inorganic carbon levels.\nUsing this same approach, we are going to include interaction terms for temperature/salinity, and depth/temperature.\n\nPythonR\n\n\nIn Python, we name the new interaction column and set it equal to the product of the terms we want to interact. We repeat this process for each interaction term.\n\n# Adding interaction terms to our training set\ntrain_py['T_degC_Salnty'] = train_py['Temperature_degC'] * train_py['Salinity1']\ntrain_py['Depth_Sal'] = train_py['R_Depth'] * train_py['R_Sal']\ntrain_py['Depth_Temp'] = train_py['R_Depth'] * train_py['Temperature_degC']\n\n# Same thing for the test set\ntest_py['T_degC_Salnty'] = test_py['Temperature_degC'] * test_py['Salinity1']\ntest_py['Depth_Sal'] = test_py['R_Depth'] * test_py['R_Sal']\ntest_py['Depth_Temp'] = test_py['R_Depth'] * test_py['Temperature_degC']\n\nWe then make our training and testing objects. We split off DIC and assign it to the y_test and y_train variables below. This allows us to explicitly tell our models which columns we’re using as features, and the outcome we want to predict.\n\n# Splitting off DIC in the training data\nX_train = train_py.drop(columns=['DIC'])\ny_train = train_py['DIC']\n\n# Same thing for the test data\nX_test = test_py.drop(columns=['DIC'])\ny_test = test_py['DIC']\n\n\n\nWe can use the same approach to make our interaction terms in R with the $ operator to initialize new columns:\n\n# Adding interaction terms to our training sets\ntraining$T_degC_Salnty <- training$Temperature_degC * training$Salinity1\ntraining$Depth_Sal <- training$R_Depth * training$R_Sal\ntraining$Depth_Temp <- training$R_Depth * training$Temperature_degC\n\n# Adding interaction terms to our test sets\ntesting$T_degC_Salnty <- testing$Temperature_degC * testing$Salinity1\ntesting$Depth_Sal <- testing$R_Depth * testing$R_Sal\ntesting$Depth_Temp <- testing$R_Depth * testing$Temperature_degC\n\nIn R, the tidymodels package provides us with useful functions for splitting training and testing data. We stratify on DIC to ensure that our data is not disproportionately split on the outcome variable, which would bias our model. We also specify our cross-validation parameters using 5 folds, and create a recipe object that we will use throughout our analysis. This recipe object uses our specifications to normalize the numeric variables, and we will use this in each R model. We will also normalize in Python, using pipelines that we create separately for each model.\n\n# Creating 5-fold CV\nfolds <- vfold_cv(data = training, v = 5, strata = DIC)\n\n# Creating our recipe object\nrec <- recipe(DIC ~ ., data = training) |> \n  step_normalize(all_numeric(), -DIC) |> \n  prep() \n\n\n\n\n\n\nLinear Regression\nLinear regression is the simplest model we can use to make predictions. It is an extremely powerful yet elegant technique used to model the relationship between one or more independent variables (predictors) and dependent variables (features). The basic form of the model is:\n\\[\\operatorname{Y}=\\beta_0+\\beta_1  x +\\beta_n x_n + \\varepsilon \\] Where \\(\\beta_0\\) is our intercept, \\(\\beta_1\\), \\(\\beta_n\\) are the coefficients of our independent variables, and \\(\\varepsilon\\) is the difference between the observed values and the values predicted by our model. We’re going to be evaluating our model performance using the Root Mean Squared Error (RMSE) metric which represents the square root of the average squared differences between the predicted values and the actual observed values. This helps us quantify the difference between the predicted values generated by the model and the actual values of the target variable.\nWe’re also going to try Lasso and Ridge regression here. Both of these follow the same concepts as regular linear regression, but they implement penalties based on slightly different metrics of the cost function. Ridge regression adds an L2 penalty term, which is the squared magnitude of the \\(\\beta\\) coefficients. This penalty term is controlled by alpha, and prevents our model from overfitting the training data by shrinking the coefficients towards zero, thus reducing the variance of the model. Lasso on the other hand uses an L1 penalty term on the cost function, which is based on the absolute magnitude of the coefficients. This penalty term is controlled by lambda, and forces some of the coefficients to be exactly zero, eliminating irrelevant features from the model. Both of these may improve our models’ performance.\n\nPythonR\n\n\nIn Python, we first instantiate a linear regression model, fit it on the training data, and then predict on the holdout data.\n\n\n# instantiate a Linear Regression model\nlr_model = LinearRegression()\n\n# fit the model to the training set\nlr_model.fit(X_train, y_train)\n\n# Predict on the test set\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\ny_test_pred_lr = lr_model.predict(X_test)\n\n# Calculate RMSE on our test set\nrmse_test_lr = mean_squared_error(y_test, y_test_pred_lr, squared=False)\n\nprint(f\"RMSE on the test set: {rmse_test_lr:.2f}\")\n\nRMSE on the test set: 4.93\n\n\nAn RMSE of 4.93 is not bad at all! Lets take a look at what this actually looks like plotted through our data:\n\n# clear our plot\nplt.clf()\n# Create a scatter plot of the true DIC values vs predicted DIC values\nplt.scatter(y_test, y_test_pred_lr, alpha=0.5)\n\n# Add a diagonal line representing perfect predictions\ndiagonal_line = np.linspace(min(y_test.min(), y_test_pred_lr.min()),\n                            max(y_test.max(), y_test_pred_lr.max()))\nplt.plot(diagonal_line, diagonal_line, color='red', linestyle='--', lw=2)\n\n# Add labels, title and RMSE text\nplt.xlabel('True DIC values')\nplt.ylabel('Predicted DIC values')\nplt.title('True vs Predicted DIC values')\nplt.text(0.05, 0.95, f'RMSE: {rmse_test_lr:.2f}', transform=plt.gca().transAxes)\n\n# Show the plot\nplt.show()\n\n\n\ndel(lr_model, y_test_pred_lr)\n\nThe red line here is what we would have observed if we correctly predicted each observation. The difference between the red line and the blue dots represent the difference between our model’s prediction, and the actual value.\nNow we’ll check out Ridge and Lasso regression. We follow the same approach as with linear regression but with one slight change. We specify the alpha parameter, which controls the strength of the penalty. This would usually be a tuning hyperparameter, which I’ll explain in a second, but we’re just going to use an alpha value of one.\n\nplt.clf()\n\n# Ridge Regression\nridge = Ridge(alpha=1)\nridge.fit(X_train, y_train)\n\nRidge(alpha=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=1)\n\nridge_preds = ridge.predict(X_test)\nridge_rmse = mean_squared_error(y_test, ridge_preds, squared=False)\nprint(\"Ridge RMSE:\", ridge_rmse)\n\n# Lasso Regression\n\nRidge RMSE: 4.928343737671815\n\nlasso = Lasso(alpha=1)\nlasso.fit(X_train, y_train)\n\nLasso(alpha=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso(alpha=1)\n\nlasso_preds = lasso.predict(X_test)\nlasso_rmse = mean_squared_error(y_test, lasso_preds, squared=False)\nprint(\"Lasso RMSE:\", lasso_rmse)\n\nLasso RMSE: 6.5330738340489605\n\ndel(ridge, ridge_preds, lasso, lasso_preds)\n\nWhile our ridge model achieved a comparable RMSE to our linear regression, our lasso model performed worse. This could be because the lasso model is shrinking important coefficients to zero.\n\n\nIn R, we use our recipe that we defined earlier, then create a linear regression model and a workflow. Our recipe object specifies the preprocessing steps required to transform the data, while the workflow includes both the preprocessing and the modeling steps. We add our recipe and model to our workflow, fit on the training data, and predict on the test data.\n\n# First we create the model\nlm_model <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  set_mode(\"regression\")\n\n\n# we then make a workflow, analogous to a pipe in python\nlm_workflow <- workflow() |> \n  add_recipe(rec) |> \n  add_model(lm_model)\n\n# fit to training\nlm_fit <- fit(lm_workflow, data = training)\n\n# predict on the test data\ntesting_preds_lr <- predict(lm_fit, testing)\n\n# Calculate and store the RMSE\nrmse_test_lr <- rmse(testing_preds_lr, truth = testing$DIC, estimate = .pred)\nrmse_lr <- rmse_test_lr$.estimate\n\nprint(paste0(\"RMSE on the holdout set: \", round(rmse_lr, 3)))\n\n[1] \"RMSE on the holdout set: 4.933\"\n\n# Create a scatter plot of the true DIC values vs predicted DIC values\nggplot(testing, aes(x = DIC, y = testing_preds_lr$.pred)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(\n    x = \"True DIC values\",\n    y = \"Predicted DIC values\",\n    title = \"True vs Predicted DIC values\"\n  ) +\n  annotate(\"text\", x = min(testing$DIC), y = max(testing$DIC), label = paste(\"RMSE:\", round(rmse_lr, 2)), hjust = 0, vjust = 1)\n\n\n\nrm(lm_model, lm_workflow, lm_fit, testing_preds_lr, rmse_test_lr)\n\nWe achieved a similar RMSE to our linear regression in python, which is reassuring. Now, lets try ridge and lasso penalties in R:\n\n# Ridge Regression, mixture=0 specifies that this is a ridge model\nridge_model <- linear_reg(penalty = 1, mixture = 0) %>%\n  set_engine(\"glmnet\") %>%\n  set_mode(\"regression\")\n\n# Ridge workflow, add our recipe and model\nridge_workflow <- workflow() |> \n  add_recipe(rec) |> \n  add_model(ridge_model)\n\n# Fit on the training, predict on the test data, store the RMSEs\nridge_fit <- fit(ridge_workflow, data = training)\nridge_testing_preds <- predict(ridge_fit, testing)\nridge_rmse <- as.numeric(rmse(ridge_testing_preds, truth = testing$DIC, estimate = .pred)[,3])\nprint(paste(\"Ridge RMSE:\", round(ridge_rmse, 2)))\n\n[1] \"Ridge RMSE: 7.89\"\n\n# Same approach for lasso regression, this time using mixture=1 for lasso\nlasso_model <- linear_reg(penalty = 1, mixture = 1) %>%\n  set_engine(\"glmnet\") %>%\n  set_mode(\"regression\")\n\n# workflow, same as before\nlasso_workflow <- workflow() |> \n  add_recipe(rec) |> \n  add_model(lasso_model)\n# fit on the training and predict on the test\nlasso_fit <- fit(lasso_workflow, data = training)\nlasso_testing_preds <- predict(lasso_fit, testing)\nlasso_rmse <- as.numeric(rmse(lasso_testing_preds, truth = testing$DIC, estimate = .pred)[,3])\nprint(paste(\"Lasso RMSE:\", round(lasso_rmse, 2)))\n\n[1] \"Lasso RMSE: 5.39\"\n\nrm(ridge_model, ridge_workflow, ridge_fit, lasso_model, lasso_workflow, lasso_fit, ridge_testing_preds, lasso_testing_preds)\n\nBoth of these models in R performed worse than our regular linear regression model.\n\n\n\nOur ridge and lasso models both performed slightly worse than our regular linear regression. This could be because we chose poor values of penalty, which we could address by tuning this hyperparameter. This could also simply be because our penalties might be penalizing coefficients that are actually important for predicting the outcome variable. This can happen when there is high correlation between the predictors. While our Lasso and Ridge models didn’t boost our performance here, they are especially useful for when we have a dataset with a high number of predictors and help us reduce complexity and minimize overfitting.\n\n\nKNN\nThe next model we want to try is K-Nearest Neighbors. Usually, this algorithm is used for classification, and would not be my first choice for a regression task. In the context of classification, KNN is used to predict the category of an unknown data point based on the categories of its neighboring data points. Given a scatterplot of data points belonging to different classes (for example, measurements of tree height and leaf width of two distinct species), the algorithm considers a specified number of the closest points (neighbors) to the unknown data point. The unknown point is then assigned a category based on the majority class of these neighbors. KNN can also be applied to regression tasks, where the goal is to predict a continuous value instead of a discrete category. Here, the algorithm again looks at the specified number of nearest neighbors to the unknown data point. However, instead of determining the majority class, it computes the average of the target values of these neighbors. The unknown point is then assigned this average value as its prediction.\nWe are also tuning our first hyperparameter here: the number of neighbors we want to use. We’re going to use grid searching to find the optimal k, essentially making a grid of a range of values that we expect our optimal value for k to fall within. We search over this grid, test each of the values, and find the value that performs best on the training set. It is important to find the right k-value, as too small of a k will result in overfitting while too large of a k will result in underfitting. We will be using this approach with all models going forward.\n\nPythonR\n\n\nWe follow a slightly different workflow to our linear regression model in python now that we have a tuning parameter. We instantiate the model, specify the number of neighbors we want to test (typically the square root of the total number of observations in the dataset is a good rule of thumb)\n\n# Instantiate our knn regressor\nknn_regressor = KNeighborsRegressor()\n\n# Define the parameter grid for hyperparameter tuning - we want to test all numbers of n_neighbors from 1 to 34\nparam_grid = {'knn__n_neighbors': list(range(1, 35))}\n\n# Create a pipeline with StandardScaler and KNeighborsRegressor\nknn_pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('knn', knn_regressor)\n])\n\n# Perform GridSearchCV for hyperparameter tuning\ngrid_search = GridSearchCV(knn_pipeline, param_grid, cv=5, scoring='neg_root_mean_squared_error')\n\n# fit on the training\ngrid_search.fit(X_train, y_train)  \n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('knn', KNeighborsRegressor())]),\n             param_grid={'knn__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,\n                                              12, 13, 14, 15, 16, 17, 18, 19,\n                                              20, 21, 22, 23, 24, 25, 26, 27,\n                                              28, 29, 30, ...]},\n             scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('knn', KNeighborsRegressor())]),\n             param_grid={'knn__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,\n                                              12, 13, 14, 15, 16, 17, 18, 19,\n                                              20, 21, 22, 23, 24, 25, 26, 27,\n                                              28, 29, 30, ...]},\n             scoring='neg_root_mean_squared_error')estimator: PipelinePipeline(steps=[('scaler', StandardScaler()), ('knn', KNeighborsRegressor())])StandardScalerStandardScaler()KNeighborsRegressorKNeighborsRegressor()\n\n\nWe can take a look at how our RMSE changes with different values of k:\n\n# first, clear the figure\nplt.clf()\n# Get the mean cross-validated scores for each n_neighbors\ncv_scores = grid_search.cv_results_['mean_test_score']  \n\ncv_scores = -cv_scores\n\n# Plot RMSE vs. n_neighbors\nplt.figure(figsize=(8, 6))\nplt.plot(list(range(1, 35)), cv_scores, marker='o')\nplt.xlabel('Number of Neighbors (n_neighbors)')\nplt.ylabel('Mean Cross-Validated RMSE')\nplt.title('RMSE vs. n_neighbors')\nplt.grid()\nplt.show()\n\n\n\n\nIt looks like our best value for k is 3.\n\n# fit on the training\ngrid_search.fit(X_train, y_train)\n\n# Retrieve the best KNN model\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('knn', KNeighborsRegressor())]),\n             param_grid={'knn__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,\n                                              12, 13, 14, 15, 16, 17, 18, 19,\n                                              20, 21, 22, 23, 24, 25, 26, 27,\n                                              28, 29, 30, ...]},\n             scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('knn', KNeighborsRegressor())]),\n             param_grid={'knn__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,\n                                              12, 13, 14, 15, 16, 17, 18, 19,\n                                              20, 21, 22, 23, 24, 25, 26, 27,\n                                              28, 29, 30, ...]},\n             scoring='neg_root_mean_squared_error')estimator: PipelinePipeline(steps=[('scaler', StandardScaler()), ('knn', KNeighborsRegressor())])StandardScalerStandardScaler()KNeighborsRegressorKNeighborsRegressor()\n\nbest_knn = grid_search.best_estimator_\n\n# predict on the test set\ny_pred = grid_search.predict(X_test)\n\n# get our RMSE\nrmse_knn = mean_squared_error(y_test, y_pred, squared=False)\n\nprint(\"Root Mean Squared Error: \", rmse_knn)\n\nRoot Mean Squared Error:  11.332377692549752\n\ndel(knn_regressor, param_grid, grid_search, best_knn, y_pred)\n\nRMSE is noticeably worse than our regularized and ordinary linear regression models.\n\n\nWe follow the same approach in R, this time specifying in the model specification that we want to tune the number of neighbors. We make a grid of possible values for our number of neighbors parameter, find the optimal value, then use this value to fit on the training and predict on the test set.\n\n# Make our model\nknn_spec <- nearest_neighbor(neighbors = tune()) |> \n  set_engine(\"kknn\") |> \n  set_mode(\"regression\")\n\n# Create a grid\nk_grid <- grid_regular(\n  neighbors(range = c(1, 34)),\n  levels = 30\n)\n\n# make our workflow\nknn_workflow <- workflow() |> \n  add_model(knn_spec) |> \n  add_recipe(rec)\n\n# tune it!\nknn_tuned <- tune_grid(\n  knn_workflow,\n  resamples = folds,\n  grid = k_grid,\n  metrics = metric_set(rmse)\n)\n\n# Get our optimal value for k\nbest_k <- knn_tuned |> \n  collect_metrics() |> \n  filter(.metric == \"rmse\") |> \n  arrange(mean) |> \n  slice(1) |> \n  pull(neighbors)\n\nWe can also check how our RMSE changes with different values of our n_neighbors hyperparameter in R.\n\n# Get the RMSE for each n_neighbors from the tuning results\nknn_tuning_metrics <- knn_tuned |> \n  collect_metrics() |> \n  filter(.metric == \"rmse\")\n\n# Plot RMSE vs. n_neighbors\nggplot(knn_tuning_metrics, aes(x = neighbors, y = mean)) +\n  geom_point() +\n  geom_line() +\n  theme_minimal() +\n  labs(\n    title = \"RMSE vs. n_neighbors\",\n    x = \"Number of Neighbors (n_neighbors)\",\n    y = \"Mean Cross-Validated RMSE\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\nOur best value for K looks to be 5 in R.\n\n# Update our model using our best k\nknn_spec_best <- nearest_neighbor(neighbors = best_k) |> \n  set_engine(\"kknn\") |> \n  set_mode(\"regression\")\n\n# Fit the final model on the whole training set\nknn_final <- knn_workflow |> \n  update_model(knn_spec_best) |> \n  fit(data = training)\n\n# Make predictions on the test set\nknn_testing_preds <- predict(knn_final, testing) |> \n  bind_cols(testing) |> \n  mutate(truth = as.double(DIC), estimate = as.double(.pred)) |> \n  metrics(truth = truth, estimate = estimate)\n\n# RMSE\nrmse_knn <- knn_testing_preds |> \n  filter(.metric == \"rmse\") |> \n  pull(.estimate)\n\nprint(paste0(\"Our KNN RMSE is: \", rmse_knn))\n\n[1] \"Our KNN RMSE is: 11.4660152657121\"\n\nrm(knn_spec, knn_spec_best, best_k, knn_final, knn_testing_preds, knn_tuned, k_grid, knn_workflow)\n\nAgain, our KNN model performs poorly here.\n\n\n\nOur RMSE in both of these KNN models were noticeably worse than any of the ordinary linear regression models. No surprises here, this is not a scenario in which we would typically use KNN. Now, we’re going to move on to the tree-based models.\n\n\nDecision Tree\nThe next algorithm we’re going to try is the decision tree. Decision trees are a popular algorithm used for both classification and regression tasks. They work by recursively splitting the input data into subsets based on the values of the input features, giving you a tree like structure. Each node in the tree represents a decision rule or condition based on the feature values, while the leaf/terminal nodes represent the final predicted class or value. The process of building a decision tree involves selecting the best feature to split the data at each node, usually based on a criterion such as information gain (for classification) or mean squared error (for regression). The tree continues to grow by making further splits until a stopping criterion is met, such as a minimum number of samples per leaf, a maximum depth of the tree, or a threshold for the improvement in the splitting criterion. A much more comprehensive explanation is here.\nDecision trees are great, but can be prone to overfitting. Fortunately, we have hyperparameters to combat this. We’re tuning 3 hyperparameters in this model. The cost complexity penalty, the maximum tree depth, and the minimum leaf samples. The cost complexity is used to “prune” the trees - reducing overfitting by removing branches of the decision tree that do not improve the accuracy of the model. This is similar to our alpha parameter in Lasso/Ridge regression, but penalizes the tree for having too many nodes. The maximum tree depth hyperparameter sets the maximum depth of the decision tree, or the number of levels the tree can have. A deeper tree is more likely to be overfitting the training data. Finally, the minimum number of samples required to be in a leaf node hyperparameter also helps us prevent overfitting and controlling tree depth by requiring a minimum number of samples in each leaf node.\nIt should be noted, unlike our KNN model, decision trees and their ensembles like random forests and bagged trees are generally not very sensitive to the scale of the input features because they make decisions based on the relative ordering of feature values, rather than the magnitudes of those values. But since we’re using the same recipe object rec for each model in R, all of our features are automatically scaled. We will use the same approach in python, by explicitly creating a Pipeline object with a scaler inside of it for each model.\n\nPythonR\n\n\nWe’re introducting our first pipeline object here. This object is analogous to a workflow in R. We pass it the preprocessing steps we want to use on our data, and our model. We fit this pipeline object to the training data, and predict on the test data.\n\n\n# Create a pipeline with a scaler and the decision tree regressor\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('dt_regressor', DecisionTreeRegressor(random_state=4))\n])\n\n# Define the parameter grid for hyperparameter tuning\nparam_grid = {\n    'dt_regressor__ccp_alpha': np.logspace(-4, 0, 20), # np.logspace(-4, 0, 5)\n    'dt_regressor__max_depth': list(range(1, 13)), # list(range(1, 21))\n    'dt_regressor__min_samples_leaf': list(range(1, 13)) # list(range(1, 21))\n}\n\n# Perform GridSearchCV for hyperparameter tuning using the pipeline\ngrid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('dt_regressor',\n                                        DecisionTreeRegressor(random_state=4))]),\n             n_jobs=-1,\n             param_grid={'dt_regressor__ccp_alpha': array([1.00000000e-04, 1.62377674e-04, 2.63665090e-04, 4.28133240e-04,\n       6.95192796e-04, 1.12883789e-03, 1.83298071e-03, 2.97635144e-03,\n       4.83293024e-03, 7.84759970e-03, 1.27427499e-02, 2.06913808e-02,\n       3.35981829e-02, 5.45559478e-02, 8.85866790e-02, 1.43844989e-01,\n       2.33572147e-01, 3.79269019e-01, 6.15848211e-01, 1.00000000e+00]),\n                         'dt_regressor__max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n                                                     10, 11, 12],\n                         'dt_regressor__min_samples_leaf': [1, 2, 3, 4, 5, 6, 7,\n                                                            8, 9, 10, 11, 12]},\n             scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('dt_regressor',\n                                        DecisionTreeRegressor(random_state=4))]),\n             n_jobs=-1,\n             param_grid={'dt_regressor__ccp_alpha': array([1.00000000e-04, 1.62377674e-04, 2.63665090e-04, 4.28133240e-04,\n       6.95192796e-04, 1.12883789e-03, 1.83298071e-03, 2.97635144e-03,\n       4.83293024e-03, 7.84759970e-03, 1.27427499e-02, 2.06913808e-02,\n       3.35981829e-02, 5.45559478e-02, 8.85866790e-02, 1.43844989e-01,\n       2.33572147e-01, 3.79269019e-01, 6.15848211e-01, 1.00000000e+00]),\n                         'dt_regressor__max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n                                                     10, 11, 12],\n                         'dt_regressor__min_samples_leaf': [1, 2, 3, 4, 5, 6, 7,\n                                                            8, 9, 10, 11, 12]},\n             scoring='neg_root_mean_squared_error')estimator: PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('dt_regressor', DecisionTreeRegressor(random_state=4))])StandardScalerStandardScaler()DecisionTreeRegressorDecisionTreeRegressor(random_state=4)\n\n\n\n# Make predictions and evaluate the model\ny_pred = grid_search.predict(X_test)\ndt_rmse = mean_squared_error(y_test, y_pred, squared=False)\nprint(\"Best Decision Tree parameters: \", grid_search.best_params_)\n\nBest Decision Tree parameters:  {'dt_regressor__ccp_alpha': 0.03359818286283781, 'dt_regressor__max_depth': 11, 'dt_regressor__min_samples_leaf': 5}\n\nprint(\"RMSE: \", dt_rmse)\n\n\n# Clear our environment\n\nRMSE:  6.711271577254955\n\ndel(pipe, param_grid, grid_search, y_pred)\n\nOur RMSE for this decision tree algorithm is 6.7. Much better than the KNN regressor!\n\n\nWe follow the same approach as we did for KNN in R. We specify the parameters we want to tune in the model specification, make a grid of values to try, and select the best values for our final model.\n\n# decision tree model specification, specifying our hyperparameters to tune. \ndt_spec <- decision_tree(\n    mode = \"regression\",\n    cost_complexity = tune(),\n    tree_depth = tune(),\n    min_n = tune()\n  ) |> \n  set_engine(\"rpart\") |> \n  set_mode(\"regression\")\n\n# Define the parameter grid for hyperparameter tuning\nparam_grid <- grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 5)\n\n# workflow\ndt_workflow <- workflow() |> \n  add_model(dt_spec) |> \n  add_recipe(rec)\n\n\n# tune our parameters\nregisterDoParallel(cores = n_cores)\ndt_tuned <- tune_grid(\n  dt_workflow,\n  resamples = folds,\n  grid = param_grid,\n  metrics = metric_set(rmse),\n  control = control_grid(save_pred = TRUE, parallel_over = \"everything\")\n)\n\n# extract the best values\nbest_params <- dt_tuned |> \n  show_best(\"rmse\") |> \n  slice(1) |> \n  select(cost_complexity, tree_depth, min_n) \n\nbest_cc <- best_params$cost_complexity\nbest_depth <- best_params$tree_depth\nbest_min_n <- best_params$min_n\n\nprint(paste0(\"Best cost complexity parameter: \", best_cc))\n\n[1] \"Best cost complexity parameter: 1e-10\"\n\nprint(paste0(\"Best maximum tree depth: \", best_depth))\n\n[1] \"Best maximum tree depth: 11\"\n\nprint(paste0(\"Best minimum number of samples in a leaf: \", best_min_n))\n\n[1] \"Best minimum number of samples in a leaf: 21\"\n\n\nWe can also use this autoplot function to visualize our best hyperparameter combinations:\n\nautoplot(dt_tuned)\n\n\n\n\nWe now use these best values in our final model:\n\n# Create the final decision tree model with the best hyperparameters\ndt_final <- decision_tree(\n  cost_complexity = best_cc,\n  tree_depth = best_depth,\n  min_n = best_min_n\n) |> \n  set_engine(\"rpart\") |> \n  set_mode(\"regression\")\n\n\n# fit to the training\ndt_final_fit <- dt_workflow |> \n  update_model(dt_final) |> \n  fit(data = training)\n\n# predict on the test set\ndt_testing_preds <- predict(dt_final_fit, testing) |> \n  bind_cols(testing) |> \n  mutate(truth = as.double(DIC), estimate = as.double(.pred)) |> \n  metrics(truth = truth, estimate = estimate)\n\ndt_rmse <- dt_testing_preds |>  filter(.metric == \"rmse\") |>  pull(.estimate)\n\nprint(best_params)\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth min_n\n            <dbl>      <int> <int>\n1    0.0000000001         11    21\n\nprint(paste0(\"RMSE:\", dt_rmse))\n\n[1] \"RMSE:6.74550963199777\"\n\nrm(dt_testing_preds, dt_tuned, dt_final_fit, dt_final, dt_spec, best_params, dt_workflow)\n\nAgain, our decision tree in R performs much better than the KNN algorithm we used.\n\n\n\nThe decision tree model will be the foundation for the next 3 models we are going to try. Bagged Trees, Random Forests, and Gradient Boosting all use decision trees as their base learner, but with slight adjustments to how the final model is constructed.\n\n\nBagged Trees\nOur normal decision tree algorithm worked well, but there are some significant improvements we can make to this algorithm. Bagging, or bootstrap aggregation is a technique that creates multiple models and then combines their predictions into a final output. Bootstrapping involves drawing samples from a dataset with replacement, to create new samples that are of the same size as the original dataset. By generating multiple samples of the data and predict the statistic of interest for each sample, we can aggregate these predictions into a final output that typically generalizes to unseen data much better than a model trained on a single dataset.\nIn the context of decision trees, this is an ensemble method that employs the bagging technique by training multiple decision trees on different subsets of the training data. In our case, the new predictions are made by averaging the predictions together from the individual base learners. This technique works particularly well on decision trees, since single decision trees are high variance learners. We’re creating multiple decision trees on different subsets of the training data. Our model’s stability and robustness are increased since the predictions are less sensitive to small changes in the training data. In other words, the variance of the model is reduced, which greatly helps us with avoiding overfitting. Bagged Trees also provide information about the importance of each feature in the model, because each tree is trained on a random subset of the features. This means that each tree will give different weights to different features, which can then be used to identify the most important ones.\nWe have an additional hyperparameter to train here: the number of trees we want to generate the final bagged model from. Aside from this, the code is much the same as regular decision trees.\n\nPythonR\n\n\nWe specify the n_estimators parameter here to be 500.\n\n\n# create a pipeline with a scaler and the bagged decision tree model\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('bagged_dt', BaggingRegressor(estimator=DecisionTreeRegressor(random_state=4), n_estimators=50, random_state=4)) #500\n])\n\n# define our grid\nparam_grid = {\n    'bagged_dt__estimator__ccp_alpha': np.logspace(-4, 0, 5), # np.logspace(-4, 0, 20),\n    'bagged_dt__estimator__max_depth': list(range(1, 13)), # list(range(1, 21)),\n    'bagged_dt__estimator__min_samples_leaf': list(range(1, 13)) # list(range(1, 21))\n}\n\n# now lets search our grid for our optimal values\ngrid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('bagged_dt',\n                                        BaggingRegressor(estimator=DecisionTreeRegressor(random_state=4),\n                                                         n_estimators=50,\n                                                         random_state=4))]),\n             n_jobs=-1,\n             param_grid={'bagged_dt__estimator__ccp_alpha': array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00]),\n                         'bagged_dt__estimator__max_depth': [1, 2, 3, 4, 5, 6,\n                                                             7, 8, 9, 10, 11,\n                                                             12],\n                         'bagged_dt__estimator__min_samples_leaf': [1, 2, 3, 4,\n                                                                    5, 6, 7, 8,\n                                                                    9, 10, 11,\n                                                                    12]},\n             scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('bagged_dt',\n                                        BaggingRegressor(estimator=DecisionTreeRegressor(random_state=4),\n                                                         n_estimators=50,\n                                                         random_state=4))]),\n             n_jobs=-1,\n             param_grid={'bagged_dt__estimator__ccp_alpha': array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00]),\n                         'bagged_dt__estimator__max_depth': [1, 2, 3, 4, 5, 6,\n                                                             7, 8, 9, 10, 11,\n                                                             12],\n                         'bagged_dt__estimator__min_samples_leaf': [1, 2, 3, 4,\n                                                                    5, 6, 7, 8,\n                                                                    9, 10, 11,\n                                                                    12]},\n             scoring='neg_root_mean_squared_error')estimator: PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('bagged_dt',\n                 BaggingRegressor(estimator=DecisionTreeRegressor(random_state=4),\n                                  n_estimators=50, random_state=4))])StandardScalerStandardScaler()bagged_dt: BaggingRegressorBaggingRegressor(estimator=DecisionTreeRegressor(random_state=4),\n                 n_estimators=50, random_state=4)estimator: DecisionTreeRegressorDecisionTreeRegressor(random_state=4)DecisionTreeRegressorDecisionTreeRegressor(random_state=4)\n\n\n\n# predict on the test set\ny_pred = grid_search.predict(X_test)\nbag_tree_rmse = mean_squared_error(y_test, y_pred, squared=False)\nprint(\"Best Bagged Decision Tree parameters: \", grid_search.best_params_)\n\nBest Bagged Decision Tree parameters:  {'bagged_dt__estimator__ccp_alpha': 0.001, 'bagged_dt__estimator__max_depth': 12, 'bagged_dt__estimator__min_samples_leaf': 2}\n\nprint(\"Bagged Decision Tree RMSE: \", bag_tree_rmse)\n\nBagged Decision Tree RMSE:  4.996459665683262\n\ndel(pipe, param_grid, grid_search, y_pred)\n\nOur RMSE is improving!\n\n\nAs usual, we follow the same approach in R. We specify the number of trees we want to use in the final model using times=500.\n\n# model specification\nbagdt_spec <- bag_tree(\n    mode = \"regression\",\n    tree_depth = tune(),\n    min_n = tune(),\n    cost_complexity = tune()\n  ) %>%\n  set_engine(\"rpart\", times = 50) |> # times = 500\n  set_mode(\"regression\")\n\n# checking out our hyperparameters\nbagdt_params <- parameters(cost_complexity(), tree_depth(), min_n())\nbagdt_grid <- grid_max_entropy(bagdt_params, size = 10, iter = 5)\n\n# make a workflow\nbagdt_wf <- workflow() |>\n  add_model(bagdt_spec) |>\n  add_recipe(rec)\n\n# Tuning\nn_cores <- detectCores() - 1 # Use all available cores except one\nregisterDoParallel(cores = n_cores)\nbagdt_rs <- tune_grid(\n  bagdt_wf,\n  resamples = folds,\n  grid = bagdt_grid,\n  metrics = metric_set(yardstick::rmse),\n  control = control_grid(save_pred = TRUE, parallel_over = \"everything\")\n)\n\n# Get the best hyperparameters\nbest_bagdt_params <- bagdt_rs |> \n  show_best(\"rmse\") |> \n  slice(1) |> \n  select(cost_complexity, tree_depth, min_n)\n\nbest_cc_bagdt <- best_bagdt_params$cost_complexity\nbest_depth_bagdt <- best_bagdt_params$tree_depth\nbest_min_n_bagdt <- best_bagdt_params$min_n\n\nshow_best(bagdt_rs)\n\n# A tibble: 5 × 9\n  cost_complexity tree_depth min_n .metric .estima…¹  mean     n std_err .config\n            <dbl>      <int> <int> <chr>   <chr>     <dbl> <int>   <dbl> <chr>  \n1   0.000000580           12    30 rmse    standard   6.41     5   0.509 Prepro…\n2   0.00000000243          8    29 rmse    standard   6.46     5   0.516 Prepro…\n3   0.0000460             10     9 rmse    standard   6.67     5   0.477 Prepro…\n4   0.0000560             13    24 rmse    standard   6.75     5   0.511 Prepro…\n5   0.00000144             5    22 rmse    standard   6.91     5   0.519 Prepro…\n# … with abbreviated variable name ¹​.estimator\n\n\nWe can again use the handy autoplot() function to visualize our best hyperparameter combinations:\n\nautoplot(bagdt_rs)\n\n\n\n\nAnd finally, we use our best hyperparameter values to construct our final model.\n\n# Create the final decision tree model with the best hyperparameters\nbagdt_final <- bag_tree(\n  cost_complexity = best_cc_bagdt,\n  tree_depth = best_depth_bagdt,\n  min_n = best_min_n_bagdt\n) %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"regression\")\n\n\n# Fit the final model on the whole training set\nbagdt_final_fit <- bagdt_wf %>%\n  update_model(bagdt_final) %>%\n  fit(data = training)\n\n# Make predictions on the holdout set and compute the RMSE\nbagdt_testing_preds <- predict(bagdt_final_fit, testing) |> \n  bind_cols(testing) |> \n  mutate(truth = as.double(DIC), estimate = as.double(.pred)) |> \n  metrics(truth = truth, estimate = estimate)\n\nbagdt_rmse <- bagdt_testing_preds %>% filter(.metric == \"rmse\") |>  pull(.estimate)\nprint(paste0(\"Bagged Tree RMSE:\", bagdt_rmse))\n\n[1] \"Bagged Tree RMSE:5.79701120060714\"\n\nrm(bagdt_final, bagdt_final_fit, bagdt_grid, bagdt_params, bagdt_rs, bagdt_spec, bagdt_testing_preds, bagdt_wf, best_bagdt_params)\n\n\n\n\nOur RMSE is getting better! Bagging our decision trees help us combat the overfitting that ordinary decision trees are prone to. However, there are still a few more ways we can improve this model!\n\n\nRandom Forest\nOur Bagged Tree RMSE was better than our normal decision tree algorithm, but we can still make this better! Bagging aggregates the predictions across all the trees, which reduces the variance of the overall procedure and results in improved predictive performance. However, simply bagging trees results in tree correlation that limits the effect of variance reduction. Random forests are built using the same fundamental principles as decision trees and bagging, but inject further randomness in the process which further reduces the variance.\nRandom forests help to reduce tree correlation by injecting more randomness into the tree-growing process. Specifically, while growing a decision tree during the bagging process, random forests specify a subset of the features to be used in each tree. This parameter is called m_try, and it controls the number of features to consider when randomly selecting a subset of features for each tree. For example, if there are 10 input features and mtry is set to 3, then for each tree in the Random Forest, a random subset of 3 features will be selected from the 10 input features. By controlling the number of features used for each tree, mtry helps to reduce the correlation between the trees and increase the diversity of the Random Forest. The optimal value for m_try depends on the problem, but a standard value for regression tasks ismtry=(p/3), where p is the number of features present in our dataset.\n\nPythonR\n\n\n\n# Create a pipeline with a scaler and the random forest regressor\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('rf_regressor', RandomForestRegressor(random_state=4))\n])\n\n# Define the parameter grid for hyperparameter tuning\nparam_grid = {\n    'rf_regressor__n_estimators': list(range(100, 1000, 50)),  \n    'rf_regressor__max_features': list(range(1, X_train.shape[1] - 10)),  # mtry\n    'rf_regressor__min_samples_leaf': list(range(1, 10))  # min_n\n}\n\n# Perform GridSearchCV for hyperparameter tuning\ngrid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\n# Make predictions and evaluate the model\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('rf_regressor',\n                                        RandomForestRegressor(random_state=4))]),\n             n_jobs=-1,\n             param_grid={'rf_regressor__max_features': [1, 2, 3, 4, 5, 6, 7, 8],\n                         'rf_regressor__min_samples_leaf': [1, 2, 3, 4, 5, 6, 7,\n                                                            8, 9],\n                         'rf_regressor__n_estimators': [100, 150, 200, 250, 300,\n                                                        350, 400, 450, 500, 550,\n                                                        600, 650, 700, 750, 800,\n                                                        850, 900, 950]},\n             scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('rf_regressor',\n                                        RandomForestRegressor(random_state=4))]),\n             n_jobs=-1,\n             param_grid={'rf_regressor__max_features': [1, 2, 3, 4, 5, 6, 7, 8],\n                         'rf_regressor__min_samples_leaf': [1, 2, 3, 4, 5, 6, 7,\n                                                            8, 9],\n                         'rf_regressor__n_estimators': [100, 150, 200, 250, 300,\n                                                        350, 400, 450, 500, 550,\n                                                        600, 650, 700, 750, 800,\n                                                        850, 900, 950]},\n             scoring='neg_root_mean_squared_error')estimator: PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('rf_regressor', RandomForestRegressor(random_state=4))])StandardScalerStandardScaler()RandomForestRegressorRandomForestRegressor(random_state=4)\n\ny_pred = grid_search.predict(X_test)\nrf_rmse = mean_squared_error(y_test, y_pred, squared=False)\nprint(\"Best Random Forest parameters: \", grid_search.best_params_)\n\nBest Random Forest parameters:  {'rf_regressor__max_features': 6, 'rf_regressor__min_samples_leaf': 1, 'rf_regressor__n_estimators': 950}\n\nprint(\"RMSE: \", rf_rmse)\n\n# Clear our environment\n\nRMSE:  5.0701395088877605\n\ndel(pipe, param_grid, grid_search, y_pred)\n\nOur RMSE is still improving!\n\n\nWe follow the same approach in R:\n\n# specifying tuning parameters\nrandfor_spec <- rand_forest(\n  trees = tune(),\n  mtry = tune(),\n  min_n = tune()\n) |>\n  set_mode(\"regression\") |>\n  set_engine(\"ranger\")\n\n# Looking at hyperparameters\nrandf_grid <-grid_regular(trees(), min_n(), mtry(range(1:13)), levels = 5)\n\n# Define new workflow\nrf_workflow <- workflow() |>\n  add_model(randfor_spec) |>\n  add_recipe(rec)\n\n# Tuning\ndoParallel::registerDoParallel()\nrandf_rs <- tune_grid(\n  rf_workflow,\n  resamples = folds,\n  grid = randf_grid,\n  metrics = metric_set(yardstick::rmse),\n  control = control_grid(save_pred = TRUE, parallel_over = \"everything\")\n)\n\nAs always, lets take a look at our best hyperparameter combinations:\n\nautoplot(randf_rs)\n\n\n\n\n\n# Get the best hyperparameters\nbest_randf_params <- randf_rs |> \n  show_best(\"rmse\") |> \n  slice(1) |> \n  select(trees, mtry, min_n)\n\nbest_trees_rf <- best_randf_params$trees\nbest_mtry_rf <- best_randf_params$mtry\nbest_min_n_rf <- best_randf_params$min_n\n\nshow_best(randf_rs)\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     7   500     2 rmse    standard    5.74     5   0.497 Preprocessor1_Model0…\n2     7  2000     2 rmse    standard    5.75     5   0.497 Preprocessor1_Model0…\n3     7  1500     2 rmse    standard    5.76     5   0.490 Preprocessor1_Model0…\n4     7  1000     2 rmse    standard    5.77     5   0.505 Preprocessor1_Model0…\n5     4  1000     2 rmse    standard    5.78     5   0.436 Preprocessor1_Model0…\n\n\n\n# Create the final random forest model with the best hyperparameters\nrandf_final <- rand_forest(\n  trees = best_trees_rf,\n  mtry = best_mtry_rf,\n  min_n = best_min_n_rf\n) |>\n  set_mode(\"regression\") |>\n  set_engine(\"ranger\")\n\n\n# fit on the training\nrandf_final_fit <- rf_workflow |> \n  update_model(randf_final) |> \n  fit(data = training)\n\n# predict on the test, calculate RMSE\nrf_testing_preds <- predict(randf_final_fit, testing) |> \n  bind_cols(testing) |> \n  mutate(truth = as.double(DIC), estimate = as.double(.pred)) |> \n  metrics(truth = truth, estimate = estimate)\n\nrf_rmse <- rf_testing_preds %>% filter(.metric == \"rmse\") |>  pull(.estimate)\n\nrm(randf_grid, randf_rs, randfor_spec, rf_workflow)\nprint(paste0(\"Random Forest RMSE:\", rf_rmse))\n\n[1] \"Random Forest RMSE:5.11937341919548\"\n\n\n\n\n\nOur model is still improving, but there is another powerful adjustment we can make to it.\n\n\nGradient Boosting\nWhereas random forests build an ensemble of deep independent trees, Gradient Boosting Machines (GBMS) build an ensemble of shallow trees in sequence, with each tree learning and improving on the previous one. Independently, these trees are relatively weak predictive models, but they can be boosted to produce a powerful ensemble of trees.\nThe boosting idea is to add new models to the ensemble sequentially. We start with a weak model, and sequentially boost its performance by continuing to build new trees, where each new tree in the sequence remedies where the previous one made had the largest prediction errors. To quantify the amount of information gained by each new tree, we use a loss function, which measures the difference between the predicted values and the true values for the training examples. The goal is to minimize this function, which means that we want to find the set of parameters for each new tree that best fit the data. To find the optimal set of parameters, we use gradient descent, which is an optimization algorithm that works by iteratively improving the parameters of the model to minimize the loss function. At each iteration, we calculate the gradient of the loss function with respect to the model parameters, and we use this gradient to update the parameters in a direction that minimizes the loss function. We continue this process until we reach a minimum value of the loss function or a stopping criterion is met. The important hyperparameter we use for this is known as the learning rate. The learning rate specifies how fast we want to descend down the gradient. Too large, and we might miss the minimum of the loss function. Too small, and we’ll need to many iterations to find the minimum.\nWe’re going to take a different approach to tuning this model. Instead of specifying all the parameters we want to tune in one step, we’re going to break it up. First, we’ll find the optimal learning rate. Based off the optimal learning rate, we’ll then tune the tree specific parameters (min_n/xgb__min_child_weight, tree_depth/xgb__max_depth, loss_reduction/xgb__gamma, xgb__n_estimators in python, we’ll set the number of trees manually in R). Then, based off these optimal values of the tree specific parameters, we’ll tune the stochastic parameters (sample_size/xgb__subsample, mtry/xgb__colsample_bytree).\n\nPythonR\n\n\nInstead of using sklearn for this algorithm in python, we’re going to use the XGB library. This is a library built specifically for gradient boosting, and will increase our computational efficiency.\n\n# instantiate our GBM\nxgb = XGBRegressor()\n\n# make our pipeline\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('xgb', xgb)\n])\n# First, tune the learning rate\nlearning_rates = np.linspace(0.01, 0.6, 50) # np.linspace(0.01, 0.6, 100)\nparam_grid_learning_rate = {\n    'xgb__learning_rate': learning_rates\n}\n\ngrid_search_lr = GridSearchCV(\n    pipeline, param_grid_learning_rate, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1\n)\ngrid_search_lr.fit(X_train, y_train)\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('xgb',\n                                        XGBRegressor(base_score=None,\n                                                     booster=None,\n                                                     callbacks=None,\n                                                     colsample_bylevel=None,\n                                                     colsample_bynode=None,\n                                                     colsample_bytree=None,\n                                                     early_stopping_rounds=None,\n                                                     enable_categorical=False,\n                                                     eval_metric=None,\n                                                     feature_types=None,\n                                                     gamma=None, gpu_id=None,\n                                                     grow_policy=None,\n                                                     importance_type=Non...\n       0.25081633, 0.26285714, 0.27489796, 0.28693878, 0.29897959,\n       0.31102041, 0.32306122, 0.33510204, 0.34714286, 0.35918367,\n       0.37122449, 0.38326531, 0.39530612, 0.40734694, 0.41938776,\n       0.43142857, 0.44346939, 0.4555102 , 0.46755102, 0.47959184,\n       0.49163265, 0.50367347, 0.51571429, 0.5277551 , 0.53979592,\n       0.55183673, 0.56387755, 0.57591837, 0.58795918, 0.6       ])},\n             scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('xgb',\n                                        XGBRegressor(base_score=None,\n                                                     booster=None,\n                                                     callbacks=None,\n                                                     colsample_bylevel=None,\n                                                     colsample_bynode=None,\n                                                     colsample_bytree=None,\n                                                     early_stopping_rounds=None,\n                                                     enable_categorical=False,\n                                                     eval_metric=None,\n                                                     feature_types=None,\n                                                     gamma=None, gpu_id=None,\n                                                     grow_policy=None,\n                                                     importance_type=Non...\n       0.25081633, 0.26285714, 0.27489796, 0.28693878, 0.29897959,\n       0.31102041, 0.32306122, 0.33510204, 0.34714286, 0.35918367,\n       0.37122449, 0.38326531, 0.39530612, 0.40734694, 0.41938776,\n       0.43142857, 0.44346939, 0.4555102 , 0.46755102, 0.47959184,\n       0.49163265, 0.50367347, 0.51571429, 0.5277551 , 0.53979592,\n       0.55183673, 0.56387755, 0.57591837, 0.58795918, 0.6       ])},\n             scoring='neg_root_mean_squared_error')estimator: PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('xgb',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, gamma=None, gpu_id=None,\n                              grow_policy=None, importance_type=None,\n                              interaction_constraints=None, learning_rate=None,\n                              max_bin=None, max_cat_threshold=None,\n                              max_cat_to_onehot=None, max_delta_step=None,\n                              max_depth=None, max_leaves=None,\n                              min_child_weight=None, missing=nan,\n                              monotone_constraints=None, n_estimators=100,\n                              n_jobs=None, num_parallel_tree=None,\n                              predictor=None, random_state=None, ...))])StandardScalerStandardScaler()XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=None, ...)\n\nbest_learning_rate = grid_search_lr.best_params_['xgb__learning_rate']\nprint(f\"Best learning rate: {best_learning_rate}\")\n\nBest learning rate: 0.3351020408163265\n\n\nNow, with our best learning rate, we’ll tune the tree-specific parameters. Note that the names of these hyperparameters have changed slightly since we’re using a different package.\n\n# Tree-specific parameters\nparam_grid_tree = {\n    'xgb__min_child_weight': range(1, 10, 3), # range(1, 21, 1),\n    'xgb__max_depth': range(1, 10, 3), # range(1, 21, 1),\n    'xgb__gamma': np.linspace(0, 1, 5), # np.linspace(0, 1, 20),\n    'xgb__n_estimators': range(100, 1500, 700)  # adjust the number of trees np.linspace(100, 1500, 30) \n}\n\n# Set the learning rate \npipeline.set_params(xgb__learning_rate=best_learning_rate)\n\n# find our best tree-specific parameters\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('xgb',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, gamma=None, gpu_id=None,\n                              grow_policy=None, importance_type=None,\n                              interaction_constraints=None,\n                              learning_rate=0.3351020408163265, max_bin=None,\n                              max_cat_threshold=None, max_cat_to_onehot=None,\n                              max_delta_step=None, max_depth=None,\n                              max_leaves=None, min_child_weight=None,\n                              missing=nan, monotone_constraints=None,\n                              n_estimators=100, n_jobs=None,\n                              num_parallel_tree=None, predictor=None,\n                              random_state=None, ...))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('xgb',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, gamma=None, gpu_id=None,\n                              grow_policy=None, importance_type=None,\n                              interaction_constraints=None,\n                              learning_rate=0.3351020408163265, max_bin=None,\n                              max_cat_threshold=None, max_cat_to_onehot=None,\n                              max_delta_step=None, max_depth=None,\n                              max_leaves=None, min_child_weight=None,\n                              missing=nan, monotone_constraints=None,\n                              n_estimators=100, n_jobs=None,\n                              num_parallel_tree=None, predictor=None,\n                              random_state=None, ...))])StandardScalerStandardScaler()XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.3351020408163265,\n             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=None, ...)\n\ngrid_search_tree = GridSearchCV(\n    pipeline, param_grid_tree, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1\n)\ngrid_search_tree.fit(X_train, y_train)\n\n# Get the best tree-specific parameters\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('xgb',\n                                        XGBRegressor(base_score=None,\n                                                     booster=None,\n                                                     callbacks=None,\n                                                     colsample_bylevel=None,\n                                                     colsample_bynode=None,\n                                                     colsample_bytree=None,\n                                                     early_stopping_rounds=None,\n                                                     enable_categorical=False,\n                                                     eval_metric=None,\n                                                     feature_types=None,\n                                                     gamma=None, gpu_id=None,\n                                                     grow_policy=None,\n                                                     importance_type=Non...\n                                                     min_child_weight=None,\n                                                     missing=nan,\n                                                     monotone_constraints=None,\n                                                     n_estimators=100,\n                                                     n_jobs=None,\n                                                     num_parallel_tree=None,\n                                                     predictor=None,\n                                                     random_state=None, ...))]),\n             n_jobs=-1,\n             param_grid={'xgb__gamma': array([0.  , 0.25, 0.5 , 0.75, 1.  ]),\n                         'xgb__max_depth': range(1, 10, 3),\n                         'xgb__min_child_weight': range(1, 10, 3),\n                         'xgb__n_estimators': range(100, 1500, 700)},\n             scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('xgb',\n                                        XGBRegressor(base_score=None,\n                                                     booster=None,\n                                                     callbacks=None,\n                                                     colsample_bylevel=None,\n                                                     colsample_bynode=None,\n                                                     colsample_bytree=None,\n                                                     early_stopping_rounds=None,\n                                                     enable_categorical=False,\n                                                     eval_metric=None,\n                                                     feature_types=None,\n                                                     gamma=None, gpu_id=None,\n                                                     grow_policy=None,\n                                                     importance_type=Non...\n                                                     min_child_weight=None,\n                                                     missing=nan,\n                                                     monotone_constraints=None,\n                                                     n_estimators=100,\n                                                     n_jobs=None,\n                                                     num_parallel_tree=None,\n                                                     predictor=None,\n                                                     random_state=None, ...))]),\n             n_jobs=-1,\n             param_grid={'xgb__gamma': array([0.  , 0.25, 0.5 , 0.75, 1.  ]),\n                         'xgb__max_depth': range(1, 10, 3),\n                         'xgb__min_child_weight': range(1, 10, 3),\n                         'xgb__n_estimators': range(100, 1500, 700)},\n             scoring='neg_root_mean_squared_error')estimator: PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('xgb',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, gamma=None, gpu_id=None,\n                              grow_policy=None, importance_type=None,\n                              interaction_constraints=None,\n                              learning_rate=0.3351020408163265, max_bin=None,\n                              max_cat_threshold=None, max_cat_to_onehot=None,\n                              max_delta_step=None, max_depth=None,\n                              max_leaves=None, min_child_weight=None,\n                              missing=nan, monotone_constraints=None,\n                              n_estimators=100, n_jobs=None,\n                              num_parallel_tree=None, predictor=None,\n                              random_state=None, ...))])StandardScalerStandardScaler()XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.3351020408163265,\n             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=None, ...)\n\nbest_min_child_weight = grid_search_tree.best_params_['xgb__min_child_weight']\nbest_max_depth = grid_search_tree.best_params_['xgb__max_depth']\nbest_gamma = grid_search_tree.best_params_['xgb__gamma']\nbest_n_estimators = grid_search_tree.best_params_['xgb__n_estimators']\n\nprint(f\"Best min_n: {best_min_child_weight}\")\n\nBest min_n: 1\n\nprint(f\"Best max_depth: {best_max_depth}\")\n\nBest max_depth: 4\n\nprint(f\"Best lost_reduction: {best_gamma}\")\n\nBest lost_reduction: 0.25\n\nprint(f\"Best n_trees: {best_n_estimators}\")\n\nBest n_trees: 800\n\n\nNow that we have the tree-specific parameters tuned, we can move on to the stochastic hyperparameters.\n\n# Stochastic parameters\nparam_grid_stochastic = {\n    'xgb__subsample': np.linspace(0.5, 1, 10), #np.linspace(0.5, 1, 20)\n    'xgb__colsample_bytree': np.linspace(1, 15, 15), # np.linspace(1, 15, 15)\n}\n\n# Use the best learning rate and tree-specific parameters \npipeline.set_params(\n    xgb__learning_rate=best_learning_rate,\n    xgb__min_child_weight=best_min_child_weight,\n    xgb__max_depth=best_max_depth,\n    xgb__gamma=best_gamma,\n    xgb__n_estimators=best_n_estimators\n)\n\n# Perform GridSearchCV for stochastic parameters\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('xgb',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, gamma=0.25, gpu_id=None,\n                              grow_policy=None, importance_type=None,\n                              interaction_constraints=None,\n                              learning_rate=0.3351020408163265, max_bin=None,\n                              max_cat_threshold=None, max_cat_to_onehot=None,\n                              max_delta_step=None, max_depth=4, max_leaves=None,\n                              min_child_weight=1, missing=nan,\n                              monotone_constraints=None, n_estimators=800,\n                              n_jobs=None, num_parallel_tree=None,\n                              predictor=None, random_state=None, ...))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('xgb',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, gamma=0.25, gpu_id=None,\n                              grow_policy=None, importance_type=None,\n                              interaction_constraints=None,\n                              learning_rate=0.3351020408163265, max_bin=None,\n                              max_cat_threshold=None, max_cat_to_onehot=None,\n                              max_delta_step=None, max_depth=4, max_leaves=None,\n                              min_child_weight=1, missing=nan,\n                              monotone_constraints=None, n_estimators=800,\n                              n_jobs=None, num_parallel_tree=None,\n                              predictor=None, random_state=None, ...))])StandardScalerStandardScaler()XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=0.25, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.3351020408163265,\n             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=4, max_leaves=None,\n             min_child_weight=1, missing=nan, monotone_constraints=None,\n             n_estimators=800, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=None, ...)\n\ngrid_search_stochastic = GridSearchCV(\n    pipeline, param_grid_stochastic, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1\n)\ngrid_search_stochastic.fit(X_train, y_train)\n\n# Get the best stochastic parameters\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('xgb',\n                                        XGBRegressor(base_score=None,\n                                                     booster=None,\n                                                     callbacks=None,\n                                                     colsample_bylevel=None,\n                                                     colsample_bynode=None,\n                                                     colsample_bytree=None,\n                                                     early_stopping_rounds=None,\n                                                     enable_categorical=False,\n                                                     eval_metric=None,\n                                                     feature_types=None,\n                                                     gamma=0.25, gpu_id=None,\n                                                     grow_policy=None,\n                                                     importance_type=Non...\n                                                     n_estimators=800,\n                                                     n_jobs=None,\n                                                     num_parallel_tree=None,\n                                                     predictor=None,\n                                                     random_state=None, ...))]),\n             n_jobs=-1,\n             param_grid={'xgb__colsample_bytree': array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n       14., 15.]),\n                         'xgb__subsample': array([0.5       , 0.55555556, 0.61111111, 0.66666667, 0.72222222,\n       0.77777778, 0.83333333, 0.88888889, 0.94444444, 1.        ])},\n             scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('xgb',\n                                        XGBRegressor(base_score=None,\n                                                     booster=None,\n                                                     callbacks=None,\n                                                     colsample_bylevel=None,\n                                                     colsample_bynode=None,\n                                                     colsample_bytree=None,\n                                                     early_stopping_rounds=None,\n                                                     enable_categorical=False,\n                                                     eval_metric=None,\n                                                     feature_types=None,\n                                                     gamma=0.25, gpu_id=None,\n                                                     grow_policy=None,\n                                                     importance_type=Non...\n                                                     n_estimators=800,\n                                                     n_jobs=None,\n                                                     num_parallel_tree=None,\n                                                     predictor=None,\n                                                     random_state=None, ...))]),\n             n_jobs=-1,\n             param_grid={'xgb__colsample_bytree': array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n       14., 15.]),\n                         'xgb__subsample': array([0.5       , 0.55555556, 0.61111111, 0.66666667, 0.72222222,\n       0.77777778, 0.83333333, 0.88888889, 0.94444444, 1.        ])},\n             scoring='neg_root_mean_squared_error')estimator: PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('xgb',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, gamma=0.25, gpu_id=None,\n                              grow_policy=None, importance_type=None,\n                              interaction_constraints=None,\n                              learning_rate=0.3351020408163265, max_bin=None,\n                              max_cat_threshold=None, max_cat_to_onehot=None,\n                              max_delta_step=None, max_depth=4, max_leaves=None,\n                              min_child_weight=1, missing=nan,\n                              monotone_constraints=None, n_estimators=800,\n                              n_jobs=None, num_parallel_tree=None,\n                              predictor=None, random_state=None, ...))])StandardScalerStandardScaler()XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=0.25, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.3351020408163265,\n             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=4, max_leaves=None,\n             min_child_weight=1, missing=nan, monotone_constraints=None,\n             n_estimators=800, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=None, ...)\n\nbest_subsample = grid_search_stochastic.best_params_['xgb__subsample']\nbest_colsample_bytree = grid_search_stochastic.best_params_['xgb__colsample_bytree']\n\nprint(f\"Best subsample: {best_subsample}\")\n\nBest subsample: 1.0\n\nprint(f\"Best colsample_bytree: {best_colsample_bytree}\")\n\nBest colsample_bytree: 1.0\n\n\nAnd finally, we can use the best values for all of these hyperparameters to build our final model.\n\n# Train the final model with the best hyperparameters on the training set (excluding the holdout set)\nfinal_pipeline = pipeline.set_params(\n    xgb__learning_rate=best_learning_rate,\n    xgb__min_child_weight=best_min_child_weight,\n    xgb__max_depth=best_max_depth,\n    xgb__gamma=best_gamma,\n    xgb__n_estimators=best_n_estimators,\n    xgb__subsample=best_subsample,\n    xgb__colsample_bytree=best_colsample_bytree\n)\nfinal_pipeline.fit(X_train, y_train)\n\n# Validate the final model on the holdout set\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('xgb',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=1.0, early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, gamma=0.25, gpu_id=None,\n                              grow_policy=None, importance_type=None,\n                              interaction_constraints=None,\n                              learning_rate=0.3351020408163265, max_bin=None,\n                              max_cat_threshold=None, max_cat_to_onehot=None,\n                              max_delta_step=None, max_depth=4, max_leaves=None,\n                              min_child_weight=1, missing=nan,\n                              monotone_constraints=None, n_estimators=800,\n                              n_jobs=None, num_parallel_tree=None,\n                              predictor=None, random_state=None, ...))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('xgb',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=1.0, early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, gamma=0.25, gpu_id=None,\n                              grow_policy=None, importance_type=None,\n                              interaction_constraints=None,\n                              learning_rate=0.3351020408163265, max_bin=None,\n                              max_cat_threshold=None, max_cat_to_onehot=None,\n                              max_delta_step=None, max_depth=4, max_leaves=None,\n                              min_child_weight=1, missing=nan,\n                              monotone_constraints=None, n_estimators=800,\n                              n_jobs=None, num_parallel_tree=None,\n                              predictor=None, random_state=None, ...))])StandardScalerStandardScaler()XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=1.0, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=0.25, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.3351020408163265,\n             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=4, max_leaves=None,\n             min_child_weight=1, missing=nan, monotone_constraints=None,\n             n_estimators=800, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=None, ...)\n\ntest_preds = final_pipeline.predict(X_test)\ngb_rmse = mean_squared_error(y_test, test_preds, squared=False)\nprint(f\"test RMSE: {gb_rmse}\")\n\ntest RMSE: 5.038058931912597\n\n\n\n\nSimilarly in R, we start by tuning the learning rate. As we have before, we use a grid of values to test to find which produces the lowest RMSE on the training data.\n\n# tune learning rate\ntune_lr <- boost_tree(learn_rate = tune()) |> \n  set_mode('regression') |> \n  set_engine(\"xgboost\") \n\n# make our cv grid\nlr_grid <- expand.grid(learn_rate = seq(0.0001, 0.5, length.out = 10)) # seq(0.0001, 0.5, length.out = 200))\n\n# now our workflow\ntune_wf_lr <- workflow() |> \n      add_model(tune_lr) |> \n      add_recipe(rec) \n\n# tuning\nfit_tune_lr <- tune_wf_lr |> \n      tune_grid(resamples = folds, grid = lr_grid)\n\nshow_best(fit_tune_lr, metric = \"rmse\")\n\n# A tibble: 5 × 7\n  learn_rate .metric .estimator  mean     n std_err .config              \n       <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1      0.444 rmse    standard    7.39     5   0.357 Preprocessor1_Model09\n2      0.5   rmse    standard    7.95     5   0.308 Preprocessor1_Model10\n3      0.389 rmse    standard    8.20     5   0.700 Preprocessor1_Model08\n4      0.333 rmse    standard    9.57     5   0.776 Preprocessor1_Model07\n5      0.278 rmse    standard   19.2      5   0.365 Preprocessor1_Model06\n\nbest_lr <- as.numeric(show_best(fit_tune_lr, metric = \"rmse\")[1,1])\n\nUsing this learning rate, we then tune the tree specific parameters. Unlike our approach in python, we’re fixing the number of trees to be 1000 in R.\n\nregisterDoParallel(cores = n_cores)\n# Specifying that we want to tune the tree-specific parameters\nlropt_tune_spec <-\n    boost_tree(\n    learn_rate = best_lr,\n    trees = 500,\n    min_n = tune(),\n    tree_depth = tune(),\n    loss_reduction = tune()\n  ) |>\n  set_engine(\"xgboost\") |>\n  set_mode('regression')\n\n# Setting up the parameters to be tuned and the grid to search over\ntree_params <- parameters(tree_depth(), min_n(), loss_reduction())\ntrees_grid <- grid_max_entropy(tree_params, size = 5, iter = 2) # size=20\n\n# Defining a new workflow, adding our models and tuning parameters\ntree_wf <- workflow() |> add_model(lropt_tune_spec) |> add_recipe(rec)\n\n#  tune our parameters\nfit_tune_trees <- tree_wf |> tune_grid(\n  resamples = folds,\n  grid = trees_grid,\n  metrics = metric_set(rmse),\n  control = control_grid(save_pred = TRUE, parallel_over = \"everything\")\n)\n\n# extract the best values\nshow_best(fit_tune_trees, metric =\"rmse\")\n\n# A tibble: 5 × 9\n  min_n tree_depth loss_reduction .metric .estimator  mean     n std_err .config\n  <int>      <int>          <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>  \n1     4          3       4.49e-10 rmse    standard    6.64     5   0.349 Prepro…\n2    13          4       4.79e- 8 rmse    standard    6.88     5   0.313 Prepro…\n3    28          4       2.59e-10 rmse    standard    7.06     5   0.374 Prepro…\n4    14         11       4.41e- 3 rmse    standard    7.15     5   0.387 Prepro…\n5    38         14       7.50e- 8 rmse    standard    7.23     5   0.263 Prepro…\n\nopt_min_n <- as.numeric(show_best(fit_tune_trees, metric = \"rmse\")[1,1])\nopt_tree_depth <- as.numeric(show_best(fit_tune_trees, metric = \"rmse\")[1,2])\nopt_loss_red <- as.numeric(show_best(fit_tune_trees, metric = \"rmse\")[1,3])\n\nWe now use our optimal tree parameters to tune the stochastic parameters.\n\nregisterDoParallel(cores = n_cores)\n\n# Specifying that we want to tune the stoachastic-specific parameters\nstoch_tune_spec <-\n  boost_tree(\n    learn_rate = best_lr,\n    trees = 500, # 1000\n    min_n = opt_min_n,\n    tree_depth = opt_tree_depth,\n    loss_reduction = opt_loss_red,\n    mtry = tune(),\n    sample_size = tune()\n  ) |>\n  set_engine(\"xgboost\") |>\n  set_mode('regression')\n\n# Define the parameters\nstoch_params <- parameters(\n  finalize(mtry(), training),\n  sample_size= sample_prop())\n\n# Generate a grid of parameter values\nstoch_grid <- grid_max_entropy(stoch_params, size = 5, iter = 2)\n        \nstoch_tune_wf <- workflow() |> \n  add_model(stoch_tune_spec) |> \n  add_recipe(rec)\n\nfit_tune_stoch <- stoch_tune_wf |> tune_grid(\n  resamples = folds,\n  grid = stoch_grid,\n  metrics = metric_set(rmse),\n  control = control_grid(save_pred = TRUE, parallel_over = \"everything\")\n)\nshow_best(fit_tune_stoch, metric = \"rmse\")\n\n# A tibble: 5 × 8\n   mtry sample_size .metric .estimator  mean     n std_err .config             \n  <int>       <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1    18       0.861 rmse    standard    7.20     5   0.353 Preprocessor1_Model2\n2    10       0.716 rmse    standard    7.76     5   0.380 Preprocessor1_Model3\n3    15       0.468 rmse    standard    8.02     5   0.357 Preprocessor1_Model1\n4     7       0.332 rmse    standard   10.2      5   0.455 Preprocessor1_Model5\n5    16       0.104 rmse    standard   29.1      5   1.86  Preprocessor1_Model4\n\nopt_mtry <- as.numeric(show_best(fit_tune_stoch, metric = \"rmse\")[1,1])\nopt_ss <- as.numeric(show_best(fit_tune_stoch, metric = \"rmse\")[1,2])\n\nFinally, we add all of this into the final model\n\nfinal_model <- boost_tree(learn_rate = best_lr,\n                          trees = 500, # 1000\n                          min_n = opt_min_n,\n                          mtry = opt_mtry,\n                          tree_depth = opt_tree_depth,\n                          loss_reduction = opt_loss_red,\n                          sample_size = opt_ss,\n                          ) |>\n  set_mode(\"regression\") |>\n  set_engine(\"xgboost\", early_stopping_rounds = 10)\n\nfinal_wf <- workflow() |>\n  add_model(final_model) |>\n  add_recipe(rec)\n\nfinal_fit <- final_wf |> fit(training)\n\n# predict on the test, calculate RMSE\ngb_testing_preds <- predict(final_fit, testing) |> \n  bind_cols(testing) |> \n  mutate(truth = as.double(DIC), estimate = as.double(.pred)) |> \n  metrics(truth = truth, estimate = estimate)\n\ngb_rmse <- gb_testing_preds %>% filter(.metric == \"rmse\") |>  pull(.estimate)\n\nprint(gb_rmse)\n\n[1] 5.502003\n\n\n\n\n\n\n\nSupport Vector Machine\nSupport Vector Machines (SVMs) are a little different than what we’ve done so far. The goal of SVMs in our case is to move our data into higher dimensions to find a hyperplane that best fits our data points, while balancing model complexity and error tolerance. SVMs accomplish this by transforming the input data into a higher-dimensional space using the kernel trick (implicitly transforming the data into a higher dimensional space without explicitly performing the transformation, this helps the algorithm capture complex relationships in the data without compromising too much computational efficiency). SVMs identify a subset of the training data, called support vectors, which are critical in determining the optimal hyperplane. Once the optimal hyperplane is found, new data points can be projected onto it to make predictions.\nThe kernel function is what maps the features to a higher-dimensional feature space. Although there are different types of kernel functions to choose from, the linear kernel function is the most appropriate for our data. The linear kernel function is particularly useful when the data is linearly separable or when a linear relationship exists between the features and the target variable.\n\nPythonR\n\n\n\n# Create the pipeline\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('svm', SVR(kernel='linear'))\n])\n\n# Define the tuning grid\nparam_grid = {\n    'svm__C': np.logspace(-5, 5, 10), # np.logspace(-5, 5, 100),\n}\n\n# Perform grid search with cross-validation\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\ngrid.fit(X_train, y_train)\n\n# Train the final model with the best parameters\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('svm', SVR(kernel='linear'))]),\n             n_jobs=-1,\n             param_grid={'svm__C': array([1.00000000e-05, 1.29154967e-04, 1.66810054e-03, 2.15443469e-02,\n       2.78255940e-01, 3.59381366e+00, 4.64158883e+01, 5.99484250e+02,\n       7.74263683e+03, 1.00000000e+05])},\n             scoring='neg_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('svm', SVR(kernel='linear'))]),\n             n_jobs=-1,\n             param_grid={'svm__C': array([1.00000000e-05, 1.29154967e-04, 1.66810054e-03, 2.15443469e-02,\n       2.78255940e-01, 3.59381366e+00, 4.64158883e+01, 5.99484250e+02,\n       7.74263683e+03, 1.00000000e+05])},\n             scoring='neg_mean_squared_error')estimator: PipelinePipeline(steps=[('scaler', StandardScaler()), ('svm', SVR(kernel='linear'))])StandardScalerStandardScaler()SVRSVR(kernel='linear')\n\nbest_params = grid.best_params_\nfinal_model = grid.best_estimator_\n\n# Make predictions on the validation set\ny_pred = final_model.predict(X_test)\n\n# Calculate the RMSE\nsvm_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f\"RMSE: {svm_rmse}\")\n\nRMSE: 4.8924520113025824\n\n\n\n\n\n# make our SVM model\nsvm_model <- svm_linear(\n  cost = tune()\n) |>\n  set_mode(\"regression\") |>\n  set_engine(\"kernlab\")\n\n# add model to workflow\nsvm_workflow <- workflow() |>\n  add_model(svm_model) |>\n  add_recipe(rec)\n\n# specify our tuning grid\ntune_svm_grid <- grid_regular(\n  cost(range = c(-10, 10)),\n  levels = 5\n)\n\n# tune our model\nfit_tune_svm <- svm_workflow |>\n  tune_grid(\n    resamples = folds,\n    grid = tune_svm_grid,\n    metrics = metric_set(rmse),\n    control = control_grid(save_pred = TRUE, parallel_over = \"everything\")\n  )\n\n# extract the best values for the hyperparameters\nbest_svm_params <- fit_tune_svm |>\n  show_best(metric = \"rmse\") |>\n  dplyr::slice(1) |>\n  select(cost)\n\n# use our best values in the final model\nfinal_svm_model <- svm_linear(\n  cost = best_svm_params$cost\n) |>\n  set_mode(\"regression\") |>\n  set_engine(\"kernlab\")\n\n# make a new workflow\nfinal_svm_wf <- workflow() |>\n  add_model(final_svm_model) |>\n  add_recipe(rec)\n\nfinal_svm_fit <- final_svm_wf |> fit(training)\n\n Setting default kernel parameters  \n\nfinal_svm_pred <- final_svm_fit |> predict(testing)\n\nsvm_rmse <- final_svm_pred |>\n  bind_cols(testing) |>\n  rmse(truth = DIC, estimate = .pred) %>%\n  pull(.estimate)\n\nprint(\"RMSE:\")\n\n[1] \"RMSE:\"\n\nprint(svm_rmse)\n\n[1] 5.63947\n\n\n\n\n\n\n\nEvaluation\nWe’ve now finished running our last model. Now, we want to consolidate all of our RMSE scores to compare our different models’ performances. We can easily do this with a bar chart.\n\nPythonR\n\n\n\n\n\nplt.clf()\nmodel_names = [\"LR\", \"Lasso\", \"Ridge\", \"KNN\", \"DT\", \"Bag DT\", \"RF\", \"GB\", \"SVM\"]\nrmses = [rmse_test_lr, ridge_rmse, lasso_rmse, rmse_knn, dt_rmse, bag_tree_rmse, rf_rmse, gb_rmse, svm_rmse]\n\n\nmodel_scores = sorted(zip(model_names, rmses), key=lambda x: x[1])\nmodel_names, rmses = zip(*model_scores)\n\nfig, ax = plt.subplots()\nax.bar(model_names, rmses)\n\n<BarContainer object of 9 artists>\n\nax.set_xlabel(\"Model\")\nax.set_ylabel(\"RMSE\")\nax.set_title(\"Model Performance Comparison\")\nplt.xticks(rotation = 45)\n\n([0, 1, 2, 3, 4, 5, 6, 7, 8], [Text(0, 0, 'SVM'), Text(1, 0, 'Lasso'), Text(2, 0, 'LR'), Text(3, 0, 'Bag DT'), Text(4, 0, 'GB'), Text(5, 0, 'RF'), Text(6, 0, 'Ridge'), Text(7, 0, 'DT'), Text(8, 0, 'KNN')])\n\nplt.show()\n\n\n\n\n\n\n\nmodel_names <- c(\"Linear Regression\", \"Lasso\", \"Ridge\", \"KNN\", \"Decision Tree\", \"Bagged DT\", \"Random Forest\", \"Gradient Boosting\", \"SVM\")\nrmses <- c(rmse_lr, lasso_rmse, ridge_rmse, rmse_knn, dt_rmse, bagdt_rmse, rf_rmse, gb_rmse, svm_rmse)\n\ndf_rmse <- data.frame(Model = model_names, RMSE = rmses)\ndf_rmse <- df_rmse[order(df_rmse$RMSE), ]\n\nggplot(df_rmse, aes(x = reorder(Model, RMSE), y = RMSE, fill = Model)) +\n  geom_bar(stat = \"identity\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(x = \"Model\", y = \"RMSE\", title = \"Model Performance Comparison\")\n\n\n\n\n\n\n\n\n\nConclusion\nOur SVM model performed the best in Python, while the linear regression performed the best in R. Overall, our linear regression performed consistently well across both languages. This is expected, given that our predictors are highly linearly correlated with the DIC variable. Our random forest regressor and support vector machines also performed quite well, which is sensible since these models also perform well with linearly correlated data. When we tuned our more complicated models (random forest, gradient boosting, svm), we kept the hyperparameter combination grids to search over fairly small. This is because our computation time exponentially increases as we specify more combinations of parameters to search over. This means that the combinations of hyperparameters we searched over may not have had an optimal combination of values, resulting in a lower score. In future work, we could greatly increase the size of the grids to search over, which would probably give us much better performances.\nAnother thing to note is that while we used RMSE to evaluate our models, this isn’t the only metric that can be used for evaluation, and the best metric depends on the problem. RMSE is a common measure for regression models and is appropriate for our case where we are predicting a continuous variable. In future work, we could explore more feature engineering techniques, or combining our models into an ensemble.\n\n\n\n\nCitationBibTeX citation:@online{bartnik2023,\n  author = {Andrew Bartnik},\n  title = {Predicting Dissolved Inorganic Carbon {(DIC)} Concentrations\n    Using Different Algorithms in {Python} and {R}},\n  date = {2023-03-18},\n  url = {https://andrewbartnik.github.io/Portfolio/ocean-modeling},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndrew Bartnik. 2023. “Predicting Dissolved Inorganic Carbon (DIC)\nConcentrations Using Different Algorithms in Python and R.” March\n18, 2023. https://andrewbartnik.github.io/Portfolio/ocean-modeling."
  },
  {
    "objectID": "Portfolio/texas_storm/index.html#overview",
    "href": "Portfolio/texas_storm/index.html#overview",
    "title": "Analyzing the Effects of the Texas February 2021 Storm on the Houston metropolitan Area",
    "section": "",
    "text": "“In February 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”1 For more background, check out these engineering and political perspectives.\nWe were tasked with:\n- estimating the number of homes in Houston that lost power as a result of the first two storms\n- investigating if socioeconomic factors are predictors of communities recovery from a power outage\nOur analysis was based on remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite. We used VNP46A1 to detect differences in night lights before and after the storm to identify areas that lost electric power.\nTo determine the number of homes that lost power, we linked (spatially join) these areas with OpenStreetMap data on buildings and roads.\nTo investigate potential socioeconomic factors that influenced recovery, we linked our analysis with data from the US Census Bureau.\n\n\n\n\nWe used NASA’s Worldview to explore the data around the day of the storm. There were several days with too much cloud cover to be useful, but 2021-02-07 and 2021-02-16 provide two clear, contrasting images to visualize the extent of the power outage in Texas.\nVIIRS data is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Many NASA Earth data products are distributed in 10x10 degree tiles in sinusoidal equal-area projection. Tiles are identified by their horizontal and vertical position in the grid. Houston lies on the border of tiles h08v05 and h08v06. We therefore needed to download two tiles per date.\n\n\n\nTypically highways account for a large portion of the night lights observable from space (see Google’s Earth at Night). To minimize falsely identifying areas with reduced traffic as areas without power, we used a buffer to ignore areas near highways.\nOpenStreetMap (OSM) is a collaborative project which creates publicly available geographic data of the world. Ingesting this data into a database where it can be subsetted and processed is a large undertaking. Fortunately, third party companies redistribute OSM data. We used Geofabrik’s download sites to retrieve a shapefile of all highways in Texas and prepared a Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area. \n\n\n\nWe also obtained building data from OpenStreetMap. We again downloaded from Geofabrick and prepared a GeoPackage containing only houses in the Houston metropolitan area.\n\n\n\n\nWe cannot readily get socioeconomic information for every home, so instead we obtained data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019. The folder ACS_2019_5YR_TRACT_48.gdb is an ArcGIS “file geodatabase”, a multi-file proprietary format that’s roughly analogous to a GeoPackage file. We used st_layers() to explore the contents of the geodatabase. Each layer contains a subset of the fields documents in the ACS metadata. The geodatabase contains a layer holding the geometry information, separate from the layers holding the ACS attributes. We had to combine the geometry with the attributes to get a feature layer that sf can use."
  },
  {
    "objectID": "Portfolio/texas_storm/index.html#footnotes",
    "href": "Portfolio/texas_storm/index.html#footnotes",
    "title": "Analyzing the Effects of the Texas February 2021 Storm on the Houston metropolitan Area",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWikipedia. 2021. “2021 Texas power crisis.” Last modified October 2, 2021. https://en.wikipedia.org/wiki/2021_Texas_power_crisis.↩︎"
  },
  {
    "objectID": "Portfolio/lexus/index.html",
    "href": "Portfolio/lexus/index.html",
    "title": "Evaluating Biodiversity Sentiment and Performing Topic Analysis",
    "section": "",
    "text": "I used the Nexis Uni database to evaluate the sentiment surrounding biodiversity articles, and then analyzed the topics that articles fall under.\nI first accessed the Nexis Uni database through the UCSB library. I then chose to search for articles related to biodiversity and downloaded a batch of 100 .docx files.\nhttps://github.com/JBGruber/LexisNexisTools/wiki/Downloading-Files-From-Nexis\n\n# Reading in files\npre_files &lt;- list.files(pattern = \".docx\", \n                        path = \"/Users/andrewbartnik/Desktop/misc/MEDS/Spring/text/text_analysis/data/lab2/files2\",\n                       full.names = TRUE, \n                       recursive = TRUE, \n                       ignore.case = TRUE)\n\n# Saving objects\npre_dat &lt;- lnt_read(pre_files)\nbing_sent &lt;- get_sentiments('bing')\nnrc_sent &lt;- get_sentiments(\"nrc\")\n\nI cleaned artifacts of the data collection process (date strings, etc), and saved the metadata, article title, and paragraph contents to their own objects - adding this all together into a dataframe.\n\nmeta &lt;- pre_dat@meta\narticles &lt;- pre_dat@articles\nparagraphs &lt;- pre_dat@paragraphs\n\ndata &lt;- tibble(Date = meta$Date, Headline = meta$Headline, id = pre_dat@articles$ID, text = pre_dat@articles$Article)\n\nhead(data)\n\n# A tibble: 6 × 4\n  Date       Headline                                                   id text \n  &lt;date&gt;     &lt;chr&gt;                                                   &lt;int&gt; &lt;chr&gt;\n1 2023-04-11 -Kao Releases Biodiversity Report Based on the TNFD Fr…     1 \"Kao…\n2 2023-04-11 -Kao Releases Biodiversity Report Based on the TNFD Fr…     2 \"Apr…\n3 2023-04-04 -PLATINUM RECOGNITION FOR MOY PARK'S BIODIVERSITY INIT…     3 \"Apr…\n4 2023-04-04 -PLATINUM RECOGNITION FOR MOY PARK'S BIODIVERSITY INIT…     4 \"Moy…\n5 2023-03-22 -Umicore - Growing our business with Zero Harm to biod…     5 \"Bio…\n6 2023-03-22 -Umicore - Growing our business with Zero Harm to biod…     6 \"Mar…\n\n\nExploring the data a bit!\n\n# date freq\ndate_freq &lt;- data %&gt;%\n  group_by(Date) %&gt;%\n  summarise(freq = n())\n\nggplot(date_freq, aes(x = Date, y = freq)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  theme_minimal() +\n  labs(x = \"Date\", y = \"Frequency\", title = \"Frequency of Dates 2022-2023\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nHere, I’m unnesting each word in the article and joining them to their bing sentiment scores.\n\n# Extract words\ntext &lt;- data |&gt; unnest_tokens(output = word, input = text, token = 'words')\n\n# join to sent \nsent_words &lt;- text |&gt; \n  anti_join(stop_words, by = \"word\") |&gt; \n  inner_join(bing_sent, by = 'word') |&gt; \n  mutate(sent_num = case_when(sentiment == \"negative\" ~ -1, sentiment == \"positive\" ~ 1))\n\nWe can calculate the average sentiment for each article\n\nsent_article2 &lt;-sent_words |&gt; \n  count(id, sentiment) |&gt; \n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |&gt; \n  mutate(polarity = positive - negative) \nmean(sent_article2$polarity)\n\n[1] 2.05\n\n\nNow we can look at the distribution of sentiments across all the articles:\n\nggplot(sent_article2, aes(x = id)) + \n  theme_classic() + \n  geom_col(aes(y = positive), stat = 'identity', fill = 'lightblue') + \n  geom_col(aes(y = negative), stat = 'identity', fill = 'red', alpha = 0.5) + \n  labs(title = 'Sentiment analysis: Biodiversity', y = \"Sentiment Score\")\n\n\n\n\nAnd for the fun part - after we filter out stop words, we can join our words to the nrc_sent object which shows the associated sentiment for each word:\n\nnrc_word_counts_bio &lt;- text |&gt; anti_join(stop_words, by = \"word\") |&gt; inner_join(nrc_sent) |&gt; count(word, sentiment, sort = T) \n\n\n# Now to look at specific nrc sentiments\nsent_counts2 &lt;- text |&gt; \n  anti_join(stop_words, by = 'word') |&gt; \n  group_by(id) |&gt; \n  inner_join(nrc_sent) |&gt; \n  group_by(sentiment) |&gt; \n  count(word, sentiment, sort = T)\n\n# Evaluating contribution to sentiment\nsent_counts2 |&gt; group_by(sentiment) |&gt; slice_max(n, n = 10) |&gt; ungroup() |&gt; mutate(word = reorder(word, n)) |&gt; ggplot(aes(x=n, y=word, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales= \"free_y\") + labs(x = \"Contribution to sentiment\", y = NULL)\n\n\n\n\n“Loss” seems to be associated with strongly negative emotions. Conservation also seems to elicit a strong emotional response.\nSoil, wind, and diverse are associated with more negative emotions, which is misleading. Since these terms are pretty neutral in this context, we can reclassify their associated sentiments.\n\n## Reclassifying\nsent_counts2 |&gt; filter(!word %in% c(\"soil\", \"wind\", \"diverse\")) |&gt; group_by(sentiment) |&gt; slice_max(n, n = 10) |&gt; ungroup() |&gt; mutate(word = reorder(word, n)) |&gt; ggplot(aes(x=n, y=word, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales= \"free_y\") + labs(x = \"Contribution to sentiment\", y = NULL)\n\n\n\n\nThats better, harm and crisis are more appropriately associated with negative sentiment than soil and wind\nNow we can plot the amount of nrc emotion words as a percentage of all the emotion words used each day. Then we can analyze the distribution of emotion words change over time.\n\nnrc_emotion_counts &lt;- text %&gt;%\n  inner_join(nrc_sent) %&gt;%\n  count(Date, sentiment)\n\n# Aggregate the text from articles published on the same day\ntotal_emotions_by_day &lt;- nrc_emotion_counts %&gt;%\n  group_by(Date) %&gt;%\n  summarise(total = sum(n))\n\n# Calculate the percentage of NRC emotion words per day\nnrc_emotion_percentage &lt;- nrc_emotion_counts %&gt;%\n  left_join(total_emotions_by_day, by = \"Date\") %&gt;%\n  mutate(percentage = n / total * 100)\n\n# Plot the distribution of emotion words over time\nggplot(nrc_emotion_percentage, aes(x = Date, y = percentage, color = sentiment)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Date\", y = \"Percentage of Emotion Words\", title = \"Distribution of Emotion Words Over Time\") +\n  theme(legend.title = element_blank(), legend.position = \"bottom\", legend.box = \"horizontal\")\n\n\n\n\nThe sentiment around the biodiversity term is overwhelmingly positive over the given time period. Trust was the second most frequent sentiment. This could be because most of the articles I downloaded were related to conservation efforts and achievements. The only time negative sentiment surpasses positive sentiment was at the end of February, when the only article published within a 6 day period was titled “Majorda locals object to alleged destruction of biodiversity, natural flow of water by RVNL”"
  },
  {
    "objectID": "Portfolio/lexus/index.html#sentiment-analysis",
    "href": "Portfolio/lexus/index.html#sentiment-analysis",
    "title": "Evaluating Biodiversity Sentiment and Performing Topic Analysis",
    "section": "",
    "text": "I used the Nexis Uni database to evaluate the sentiment surrounding biodiversity articles, and then analyzed the topics that articles fall under.\nI first accessed the Nexis Uni database through the UCSB library. I then chose to search for articles related to biodiversity and downloaded a batch of 100 .docx files.\nhttps://github.com/JBGruber/LexisNexisTools/wiki/Downloading-Files-From-Nexis\n\n# Reading in files\npre_files &lt;- list.files(pattern = \".docx\", \n                        path = \"/Users/andrewbartnik/Desktop/misc/MEDS/Spring/text/text_analysis/data/lab2/files2\",\n                       full.names = TRUE, \n                       recursive = TRUE, \n                       ignore.case = TRUE)\n\n# Saving objects\npre_dat &lt;- lnt_read(pre_files)\nbing_sent &lt;- get_sentiments('bing')\nnrc_sent &lt;- get_sentiments(\"nrc\")\n\nI cleaned artifacts of the data collection process (date strings, etc), and saved the metadata, article title, and paragraph contents to their own objects - adding this all together into a dataframe.\n\nmeta &lt;- pre_dat@meta\narticles &lt;- pre_dat@articles\nparagraphs &lt;- pre_dat@paragraphs\n\ndata &lt;- tibble(Date = meta$Date, Headline = meta$Headline, id = pre_dat@articles$ID, text = pre_dat@articles$Article)\n\nhead(data)\n\n# A tibble: 6 × 4\n  Date       Headline                                                   id text \n  &lt;date&gt;     &lt;chr&gt;                                                   &lt;int&gt; &lt;chr&gt;\n1 2023-04-11 -Kao Releases Biodiversity Report Based on the TNFD Fr…     1 \"Kao…\n2 2023-04-11 -Kao Releases Biodiversity Report Based on the TNFD Fr…     2 \"Apr…\n3 2023-04-04 -PLATINUM RECOGNITION FOR MOY PARK'S BIODIVERSITY INIT…     3 \"Apr…\n4 2023-04-04 -PLATINUM RECOGNITION FOR MOY PARK'S BIODIVERSITY INIT…     4 \"Moy…\n5 2023-03-22 -Umicore - Growing our business with Zero Harm to biod…     5 \"Bio…\n6 2023-03-22 -Umicore - Growing our business with Zero Harm to biod…     6 \"Mar…\n\n\nExploring the data a bit!\n\n# date freq\ndate_freq &lt;- data %&gt;%\n  group_by(Date) %&gt;%\n  summarise(freq = n())\n\nggplot(date_freq, aes(x = Date, y = freq)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  theme_minimal() +\n  labs(x = \"Date\", y = \"Frequency\", title = \"Frequency of Dates 2022-2023\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nHere, I’m unnesting each word in the article and joining them to their bing sentiment scores.\n\n# Extract words\ntext &lt;- data |&gt; unnest_tokens(output = word, input = text, token = 'words')\n\n# join to sent \nsent_words &lt;- text |&gt; \n  anti_join(stop_words, by = \"word\") |&gt; \n  inner_join(bing_sent, by = 'word') |&gt; \n  mutate(sent_num = case_when(sentiment == \"negative\" ~ -1, sentiment == \"positive\" ~ 1))\n\nWe can calculate the average sentiment for each article\n\nsent_article2 &lt;-sent_words |&gt; \n  count(id, sentiment) |&gt; \n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |&gt; \n  mutate(polarity = positive - negative) \nmean(sent_article2$polarity)\n\n[1] 2.05\n\n\nNow we can look at the distribution of sentiments across all the articles:\n\nggplot(sent_article2, aes(x = id)) + \n  theme_classic() + \n  geom_col(aes(y = positive), stat = 'identity', fill = 'lightblue') + \n  geom_col(aes(y = negative), stat = 'identity', fill = 'red', alpha = 0.5) + \n  labs(title = 'Sentiment analysis: Biodiversity', y = \"Sentiment Score\")\n\n\n\n\nAnd for the fun part - after we filter out stop words, we can join our words to the nrc_sent object which shows the associated sentiment for each word:\n\nnrc_word_counts_bio &lt;- text |&gt; anti_join(stop_words, by = \"word\") |&gt; inner_join(nrc_sent) |&gt; count(word, sentiment, sort = T) \n\n\n# Now to look at specific nrc sentiments\nsent_counts2 &lt;- text |&gt; \n  anti_join(stop_words, by = 'word') |&gt; \n  group_by(id) |&gt; \n  inner_join(nrc_sent) |&gt; \n  group_by(sentiment) |&gt; \n  count(word, sentiment, sort = T)\n\n# Evaluating contribution to sentiment\nsent_counts2 |&gt; group_by(sentiment) |&gt; slice_max(n, n = 10) |&gt; ungroup() |&gt; mutate(word = reorder(word, n)) |&gt; ggplot(aes(x=n, y=word, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales= \"free_y\") + labs(x = \"Contribution to sentiment\", y = NULL)\n\n\n\n\n“Loss” seems to be associated with strongly negative emotions. Conservation also seems to elicit a strong emotional response.\nSoil, wind, and diverse are associated with more negative emotions, which is misleading. Since these terms are pretty neutral in this context, we can reclassify their associated sentiments.\n\n## Reclassifying\nsent_counts2 |&gt; filter(!word %in% c(\"soil\", \"wind\", \"diverse\")) |&gt; group_by(sentiment) |&gt; slice_max(n, n = 10) |&gt; ungroup() |&gt; mutate(word = reorder(word, n)) |&gt; ggplot(aes(x=n, y=word, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales= \"free_y\") + labs(x = \"Contribution to sentiment\", y = NULL)\n\n\n\n\nThats better, harm and crisis are more appropriately associated with negative sentiment than soil and wind\nNow we can plot the amount of nrc emotion words as a percentage of all the emotion words used each day. Then we can analyze the distribution of emotion words change over time.\n\nnrc_emotion_counts &lt;- text %&gt;%\n  inner_join(nrc_sent) %&gt;%\n  count(Date, sentiment)\n\n# Aggregate the text from articles published on the same day\ntotal_emotions_by_day &lt;- nrc_emotion_counts %&gt;%\n  group_by(Date) %&gt;%\n  summarise(total = sum(n))\n\n# Calculate the percentage of NRC emotion words per day\nnrc_emotion_percentage &lt;- nrc_emotion_counts %&gt;%\n  left_join(total_emotions_by_day, by = \"Date\") %&gt;%\n  mutate(percentage = n / total * 100)\n\n# Plot the distribution of emotion words over time\nggplot(nrc_emotion_percentage, aes(x = Date, y = percentage, color = sentiment)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Date\", y = \"Percentage of Emotion Words\", title = \"Distribution of Emotion Words Over Time\") +\n  theme(legend.title = element_blank(), legend.position = \"bottom\", legend.box = \"horizontal\")\n\n\n\n\nThe sentiment around the biodiversity term is overwhelmingly positive over the given time period. Trust was the second most frequent sentiment. This could be because most of the articles I downloaded were related to conservation efforts and achievements. The only time negative sentiment surpasses positive sentiment was at the end of February, when the only article published within a 6 day period was titled “Majorda locals object to alleged destruction of biodiversity, natural flow of water by RVNL”"
  },
  {
    "objectID": "Portfolio/lexus/index.html#topic-analysis",
    "href": "Portfolio/lexus/index.html#topic-analysis",
    "title": "Evaluating Biodiversity Sentiment and Performing Topic Analysis",
    "section": "Topic Analysis",
    "text": "Topic Analysis\nNow for topic analysis. We’ll first build the corpus using corpus() from {quanteda}:\nNext, we can use tokens(), also from {quanteda}, to construct a tokens object. tokens() takes a range of arguments related to cleaning the data.\n\n# Making a corpus of the articles\ncorpus_bio &lt;- corpus(x = articles, text_field = \"Article\")\nstories_stats &lt;- summary(corpus_bio)\n\n#Lets take a look\nhead(stories_stats)\n\n   Text Types Tokens Sentences ID\n1 text1   378    767        26  1\n2 text2   383    775        26  2\n3 text3   228    394        12  3\n4 text4   221    386        12  4\n5 text5   655   1790        60  5\n6 text6   661   1798        60  6\n\n\nNow, we’ll create a stopwords lexicon and remove each word contained in it from our tokens object. The quanteda function tokens_select() lets us do the removal.\n\n# Removing punctuation, numbers, and stopwords.\ntoks2 &lt;- tokens(corpus_bio, remove_punct = T, remove_numbers = T)\nadd_stops &lt;- stopwords(\"en\")\ntoks3 &lt;- tokens_select(toks2, pattern = add_stops, selection = \"remove\")\n\nNow we can transform our data to prepare for topic modeling. We can create a document-feature matrix with quanteda::dfm(). Topic modeling doesn’t work with empty rows in this matrix, so we’ll need to remove those. We can do that here using {slam}, which is designed to deal with sparse matrices like ours.\n\n# Creating the document feature matrix\ndfm_bio &lt;- dfm(toks3, tolower = T)\ndfm &lt;- dfm_trim(dfm_bio, min_docfreq = 2)\nhead(dfm)\n\nDocument-feature matrix of: 6 documents, 3,297 features (89.93% sparse) and 1 docvar.\n       features\ndocs    kao corporation published report business risk opportunity biodiversity\n  text1  17           2         3      7        9    3           2           11\n  text2  17           2         3      7        9    3           2           11\n  text3   0           0         0      0        5    1           0            9\n  text4   0           0         0      0        5    1           0            9\n  text5   0           0         0      0        5    2           0           33\n  text6   0           0         0      0        5    2           0           33\n       features\ndocs    tnfd case\n  text1    5    1\n  text2    5    1\n  text3    0    0\n  text4    0    0\n  text5    0    0\n  text6    0    0\n[ reached max_nfeat ... 3,287 more features ]\n\n\n\nsel_idx &lt;- slam::row_sums(dfm)&gt;0\ndfm &lt;- dfm[sel_idx,]\n\nWe’re almost ready to run a model. We just have to come up with an initial value for k, the number of latent topics present in the data. Lets first start with 10. We will experiment with a few more values of k later.\nRunning topicmodels::LDA() produces two probability distributions: theta, a distribution over k topics (here, 10) within each document and beta,the distribution v terms within each topic, where v is our vocabulary (total unique words in our data set).\n\nK = 10\n\n# Lets first set K\nk &lt;- 10\n\ntopicModel_k10 &lt;- LDA(dfm, \n                     k, \n                     method= \"Gibbs\", \n                     control = list(iter = 500,\n                             verbose = 25))\n\nK = 10; V = 3297; M = 100\nSampling 500 iterations!\nIteration 25 ...\nIteration 50 ...\nIteration 75 ...\nIteration 100 ...\nIteration 125 ...\nIteration 150 ...\nIteration 175 ...\nIteration 200 ...\nIteration 225 ...\nIteration 250 ...\nIteration 275 ...\nIteration 300 ...\nIteration 325 ...\nIteration 350 ...\nIteration 375 ...\nIteration 400 ...\nIteration 425 ...\nIteration 450 ...\nIteration 475 ...\nIteration 500 ...\nGibbs sampling completed!\n\n\nLet’s examine at our results. posterior() extracts theta and beta\n\n# Extracting the result and examininng the attributes\nresult &lt;- posterior(topicModel_k10)\nattributes(result)\n\n$names\n[1] \"terms\"  \"topics\"\n\n# Getting beta, theta\nbeta &lt;- result$terms\ntheta &lt;- result$topics\nvocab &lt;- colnames(beta)\n\ndim(beta)\n\n[1]   10 3297\n\ndim(theta)\n\n[1] 100  10\n\nterms(topicModel_k10, 10)\n\n      Topic 1        Topic 2      Topic 3        Topic 4         Topic 5       \n [1,] \"biodiversity\" \"said\"       \"global\"       \"environment\"   \"biodiversity\"\n [2,] \"risks\"        \"wildlife\"   \"climate\"      \"biodiversity\"  \"forest\"      \n [3,] \"conservation\" \"area\"       \"conservation\" \"national\"      \"management\"  \n [4,] \"impact\"       \"project\"    \"change\"       \"local\"         \"content\"     \n [5,] \"natural\"      \"lake\"       \"loss\"         \"protect\"       \"department\"  \n [6,] \"projects\"     \"park\"       \"biological\"   \"environmental\" \"also\"        \n [7,] \"risk\"         \"also\"       \"conference\"   \"minister\"      \"meeting\"     \n [8,] \"investment\"   \"protection\" \"biodiversity\" \"development\"   \"officer\"     \n [9,] \"can\"          \"land\"       \"framework\"    \"including\"     \"programme\"   \n[10,] \"also\"         \"living\"     \"china\"        \"efforts\"       \"council\"     \n      Topic 6        Topic 7       Topic 8        Topic 9        \n [1,] \"biodiversity\" \"food\"        \"biodiversity\" \"biodiversity\" \n [2,] \"business\"     \"said\"        \"cent\"         \"environmental\"\n [3,] \"report\"       \"water\"       \"companies\"    \"water\"        \n [4,] \"kao\"          \"may\"         \"fund\"         \"nature\"       \n [5,] \"nature\"       \"climate\"     \"$\"            \"site\"         \n [6,] \"help\"         \"natural\"     \"financial\"    \"eritrea\"      \n [7,] \"development\"  \"agriculture\" \"use\"          \"sites\"        \n [8,] \"activities\"   \"major\"       \"group\"        \"areas\"        \n [9,] \"part\"         \"resources\"   \"funds\"        \"protect\"      \n[10,] \"provide\"      \"services\"    \"climate\"      \"life\"         \n      Topic 10      \n [1,] \"species\"     \n [2,] \"biodiversity\"\n [3,] \"diversity\"   \n [4,] \"can\"         \n [5,] \"ecosystems\"  \n [6,] \"human\"       \n [7,] \"animals\"     \n [8,] \"conservation\"\n [9,] \"health\"      \n[10,] \"many\"        \n\n\nAn alternative to specifying k based on theory or a hypothesis is to run a series of models using a range of k values. ldatuning::FindTopicsNumber gives us the tools for this.\n\n# Running a sequence of K values\nresult &lt;- FindTopicsNumber(dfm, \n                           topics = seq(from = 2, to = 20, by = 1), metrics = c(\"CaoJuan2009\", \"Deveaud2014\"),\n                           method = \"Gibbs\",\n                           verbose = T)\n\nfit models... done.\ncalculate metrics:\n  CaoJuan2009... done.\n  Deveaud2014... done.\n\n# Plotting our results\nFindTopicsNumber_plot(result)\n\n\n\n\nLet’s estimate another model, this time with a new value of k.\n\n\nK = 5\n\n# Set K\nk &lt;- 5\n\n# Taking the same approach as before\ntopicModel_k5 &lt;- LDA(dfm, \n                     k, \n                     method= \"Gibbs\", \n                     control = list(iter = 500,\n                             verbose = 25))\n\nK = 5; V = 3297; M = 100\nSampling 500 iterations!\nIteration 25 ...\nIteration 50 ...\nIteration 75 ...\nIteration 100 ...\nIteration 125 ...\nIteration 150 ...\nIteration 175 ...\nIteration 200 ...\nIteration 225 ...\nIteration 250 ...\nIteration 275 ...\nIteration 300 ...\nIteration 325 ...\nIteration 350 ...\nIteration 375 ...\nIteration 400 ...\nIteration 425 ...\nIteration 450 ...\nIteration 475 ...\nIteration 500 ...\nGibbs sampling completed!\n\nresult &lt;- posterior(topicModel_k10)\nattributes(result)\n\n$names\n[1] \"terms\"  \"topics\"\n\nbeta &lt;- result$terms\ntheta &lt;- result$topics\nvocab &lt;- colnames(beta)\n\ndim(beta)\n\n[1]   10 3297\n\ndim(theta)\n\n[1] 100  10\n\nterms(topicModel_k5, 10)\n\n      Topic 1        Topic 2        Topic 3        Topic 4       \n [1,] \"biodiversity\" \"species\"      \"ecosystems\"   \"biodiversity\"\n [2,] \"risks\"        \"climate\"      \"biodiversity\" \"said\"        \n [3,] \"financial\"    \"conservation\" \"water\"        \"forest\"      \n [4,] \"said\"         \"diversity\"    \"report\"       \"development\" \n [5,] \"companies\"    \"loss\"         \"health\"       \"national\"    \n [6,] \"impact\"       \"global\"       \"human\"        \"environment\" \n [7,] \"per\"          \"change\"       \"activities\"   \"also\"        \n [8,] \"climate\"      \"areas\"        \"important\"    \"area\"        \n [9,] \"fund\"         \"natural\"      \"eritrea\"      \"lake\"        \n[10,] \"last\"         \"also\"         \"kao\"          \"project\"     \n      Topic 5        \n [1,] \"biodiversity\" \n [2,] \"nature\"       \n [3,] \"environmental\"\n [4,] \"business\"     \n [5,] \"new\"          \n [6,] \"protect\"      \n [7,] \"can\"          \n [8,] \"help\"         \n [9,] \"site\"         \n[10,] \"work\"         \n\n\nOne last value for K:\n\n\nK = 7\n\nk &lt;- 7\n\ntopicModel_k7 &lt;- LDA(dfm, \n                     k, \n                     method= \"Gibbs\", \n                     control = list(iter = 500,\n                             verbose = 25))\n\nK = 7; V = 3297; M = 100\nSampling 500 iterations!\nIteration 25 ...\nIteration 50 ...\nIteration 75 ...\nIteration 100 ...\nIteration 125 ...\nIteration 150 ...\nIteration 175 ...\nIteration 200 ...\nIteration 225 ...\nIteration 250 ...\nIteration 275 ...\nIteration 300 ...\nIteration 325 ...\nIteration 350 ...\nIteration 375 ...\nIteration 400 ...\nIteration 425 ...\nIteration 450 ...\nIteration 475 ...\nIteration 500 ...\nGibbs sampling completed!\n\nresult &lt;- posterior(topicModel_k10)\nattributes(result)\n\n$names\n[1] \"terms\"  \"topics\"\n\nbeta &lt;- result$terms\ntheta &lt;- result$topics\nvocab &lt;- colnames(beta)\n\ndim(beta)\n\n[1]   10 3297\n\ndim(theta)\n\n[1] 100  10\n\nterms(topicModel_k7, 10)\n\n      Topic 1         Topic 2        Topic 3        Topic 4       \n [1,] \"biodiversity\"  \"biodiversity\" \"can\"          \"biodiversity\"\n [2,] \"environment\"   \"risks\"        \"biodiversity\" \"forest\"      \n [3,] \"environmental\" \"conservation\" \"ecosystems\"   \"said\"        \n [4,] \"nature\"        \"also\"         \"human\"        \"lake\"        \n [5,] \"site\"          \"activities\"   \"world\"        \"also\"        \n [6,] \"new\"           \"nature\"       \"health\"       \"national\"    \n [7,] \"protect\"       \"business\"     \"many\"         \"department\"  \n [8,] \"work\"          \"impact\"       \"animal\"       \"project\"     \n [9,] \"local\"         \"risk\"         \"land\"         \"meeting\"     \n[10,] \"protecting\"    \"report\"       \"aquatic\"      \"state\"       \n      Topic 5        Topic 6        Topic 7       \n [1,] \"species\"      \"biodiversity\" \"climate\"     \n [2,] \"diversity\"    \"per\"          \"conference\"  \n [3,] \"natural\"      \"cent\"         \"conservation\"\n [4,] \"said\"         \"climate\"      \"convention\"  \n [5,] \"resources\"    \"companies\"    \"framework\"   \n [6,] \"areas\"        \"financial\"    \"national\"    \n [7,] \"conservation\" \"year\"         \"montreal\"    \n [8,] \"food\"         \"fund\"         \"china\"       \n [9,] \"extinction\"   \"$\"            \"biological\"  \n[10,] \"plants\"       \"last\"         \"eritrea\"     \n\n\nAlthough the Findtopicsnumber() optimization metrics didn’t suggest a consistent value for K, k=5 seems like a good number for interpretability. Running more topics resulted in more low-value words and worse interpretability between topics.\nNow we can plot the top terms in each topic, and the distribution of topics across a sample of the documents\n\n# Using k=5:\nbio_topics &lt;- tidy(topicModel_k5, matrix = \"beta\")\n\n# Finding the top terms in each topic, turning them into a cool plot\ntop_terms &lt;- bio_topics |&gt; group_by(topic) |&gt; top_n(10, beta) |&gt; ungroup() |&gt; arrange(topic, -beta)\ntop_terms %&gt;%\n  mutate(term = reorder_within(term, beta, topic, sep = \"\")) %&gt;%\n  ggplot(aes(term, beta, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free_y\") +\n  scale_x_reordered()+\n  coord_flip()\n\n\n\n\nWe can assign names to the topics so we know what we are working with. We can name the topics by interpreting the overall theme or idea they represent, but for now let’s just name them by their top terms.\n\ntopic_words &lt;- terms(topicModel_k10, 5)\ntopic_names &lt;- apply(topic_words, 2, paste, collapse = \"\")\n\nWe can explore the theta matrix, which contains the distribution of each topic over each document.\n\nexample_ids &lt;- c(5:10)\nn &lt;- length(example_ids)\nexample_props &lt;- theta[example_ids,]\ncolnames(example_props) &lt;- topic_names\nviz_df &lt;- melt(cbind(data.frame(example_props),\n                     document = factor(1:n),\n                     variable.name = 'topic',\n                     id.vars = 'document'))\n\n# Plotting\nggplot(data = viz_df, aes(variable, value, fill = document), ylab = \"proportion\") +\n  geom_bar(stat=\"identity\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  coord_flip() +\n  facet_wrap(~ document, ncol = n)\n\n\n\n\nBased off the top terms in each topic, topic 1 seems to be most closely associated with different levels of government and their efforts to work on biodiversity projects. Topic 2 seems most closely associated with management and conservation, topic 3 seems to be associated with the theme of international climate change, topic 4 looks to be associated with risk and impact assessment for companies, and topic 5 seems to be associated with the theme of climate change’s effect on natural ecosystems. K = 5 seems to have been a reasonable choice."
  },
  {
    "objectID": "Portfolio/fatal_preds/index.html",
    "href": "Portfolio/fatal_preds/index.html",
    "title": "Predicting Climbing Accident Outcomes from Article Sentiment",
    "section": "",
    "text": "Here, we are going to predict the outcome of climbing incidents (fatal or nonfatal) using text from climbing incident articles. This data set includes more possible predictors than the text alone, but for this model we will only use the text variable.\n\nurlfile =\"https://raw.githubusercontent.com/MaRo406/EDS-231-text-sentiment/main/data/climbing_reports_model_dat.csv\"\nincidents_df&lt;-readr::read_csv(url(urlfile))\nhead(incidents_df)\n\n# A tibble: 6 × 5\n  ID    `Accident Title`                                    Publi…¹ Text  Deadly\n  &lt;chr&gt; &lt;chr&gt;                                                 &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 1     Failure of Rappel Setup (Protection Pulled Out), I…    1990 \"Col…     NA\n2 2     Failure of Rappel—Failure to Check System, British…    1990 \"Bri…     NA\n3 3     Fall into Crevasse, Climbing Alone, Inadequate Equ…    1990 \"Alb…     NA\n4 4     Fall into Crevasse, Climbing Unroped, British Colu…    1990 \"Bri…     NA\n5 5     Fall Into Crevasse, Unroped, Inadequate Equipment,…    1990 \"On …      1\n6 6     Fall into Moat, Descending Unroped, Poor Position,…    1990 \"Alb…      1\n# … with abbreviated variable name ¹​`Publication Year`\n\n\nWe’ll split our data into training and test portions\n\nset.seed(1234)\n\n# Adding a fatality binary indicator\nincidents2class &lt;- incidents_df |&gt; \n  mutate(fatal = factor(if_else(\n    is.na(Deadly) ,\n    \"non-fatal\", \"fatal\")))\n\n# Splitting our data, stratifying on the outcome\nincidents_split &lt;- initial_split(incidents2class, strata = fatal)\n\n# Now for making our training and testing sets:\nincidents_train &lt;- training(incidents_split)\nincidents_test &lt;- testing(incidents_split)\n\nWe use recipe() from tidymodels to specify the predictor and outcome variables and the data.\n\n# Making the recipe. We're saying we want to predict fatal by using Text\nincidents_rec &lt;- recipe(fatal ~ Text, data = incidents_train)\n\nNext we add some familiar pre-processing steps on our Text variable. We first tokenize the articles to word level, filter to the most common words, and calculate tf-idf.\n\n# Adding steps to our recipe\nrecipe &lt;- incidents_rec %&gt;%\n  step_tokenize(Text) %&gt;% \n  step_tokenfilter(Text, max_tokens = 1000) %&gt;%\n  step_tfidf(Text) #new one from textrecipes"
  },
  {
    "objectID": "Portfolio/fatal_preds/index.html#naive-bayes",
    "href": "Portfolio/fatal_preds/index.html#naive-bayes",
    "title": "Predicting Climbing Accident Outcomes from Article Sentiment",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\n# Making the workflow object\nincidents_wf &lt;- workflow() %&gt;%\n  add_recipe(recipe)\n\nWe want to use Naive Bayes to classify the outcomes.\n\nnb_spec &lt;- naive_Bayes() %&gt;%\n  set_mode(\"classification\") %&gt;% #set modeling context\n  set_engine(\"naivebayes\") #method for fitting model\n\nnb_spec\n\nNaive Bayes Model Specification (classification)\n\nComputational engine: naivebayes \n\n\nNow we are ready to add our model to the workflow and fit it to the training data\n\n# Fitting the model to our workflow\nnb_fit &lt;- incidents_wf %&gt;%\n  add_model(nb_spec) %&gt;%\n  fit(data = incidents_train)\n\nNext up is model evaluation. We’ll stretch our training data further and use resampling to evaluate our Naive Bayes model. Here we create 10-fold cross-validation sets, and use them to estimate performance.\n\n# Set the seed for reproducibility\nset.seed(234)\nincidents_folds &lt;- vfold_cv(incidents_train) #default is v = 10\n\nincidents_folds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits             id    \n   &lt;list&gt;             &lt;chr&gt; \n 1 &lt;split [1869/208]&gt; Fold01\n 2 &lt;split [1869/208]&gt; Fold02\n 3 &lt;split [1869/208]&gt; Fold03\n 4 &lt;split [1869/208]&gt; Fold04\n 5 &lt;split [1869/208]&gt; Fold05\n 6 &lt;split [1869/208]&gt; Fold06\n 7 &lt;split [1869/208]&gt; Fold07\n 8 &lt;split [1870/207]&gt; Fold08\n 9 &lt;split [1870/207]&gt; Fold09\n10 &lt;split [1870/207]&gt; Fold10\n\n\n\nnb_wf &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(nb_spec)\n\nnb_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: naive_Bayes()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\nNaive Bayes Model Specification (classification)\n\nComputational engine: naivebayes \n\n\nTo estimate its performance, we fit the model many times, once to each of these resampled folds, and then evaluate on the holdout part of each resampled fold.\n\nnb_rs &lt;- fit_resamples(\n  nb_wf,\n  incidents_folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\nWe then extract the relevant information using collect_metrics() and collect_predictions() and examine the performance metrics.\n\nnb_rs_metrics &lt;- collect_metrics(nb_rs)\nnb_rs_predictions &lt;- collect_predictions(nb_rs)\nnb_rs_metrics\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.800    10 0.00474 Preprocessor1_Model1\n2 roc_auc  binary     0.730    10 0.0118  Preprocessor1_Model1\n\n\nWe’ll use two performance metrics: accuracy and ROC AUC. Accuracy is the proportion of the data that is predicted correctly. The ROC curve plots the true positive rate against the false positive rate; AUC closer to 1 indicates a better-performing model, while AUC closer to 0.5 indicates a model that does no better than random guessing.\n\nnb_rs_predictions %&gt;%\n  group_by(id) %&gt;%\n  roc_curve(truth = fatal, .pred_fatal) %&gt;%\n  autoplot() +\n  labs(\n    \"Resamples\",\n    title = \"ROC curve for Climbing Incident Reports\"\n  )\n\n\n\n\nAnother model method involves the confusion matrix. A confusion matrix tabulates a model’s false positives and false negatives for each class.\n\nconf_mat_resampled(nb_rs, tidy = FALSE) %&gt;% #compute matrix for each fold then average\n  autoplot(type = \"heatmap\")"
  },
  {
    "objectID": "Portfolio/fatal_preds/index.html#lasso-regression",
    "href": "Portfolio/fatal_preds/index.html#lasso-regression",
    "title": "Predicting Climbing Accident Outcomes from Article Sentiment",
    "section": "Lasso Regression",
    "text": "Lasso Regression\nNow, lets try a different model - lasso regression to see if we can score better. We follow the same approach that we did for the Naive Bayes model.\n\n# Specifying the model\nlasso_spec &lt;- logistic_reg(penalty = 0.01, mixture =1) |&gt; \n  set_mode('classification') |&gt; \n  set_engine(\"glmnet\")\n\n# Making the workflow  \nlasso_wf &lt;- workflow() |&gt; \n  add_recipe(recipe) |&gt; \n  add_model(lasso_spec)\n\nlasso_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 0.01\n  mixture = 1\n\nComputational engine: glmnet \n\n\nWe then fit the model to the resamples, and collect our metrics\n\nset.seed(123)\nlasso_rs &lt;- fit_resamples(\n  lasso_wf,\n  incidents_folds,\n  control = control_resamples(save_pred = T)\n)\n\nlasso_rs_metrics &lt;- collect_metrics(lasso_rs)\nlasso_rs_predictions &lt;- collect_predictions(lasso_rs)\nlasso_rs_metrics\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.895    10 0.00538 Preprocessor1_Model1\n2 roc_auc  binary     0.947    10 0.00593 Preprocessor1_Model1\n\n\nNow we can check our predictions:\n\nlasso_rs_predictions |&gt; \n  group_by(id) |&gt; \n  roc_curve(truth = fatal, .pred_fatal) |&gt; \n  autoplot() + labs(color = \"Resamples\",\n                    title = \"ROC for Climbing Incident Reports\")"
  },
  {
    "objectID": "Portfolio/fatal_preds/index.html#logistic-regression",
    "href": "Portfolio/fatal_preds/index.html#logistic-regression",
    "title": "Predicting Climbing Accident Outcomes from Article Sentiment",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nNow for Logistic Regression:\n\nlog_spec &lt;- logistic_reg(penalty = tune(), mixture = 1) |&gt; set_mode(\"classification\") |&gt; set_engine(\"glmnet\")\nlog_wf &lt;- workflow() |&gt; \n  add_recipe(recipe) |&gt; \n  add_model(log_spec)\n\nset.seed(123)\nlambda_grid &lt;- grid_regular(penalty(), levels = 30)\nlog_rs &lt;- tune_grid(log_wf, incidents_folds, grid = lambda_grid, control = control_resamples(save_pred = T))\n\nLets evaluate our Logistic Regression model:\n\n# Collecting metrics\ncollect_metrics(log_rs)\n\n# A tibble: 60 × 7\n    penalty .metric  .estimator  mean     n std_err .config              \n      &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1 1   e-10 accuracy binary     0.889    10 0.00644 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.921    10 0.00613 Preprocessor1_Model01\n 3 2.21e-10 accuracy binary     0.889    10 0.00644 Preprocessor1_Model02\n 4 2.21e-10 roc_auc  binary     0.921    10 0.00613 Preprocessor1_Model02\n 5 4.89e-10 accuracy binary     0.889    10 0.00644 Preprocessor1_Model03\n 6 4.89e-10 roc_auc  binary     0.921    10 0.00613 Preprocessor1_Model03\n 7 1.08e- 9 accuracy binary     0.889    10 0.00644 Preprocessor1_Model04\n 8 1.08e- 9 roc_auc  binary     0.921    10 0.00613 Preprocessor1_Model04\n 9 2.40e- 9 accuracy binary     0.889    10 0.00644 Preprocessor1_Model05\n10 2.40e- 9 roc_auc  binary     0.921    10 0.00613 Preprocessor1_Model05\n# … with 50 more rows\n\nautoplot(log_rs) + labs(title = \"Lasso Performance Across Regular Penalties\")\n\n\n\n\n\nlog_rs |&gt; show_best(\"roc_auc\")\n\n# A tibble: 5 × 7\n   penalty .metric .estimator  mean     n std_err .config              \n     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 0.00853  roc_auc binary     0.947    10 0.00586 Preprocessor1_Model24\n2 0.00386  roc_auc binary     0.945    10 0.00532 Preprocessor1_Model23\n3 0.00174  roc_auc binary     0.939    10 0.00500 Preprocessor1_Model22\n4 0.0189   roc_auc binary     0.936    10 0.00596 Preprocessor1_Model25\n5 0.000788 roc_auc binary     0.932    10 0.00527 Preprocessor1_Model21\n\nlog_rs |&gt; show_best(\"accuracy\")\n\n# A tibble: 5 × 7\n   penalty .metric  .estimator  mean     n std_err .config              \n     &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 0.00386  accuracy binary     0.904    10 0.00581 Preprocessor1_Model23\n2 0.00174  accuracy binary     0.899    10 0.00672 Preprocessor1_Model22\n3 0.00853  accuracy binary     0.898    10 0.00516 Preprocessor1_Model24\n4 0.000788 accuracy binary     0.897    10 0.00624 Preprocessor1_Model21\n5 0.000356 accuracy binary     0.896    10 0.00648 Preprocessor1_Model20\n\n\nUsing our best model, we perform a final fit and evaluate its performance:\n\n# Finalizing the workflow. extracting the metrics\nchosen_acc &lt;- log_rs |&gt; select_by_one_std_err(metric= \"accuracy\", -penalty)\nfinal_log &lt;- finalize_workflow(log_wf, chosen_acc)\nfinal_log &lt;- fit(final_log, incidents_train)\nfinal_log |&gt; extract_fit_parsnip() |&gt; tidy() |&gt; arrange(estimate)\n\n# A tibble: 1,001 × 3\n   term                 estimate penalty\n   &lt;chr&gt;                   &lt;dbl&gt;   &lt;dbl&gt;\n 1 tfidf_Text_died        -277.  0.00853\n 2 tfidf_Text_body        -158.  0.00853\n 3 tfidf_Text_death       -132.  0.00853\n 4 tfidf_Text_fatal        -76.8 0.00853\n 5 tfidf_Text_breathing    -47.7 0.00853\n 6 tfidf_Text_past         -32.5 0.00853\n 7 tfidf_Text_observed     -32.5 0.00853\n 8 tfidf_Text_occurred     -31.7 0.00853\n 9 tfidf_Text_noticed      -31.1 0.00853\n10 tfidf_Text_either       -31.0 0.00853\n# … with 991 more rows\n\nlast_fit(final_log, incidents_split) |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.916 Preprocessor1_Model1\n2 roc_auc  binary         0.951 Preprocessor1_Model1"
  },
  {
    "objectID": "Portfolio/fatal_preds/index.html#random-forest-classifier",
    "href": "Portfolio/fatal_preds/index.html#random-forest-classifier",
    "title": "Predicting Climbing Accident Outcomes from Article Sentiment",
    "section": "Random Forest Classifier",
    "text": "Random Forest Classifier\nNow we’re going to try a more powerful algorithm: the random forest classifier.\n\n# Specifying the model\nrf_spec &lt;- rand_forest() |&gt; set_mode(\"classification\") |&gt; set_engine(\"ranger\")\n\nWe’re first going to conduct an initial out-of-the-box model fit on the training data and prediction on the test test data. Assess the performance of this initial model.\n\n# Initializing a workflow, fitting it to resamples\nrf_workflow &lt;- workflow() |&gt; add_recipe(recipe) |&gt; add_model(rf_spec)\nrf_fit &lt;- rf_workflow |&gt; fit(data = incidents_train)\n\nrf_rs &lt;- fit_resamples(\n  rf_workflow,\n  incidents_folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\n\nrf_rs_metrics &lt;- collect_metrics(rf_rs)\nnb_rs_predictions &lt;- collect_predictions(rf_rs)\nrf_rs_metrics\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.867    10 0.00668 Preprocessor1_Model1\n2 roc_auc  binary     0.947    10 0.00400 Preprocessor1_Model1\n\n\nInitial model has an accuracy of 86% and roc_auc of 0.95. Not bad at all.\nNow, we’re going to tune the hyperparameters:\n\n# specifying which parameters to tune, adding this new model back into a workflow\ntune_rf_spec &lt;- rand_forest(trees = tune(), mtry = tune(), min_n = tune()) |&gt; set_mode(\"classification\") |&gt; set_engine(\"ranger\")\ntune_rf_wf &lt;- workflow() |&gt; \n  add_recipe(recipe) |&gt; \n  add_model(tune_rf_spec)\n\nset.seed(123)\nrandf_grid &lt;-grid_regular(trees(), min_n(), mtry(range(1:13)))\ndoParallel::registerDoParallel() # running this in parallel\n\ntune_rs &lt;- tune_grid(tune_rf_wf, incidents_folds, grid = randf_grid, control = control_resamples(save_pred = T, parallel_over = 'everything'))\n\nWe then conduct a model fit using your newly tuned model specification and investigate the terms most highly associated with non-fatal, and fatal reports\n\n# extracting our best hyperparameters:\nparams &lt;- tune_rs |&gt; show_best(\"accuracy\") |&gt; slice(1) |&gt; select(trees, mtry, min_n)\nbest_trees_rf &lt;- params$trees\nbest_mtry_rf &lt;- params$mtry\nbest_min_n_rf &lt;- params$min_n\n\n# Final model using our best parameters\nrandf_final &lt;- rand_forest(\n  trees = best_trees_rf,\n  mtry = best_mtry_rf,\n  min_n = best_min_n_rf\n) |&gt;\n  set_mode(\"classification\") |&gt;\n  set_engine(\"ranger\")\n\n\n# fit on the training\nrandf_final_fit &lt;- tune_rf_wf |&gt; \n  update_model(randf_final) |&gt; \n  fit(data = incidents_train)\n\nUnfortunately tidy doesnt support ranger and we are unable to see variable importance/terms most highly associated with different reports\nNow, we’ll predict fatality of the reports in the test set. We can compare this prediction performance to that of the Naive Bayes and Lasso models.\n\n# predict on the test, calculate RMSE\nrf_testing_preds &lt;- predict(randf_final_fit, incidents_test) |&gt; \n  bind_cols(incidents_test) |&gt; \n  mutate(truth = as.factor(fatal), estimate = as.factor(.pred_class)) |&gt; \n  metrics(truth = truth, estimate = estimate)\n\nrf_testing_preds\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.812\n2 kap      binary         0.119\n\n\nUnfortunately, our predictions got worse as we tuned the model. This could be due to bad combinations chosen by the grid space, which is likely since our tuning grid isn’t very big. Our random forest model also performs worse than the lasso model (which scored 92% accuracy), but slightly better than the Naive Bayes model (81% accuracy)"
  },
  {
    "objectID": "Portfolio/texas_aq/index.html",
    "href": "Portfolio/texas_aq/index.html",
    "title": "Texas Vehicle Growth and Air Quality",
    "section": "",
    "text": "After moving back to Dallas after spending a year in Santa Barbara, California, I noticed that traffic had gotten worse. The highways we took to get back to my house were completely backed up with accident after accident. I also noticed that air quality warnings, which used to be uncommon, are now the normal - especially in the hot summer weather. With the knowledge that the DFW metroplex is one of the fastest growing areas in the nation by population, my intuition tells me that the growing population has contributed to worse traffic conditions, leading to people being stuck in their cars longer and therefore deteriorating air quality. This post will be an exploration into this relationship.\n\n\nDallas is a sprawling city with a car-dependent infrastructure. Cities like it see high levels of vehicle emissions, which inherently affect air quality."
  },
  {
    "objectID": "Portfolio/texas_aq/index.html#introduction",
    "href": "Portfolio/texas_aq/index.html#introduction",
    "title": "Texas Vehicle Growth and Air Quality",
    "section": "",
    "text": "Dallas is a sprawling city with a car-dependent infrastructure. Cities like it see high levels of vehicle emissions, which inherently affect air quality."
  }
]