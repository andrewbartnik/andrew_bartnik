{
  "hash": "57c97f0f66d2ce3fbefdf5a616a44a2f",
  "result": {
    "markdown": "---\ntitle: \"Examining Word Frequencies in NYT Biodiversity articles\"\ndescription: \"This post uses the NYT API to access biodiversity-related articles to perform some transformations and analyze word frequencies\"\nauthor:\n  - name: Andrew Bartnik\n    url: https://andrewbartnik.github.io/\n    affiliation: Master of Environmental Data Science Program @ The Bren School (UCSB)\n    affiliation-url: https://ucsb-meds.github.io/ \ndate: 07-31-2023\ncategories: [NLP, R] # self-defined categories\nimage: bd.jpeg\ncitation: \n  url: https://andrewbartnik.github.io/Portfolio/ocean-modeling\ndraft: true # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\nformat: \n  html: \n    toc: true\neditor: visual\nengine: knitr\ncode-fold: false\n---\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create the query url\nurl <- paste(\"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=haaland&api-key=\",API_KEY, sep =\"\")\n#send the request, receive the response, and flatten\nt <- fromJSON(url, flatten = T)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nterm2 <- \"biodiversity\" # Need to use $ to string  together separate terms\nbegin_date2 <- \"20210120\"\nend_date2 <- \"20230401\"\n#construct the query url using API operators\nbaseurl2 <- paste0(\"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=\",term2,\"&begin_date=\",begin_date2,\"&end_date=\",end_date2,\"&facet_filter=true&api-key=\",\"NTKBHbsb6XFEkGymGumAiba7n3uBvs8V\", sep=\"\")\n#examine our query url\nbaseurl2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=biodiversity&begin_date=20210120&end_date=20230401&facet_filter=true&api-key=NTKBHbsb6XFEkGymGumAiba7n3uBvs8V\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#dig into the JSON object to find total hits\ninitialQuery2 <- fromJSON(baseurl2)\nmaxPages <- round((initialQuery2$response$meta$hits[1] / 20)-1) \n#initiate a list to hold results of our for loop\npages2 <- list()\n#loop\nfor(i in 0:maxPages){\n  nytSearch2 <- fromJSON(paste0(baseurl2, \"&page=\", i), flatten = TRUE) %>% data.frame() \n  message(\"Retrieving page \", i)\n  pages2[[i+1]] <- nytSearch2 \n  Sys.sleep(60) \n  }\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 0\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 4\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 5\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 6\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 7\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 8\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 9\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 10\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 11\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 12\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 13\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 14\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 15\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 16\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 17\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 18\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 19\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 20\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 21\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 22\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRetrieving page 23\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#need to bind the pages and create a tibble from nytDat\ndf2 <- rbind.fill(pages2)\nsaveRDS(df2, \"nyt_df2.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnytDat2 <- readRDS(\"nyt_df2.rds\")\ndim(nytDat2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 240  33\n```\n:::\n\n```{.r .cell-code}\ndf2 <- nytDat2\n```\n:::\n\n\n\n\n\n\nWe will now recreate the publications per day and word frequency plots using the first paragraph. This time we filter on the response.docs.news_desk variable to winnow out irrelevant results. We will then perform some standard transformations - filtering out numbers etc. \n\n\n\n\n## Publications per day\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparagraph2 <- names(nytDat2)[6] \ntokenized2 <- nytDat2 %>%\nunnest_tokens(word, paragraph2) \n\n# Remove numbers\nclean_tokens2 <- str_remove_all(tokenized2$word, \"[:digit:]\")\n\n# Remove apostrophes \nclean_tokens2 <- gsub(\"’s\", '', clean_tokens2)\ntokenized2$clean <- clean_tokens2\n\n#remove the empty strings\ntib2 <-subset(tokenized2, clean!=\"\")\n#reassign\ntokenized2 <- tib2\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nppd <- tokenized2 %>%\n  mutate(pubDay=gsub(\"T.*\",\"\",response.docs.pub_date)) %>%\n  filter(response.docs.news_desk %in% c(\"Climate\", \"Science\")) |> \n  group_by(pubDay) %>%\n  summarise(count=n()) %>%\n  filter(count >= 50) %>%\n  ggplot() +\n  geom_bar(aes(x=reorder(pubDay, count), y=count), stat=\"identity\") +\n  coord_flip()\nppd\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n## Word frequency\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(stop_words)\nstop_words\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,149 × 2\n   word        lexicon\n   <chr>       <chr>  \n 1 a           SMART  \n 2 a's         SMART  \n 3 able        SMART  \n 4 about       SMART  \n 5 above       SMART  \n 6 according   SMART  \n 7 accordingly SMART  \n 8 across      SMART  \n 9 actually    SMART  \n10 after       SMART  \n# … with 1,139 more rows\n```\n:::\n\n```{.r .cell-code}\ntokenized2 <- tokenized2 %>%\n  anti_join(stop_words)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(word)`\n```\n:::\n\n```{.r .cell-code}\nwf <- tokenized2 %>%\n  count(clean, sort = TRUE) %>%\n  filter(n > 10) %>%\n  mutate(clean = reorder(clean, n)) %>%\n  ggplot(aes(n, clean)) +\n  geom_col() +\n  labs(y = NULL)\n\nwf\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\nNow, we'll recreate the publications per day and word frequency plots using the headlines variable (response.docs.headline.main) and examine the differences between the words appearing in the titles and the bodies of the text. \n\n# Publications per day\n\n::: {.cell}\n\n```{.r .cell-code}\nheadline <- names(nytDat2)[20] \ntoken_headlines <- nytDat2 %>%\nunnest_tokens(word, headline) \nhead(token_headlines[,\"word\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"why\"     \"tiny\"    \"ponds\"   \"and\"     \"singing\" \"frogs\"  \n```\n:::\n\n```{.r .cell-code}\nhead(token_headlines$word)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"why\"     \"tiny\"    \"ponds\"   \"and\"     \"singing\" \"frogs\"  \n```\n:::\n\n```{.r .cell-code}\n#remove the empty strings\ntib3 <-subset(token_headlines, word!=\"\")\n#reassign\ntoken_headlines <- tib3\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(stop_words)\nstop_words\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,149 × 2\n   word        lexicon\n   <chr>       <chr>  \n 1 a           SMART  \n 2 a's         SMART  \n 3 able        SMART  \n 4 about       SMART  \n 5 above       SMART  \n 6 according   SMART  \n 7 accordingly SMART  \n 8 across      SMART  \n 9 actually    SMART  \n10 after       SMART  \n# … with 1,139 more rows\n```\n:::\n\n```{.r .cell-code}\ntoken_headlines <- token_headlines %>%\n  anti_join(stop_words)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(word)`\n```\n:::\n\n```{.r .cell-code}\nppd2 <- token_headlines %>%\n  mutate(pubDay=gsub(\"T.*\",\"\",response.docs.pub_date)) %>%\n  filter(response.docs.news_desk %in% c(\"Climate\", \"Science\")) |> \n  group_by(pubDay) %>%\n  summarise(count=n()) %>%\n  filter(count >= 8) %>%\n  ggplot() +\n  geom_bar(aes(x=reorder(pubDay, count), y=count), stat=\"identity\") +\n  coord_flip()\n```\n:::\n\n\n# Word frequency\n\n::: {.cell}\n\n```{.r .cell-code}\n#reassign\ntoken_headlines <- tib3\nwf2 <- token_headlines %>%\n  count(word, sort = TRUE) %>%\n  filter(n > 5) %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngridExtra::grid.arrange(wf, wf2, ncol=2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n**The word frequencies in the headlines and first paragraphs within NYT \"biodiversity\" articles are very similar. Virtually all of the words that appear in the headlines also appear in the first paragraph.**\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}