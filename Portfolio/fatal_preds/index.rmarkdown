---
title: "Predicting Climbing Incident Outcomes from Article Text"
description: "I used NLP techniques and different ML models to predict the outcome of climbing accidents. This assignment was part of EDS 231 - Text and Sentiment Analysis for Environmental Science"
author:
  - name: Andrew Bartnik
    url: https://andrewbartnik.github.io/
    affiliation: Master of Environmental Data Science Program @ The Bren School (UCSB)
    affiliation-url: https://ucsb-meds.github.io/ 
date: 05-25-2023
categories: [NLP, R, Assignments, ML] # self-defined categories
image: mtn.jpeg
draft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!
format: 
  html: 
    code-fold: false
    toc: true
editor: visual
engine: knitr
---

```{r packages, include = FALSE}
library(tidytext)
library(tidyverse)
library(tidymodels)
library(textrecipes)
library(discrim) # naive-bayes
library(vip)
library(widyr)
library(irlba) 
library(broom) 
library(textdata)
library(ggplot2)
library(LexisNexisTools)
```


# Preprocessing

Here, we are going to predict the outcome of climbing incidents (fatal or nonfatal) using text from climbing incident articles.
This data set includes more possible predictors than the text alone, but for this model we will only use the text variable.


```{r data, warning=FALSE, message=FALSE}
urlfile ="https://raw.githubusercontent.com/MaRo406/EDS-231-text-sentiment/main/data/climbing_reports_model_dat.csv"
incidents_df<-readr::read_csv(url(urlfile))
head(incidents_df)
```


We'll split our data into training and test portions


```{r split-data, warning=FALSE, message=FALSE}
set.seed(1234)

# Adding a fatality binary indicator
incidents2class <- incidents_df |> 
  mutate(fatal = factor(if_else(
    is.na(Deadly) ,
    "non-fatal", "fatal")))

# Splitting our data, stratifying on the outcome
incidents_split <- initial_split(incidents2class, strata = fatal)

# Now for making our training and testing sets:
incidents_train <- training(incidents_split)
incidents_test <- testing(incidents_split)

```


We use recipe() from tidymodels to specify the predictor and outcome variables and the data.


```{r recipe, message=FALSE, warning=FALSE}
# Making the recipe. We're saying we want to predict fatal by using Text
incidents_rec <- recipe(fatal ~ Text, data = incidents_train)
```


Next we add some familiar pre-processing steps on our Text variable. We first tokenize the articles to word level, filter to the most common words, and calculate tf-idf.


```{r pre-process, message=FALSE, warning=FALSE}
# Adding steps to our recipe
recipe <- incidents_rec %>%
  step_tokenize(Text) %>% 
  step_tokenfilter(Text, max_tokens = 1000) %>%
  step_tfidf(Text) #new one from textrecipes
```


# Modeling

We use tidymodels workflow to combine the modeling components. There are several advantages of doing this: We don't have to keep track of separate objects in our workspace. The recipe prepping and model fitting can be executed using a single call to fit() . If we have custom tuning parameter settings, these can be defined using a simpler interface when combined with tune.

## Naive Bayes

```{r workflow, message=FALSE, warning=FALSE}
# Making the workflow object
incidents_wf <- workflow() %>%
  add_recipe(recipe)
```


We want to use Naive Bayes to classify the outcomes. 

```{r nb-spec, message=FALSE, warning=FALSE}
nb_spec <- naive_Bayes() %>%
  set_mode("classification") %>% #set modeling context
  set_engine("naivebayes") #method for fitting model

nb_spec
```


Now we are ready to add our model to the workflow and fit it to the training data


```{r fit-model, message=FALSE, warning=FALSE}
# Fitting the model to our workflow
nb_fit <- incidents_wf %>%
  add_model(nb_spec) %>%
  fit(data = incidents_train)
```


Next up is model evaluation. We'll stretch our training data further and use resampling to evaluate our Naive Bayes model. Here we create 10-fold cross-validation sets, and use them to estimate performance.


```{r, message=FALSE, warning=FALSE}
# Set the seed for reproducibility
set.seed(234)
incidents_folds <- vfold_cv(incidents_train) #default is v = 10

incidents_folds
```

```{r nb-workflow, message=FALSE, warning=FALSE}
nb_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(nb_spec)

nb_wf
```


To estimate its performance, we fit the model many times, once to each of these resampled folds, and then evaluate on the holdout part of each resampled fold.


```{r fit-resamples, message=FALSE, warning=FALSE}
nb_rs <- fit_resamples(
  nb_wf,
  incidents_folds,
  control = control_resamples(save_pred = TRUE)
)
```


We then extract the relevant information using `collect_metrics()` and `collect_predictions()` and examine the performance metrics.


```{r, message=FALSE, warning=FALSE}
nb_rs_metrics <- collect_metrics(nb_rs)
nb_rs_predictions <- collect_predictions(nb_rs)
nb_rs_metrics
```


We'll use two performance metrics: accuracy and ROC AUC. Accuracy is the proportion of the data that is predicted correctly. The ROC curve plots the true positive rate against the false positive rate; AUC closer to 1 indicates a better-performing model, while AUC closer to 0.5 indicates a model that does no better than random guessing.


```{r performance-plot, message=FALSE, warning=FALSE}
nb_rs_predictions %>%
  group_by(id) %>%
  roc_curve(truth = fatal, .pred_fatal) %>%
  autoplot() +
  labs(
    "Resamples",
    title = "ROC curve for Climbing Incident Reports"
  )
```


Another model method involves the confusion matrix. A confusion matrix tabulates a model's false positives and false negatives for each class.


```{r confusion-matrix, message=FALSE, warning=FALSE}
conf_mat_resampled(nb_rs, tidy = FALSE) %>% #compute matrix for each fold then average
  autoplot(type = "heatmap")
```

## Lasso Regression

Now, lets try a different model - lasso regression to see if we can score better. We follow the same approach that we did for the Naive Bayes model. 

```{r, message=FALSE, warning=FALSE}
# Specifying the model
lasso_spec <- logistic_reg(penalty = 0.01, mixture =1) |> 
  set_mode('classification') |> 
  set_engine("glmnet")

# Making the workflow  
lasso_wf <- workflow() |> 
  add_recipe(recipe) |> 
  add_model(lasso_spec)

lasso_wf
```

We then fit the model to the resamples, and collect our metrics

```{r, message=FALSE, warning=FALSE}
set.seed(123)
lasso_rs <- fit_resamples(
  lasso_wf,
  incidents_folds,
  control = control_resamples(save_pred = T)
)

lasso_rs_metrics <- collect_metrics(lasso_rs)
lasso_rs_predictions <- collect_predictions(lasso_rs)
lasso_rs_metrics
```


Now we can check our predictions:


```{r, message=FALSE, warning=FALSE}
lasso_rs_predictions |> 
  group_by(id) |> 
  roc_curve(truth = fatal, .pred_fatal) |> 
  autoplot() + labs(color = "Resamples",
                    title = "ROC for Climbing Incident Reports")

```


## Logistic Regression

Now for Logistic Regression: 


```{r, message=FALSE, warning=FALSE}
log_spec <- logistic_reg(penalty = tune(), mixture = 1) |> set_mode("classification") |> set_engine("glmnet")
log_wf <- workflow() |> 
  add_recipe(recipe) |> 
  add_model(log_spec)

set.seed(123)
lambda_grid <- grid_regular(penalty(), levels = 30)
log_rs <- tune_grid(log_wf, incidents_folds, grid = lambda_grid, control = control_resamples(save_pred = T))
```


Lets evaluate our Logistic Regression model:


```{r, message=FALSE, warning=FALSE}
# Collecting metrics
collect_metrics(log_rs)
autoplot(log_rs) + labs(title = "Lasso Performance Across Regular Penalties")
```

```{r, message=FALSE, warning=FALSE}
log_rs |> show_best("roc_auc")
log_rs |> show_best("accuracy")
```

Using our best model, we perform a final fit and evaluate its performance:

```{r, message=FALSE, warning=FALSE}
# Finalizing the workflow. extracting the metrics
chosen_acc <- log_rs |> select_by_one_std_err(metric= "accuracy", -penalty)
final_log <- finalize_workflow(log_wf, chosen_acc)
final_log <- fit(final_log, incidents_train)
final_log |> extract_fit_parsnip() |> tidy() |> arrange(estimate)
last_fit(final_log, incidents_split) |> collect_metrics()
```



## Random Forest Classifier

Now we're going to try a more powerful algorithm: the random forest classifier.

```{r, message=FALSE, warning=FALSE}
# Specifying the model
rf_spec <- rand_forest() |> set_mode("classification") |> set_engine("ranger")
```


We're first going to conduct an initial out-of-the-box model fit on the training data and prediction on the test test data.  Assess the performance of this initial model.

```{r, message=FALSE, warning=FALSE}

# Initializing a workflow, fitting it to resamples
rf_workflow <- workflow() |> add_recipe(recipe) |> add_model(rf_spec)
rf_fit <- rf_workflow |> fit(data = incidents_train)

rf_rs <- fit_resamples(
  rf_workflow,
  incidents_folds,
  control = control_resamples(save_pred = TRUE)
)
```

```{r, message=FALSE, warning=FALSE}
rf_rs_metrics <- collect_metrics(rf_rs)
nb_rs_predictions <- collect_predictions(rf_rs)
rf_rs_metrics
```

**Initial model has an accuracy of 86% and roc_auc of 0.95. Not bad at all.**

Now, we're going to tune the hyperparameters:


```{r, message=FALSE, warning=FALSE}
# specifying which parameters to tune, adding this new model back into a workflow
tune_rf_spec <- rand_forest(trees = tune(), mtry = tune(), min_n = tune()) |> set_mode("classification") |> set_engine("ranger")
tune_rf_wf <- workflow() |> 
  add_recipe(recipe) |> 
  add_model(tune_rf_spec)

set.seed(123)
randf_grid <-grid_regular(trees(), min_n(), mtry(range(1:13)))
doParallel::registerDoParallel() # running this in parallel

tune_rs <- tune_grid(tune_rf_wf, incidents_folds, grid = randf_grid, control = control_resamples(save_pred = T, parallel_over = 'everything'))
```



We then conduct a model fit using your newly tuned model specification and investigate the terms most highly associated with non-fatal, and fatal reports

```{r, message=FALSE, warning=FALSE}

# extracting our best hyperparameters:
params <- tune_rs |> show_best("accuracy") |> slice(1) |> select(trees, mtry, min_n)
best_trees_rf <- params$trees
best_mtry_rf <- params$mtry
best_min_n_rf <- params$min_n

# Final model using our best parameters
randf_final <- rand_forest(
  trees = best_trees_rf,
  mtry = best_mtry_rf,
  min_n = best_min_n_rf
) |>
  set_mode("classification") |>
  set_engine("ranger")


# fit on the training
randf_final_fit <- tune_rf_wf |> 
  update_model(randf_final) |> 
  fit(data = incidents_train)
```

**Unfortunately tidy doesnt support ranger and we are unable to see variable importance/terms most highly associated with different reports**



Now, we'll predict fatality of the reports in the test set.  We can compare this prediction performance to that of the Naive Bayes and Lasso models.  


```{r, message=FALSE, warning=FALSE}
# predict on the test, calculate RMSE
rf_testing_preds <- predict(randf_final_fit, incidents_test) |> 
  bind_cols(incidents_test) |> 
  mutate(truth = as.factor(fatal), estimate = as.factor(.pred_class)) |> 
  metrics(truth = truth, estimate = estimate)

rf_testing_preds

```


**Unfortunately, our predictions got worse as we tuned the model. This could be due to bad combinations chosen by the grid space, which is likely since our tuning grid isn't very big. Our random forest model also performs worse than the lasso model (which scored 92% accuracy), but slightly better than the Naive Bayes model (81% accuracy)**


# Embeddings
First, let's calculate the unigram probabilities -- how often we see each word in this corpus.


```{r unigrams, message=FALSE, warning=FALSE}
unigram_probs <- incidents_df |> unnest_tokens(word, Text) |> anti_join(stop_words, by = 'word') |> count(word, sort=T) |> mutate(p = n/sum(n))
unigram_probs
```


So now we have the probability of each word.

Next, we need to know how often we find each word near each other word -- the skipgram probabilities. In this case we'll define the word context as a five-word window. We'll slide that window across all of our text and record which words occur together within that window.
Let's write some code that adds an `ngramID` column that contains constituent information about each 5-gram we constructed by sliding our window.


```{r make-skipgrams, warning=FALSE, message=FALSE}
skipgrams <- incidents_df |> unnest_tokens(ngram, Text, token = "ngrams", n = 5) |> mutate(ngramID = row_number()) |> tidyr::unite(skipgramID, ID, ngramID) |> unnest_tokens(word, ngram) |> anti_join(stop_words, by = 'word')
```


Now we use widyr::pairwise_count() to sum the total \# of occurrences of each pair of words.


```{r pairwise_count, warning=FALSE, message=FALSE}
#calculate probabilities
skipgram_probs <- skipgrams |> pairwise_count(word, skipgramID,diag = T, sort = T) |> mutate(p = n/sum(n))
```


The next step is to normalize these probabilities, that is, to calculate how often words occur together within a window, relative to their total occurrences in the data.


```{r norm-prob, warning=FALSE, message=FALSE}
normalized_prob <- skipgram_probs |> filter(n>20) |> rename(word1 = item1, word2=item2) |> left_join(unigram_probs |> select(word1 = word, p1 = p), by = 'word1') |> 
  left_join(unigram_probs |> select(word2 = word, p2 = p), by = 'word2') |> mutate(p_together = p/p1/p2)

normalized_prob
```


Now we have all the pieces to calculate the point-wise mutual information (PMI) measure. It's the logarithm of the normalized probability of finding two words together. PMI tells us which words occur together more often than expected based on how often they occurred on their own.

Then we convert to a matrix so we can use matrix factorization and reduce the dimensionality of the data.


```{r pmi, warning=FALSE, message=FALSE}
pmi_matrix <- normalized_prob |> mutate(pmi = log10(p_together)) |> cast_sparse(word1, word2, pmi)
```


We do the singluar value decomposition with irlba::irlba(). It's a "partial decomposition" as we are specifying a limited number of dimensions, in this case 100.


```{r pmi2, warning=FALSE, message=FALSE}
pmi_matrix@x[is.na(pmi_matrix@x)]<0
pmi_svd <- irlba(pmi_matrix, 100, maxit = 500)
word_vectors <- pmi_svd$u

rownames(word_vectors) <- rownames(pmi_matrix)
```


These vectors in the "u" matrix are contain "left singular values". They are orthogonal vectors that create a 100-dimensional semantic space where we can locate each word. The distance between words in this space gives an estimate of their semantic similarity.

Here's a function written by Julia Silge for matching the most similar vectors to a given vector.


```{r syn-function, warning=FALSE, message=FALSE}
search_synonyms <- function(word_vectors, selected_vector) {
dat <- word_vectors %*% selected_vector
    
similarities <- dat %>%
        tibble(token = rownames(dat), similarity = dat[,1])

similarities %>%
       arrange(-similarity) %>%
        select(c(2,3))
}
```


Let's test it out!


```{r find-synonyms, warning=FALSE, message=FALSE}
fall <- search_synonyms(word_vectors = word_vectors, word_vectors['fall',])
fall

slip <- search_synonyms(word_vectors = word_vectors, word_vectors['slip',])
slip

ice <- search_synonyms(word_vectors = word_vectors, word_vectors['ice',])
ice
```


Here's a plot for visualizing the most similar words to a given target word.


```{r plot-synonyms, warning=FALSE, message=FALSE}
slip %>%
    mutate(selected = "slip") %>%
    bind_rows(fall %>%
                  mutate(selected = "fall")) %>%
    group_by(selected) %>%
    top_n(15, similarity) %>%
    ungroup %>%
    mutate(token = reorder(token, similarity)) %>%
    ggplot(aes(token, similarity, fill = selected)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~selected, scales = "free") +
    coord_flip() +
    theme(strip.text=element_text(hjust=0, size=12)) +
    scale_y_continuous(expand = c(0,0)) +
    labs(x = NULL, title = "What word vectors are most similar to slip or fall?")
  
```


One of the cool things about representing words as numerical vectors is that we can use math on those numbers that has some semantic meaning.


```{r word-math, warning=FALSE, message=FALSE}
snow_danger <- word_vectors["snow",] + word_vectors["danger",]
search_synonyms(word_vectors, snow_danger)

no_snow_danger <- word_vectors["danger",] - word_vectors["snow",]
search_synonyms(word_vectors, no_snow_danger)
```

