---
title: "Biodiversity Sentiment and Topic Analysis"
description: "I used the Nexis Uni database to analyze the sentiment associated with Biodiversity. This assignment was part of EDS 231 - Text and Sentiment Analysis for Environmental Science"
author:
  - name: Andrew Bartnik
    url: https://andrewbartnik.github.io/
    affiliation: Master of Environmental Data Science Program @ The Bren School (UCSB)
    affiliation-url: https://ucsb-meds.github.io/ 
date: 08-01-2023
categories: [NLP, R, Assignments, ML] # self-defined categories
image: bd.jpeg
draft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!
format: 
  html: 
    code-fold: false
    toc: true
editor: visual
engine: knitr
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(LexisNexisTools)
library(dplyr)
library(readr)
library(stringr)
library(here)
library(tidytext)
library(tidyr) #pivot_wider()
library(ggplot2)
library(quanteda)
library(topicmodels)
library(ldatuning)
library(tidyverse)
library(reshape2)
```

## Sentiment Analysis
I used the Nexis Uni database to evaluate the sentiment surrounding biodiversity articles, and then analyzed the topics that articles fall under.

I first accessed the Nexis Uni database through the UCSB library. I then chose to search for articles related to biodiversity and downloaded a batch of 100 .docx files.

<https://github.com/JBGruber/LexisNexisTools/wiki/Downloading-Files-From-Nexis>

```{r, warning=FALSE, message=FALSE}

# Reading in files
pre_files <- list.files(pattern = ".docx", 
                        path = "/Users/andrewbartnik/Desktop/misc/MEDS/Spring/text/text_analysis/data/lab2/files2",
                       full.names = TRUE, 
                       recursive = TRUE, 
                       ignore.case = TRUE)

# Saving objects
pre_dat <- lnt_read(pre_files)
bing_sent <- get_sentiments('bing')
nrc_sent <- get_sentiments("nrc")


```

I cleaned artifacts of the data collection process (date strings, etc), and saved the metadata, article title, and paragraph contents to their own objects - adding this all together into a dataframe.

```{r, warning=FALSE, message=FALSE}

meta <- pre_dat@meta
articles <- pre_dat@articles
paragraphs <- pre_dat@paragraphs

data <- tibble(Date = meta$Date, Headline = meta$Headline, id = pre_dat@articles$ID, text = pre_dat@articles$Article)

head(data)
```

Exploring the data a bit!

```{r, warning=FALSE, message=FALSE}
# date freq
date_freq <- data %>%
  group_by(Date) %>%
  summarise(freq = n())

ggplot(date_freq, aes(x = Date, y = freq)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  theme_minimal() +
  labs(x = "Date", y = "Frequency", title = "Frequency of Dates 2022-2023") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

Here, I'm unnesting each word in the article and joining them to their bing sentiment scores.

```{r, warning=FALSE, message=FALSE}
# Extract words
text <- data |> unnest_tokens(output = word, input = text, token = 'words')

# join to sent 
sent_words <- text |> 
  anti_join(stop_words, by = "word") |> 
  inner_join(bing_sent, by = 'word') |> 
  mutate(sent_num = case_when(sentiment == "negative" ~ -1, sentiment == "positive" ~ 1))
```

We can calculate the average sentiment for each article

```{r, warning=FALSE, message=FALSE}
sent_article2 <-sent_words |> 
  count(id, sentiment) |> 
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |> 
  mutate(polarity = positive - negative) 
mean(sent_article2$polarity)

```

Now we can look at the distribution of sentiments across all the articles:

```{r, warning=FALSE, message=FALSE}
ggplot(sent_article2, aes(x = id)) + 
  theme_classic() + 
  geom_col(aes(y = positive), stat = 'identity', fill = 'lightblue') + 
  geom_col(aes(y = negative), stat = 'identity', fill = 'red', alpha = 0.5) + 
  labs(title = 'Sentiment analysis: Biodiversity', y = "Sentiment Score")
```

And for the fun part - after we filter out stop words, we can join our words to the `nrc_sent` object which shows the associated sentiment for each word:

```{r, warning=FALSE, message=FALSE}
nrc_word_counts_bio <- text |> anti_join(stop_words, by = "word") |> inner_join(nrc_sent) |> count(word, sentiment, sort = T) 
```

```{r, warning=FALSE, message=FALSE}
# Now to look at specific nrc sentiments
sent_counts2 <- text |> 
  anti_join(stop_words, by = 'word') |> 
  group_by(id) |> 
  inner_join(nrc_sent) |> 
  group_by(sentiment) |> 
  count(word, sentiment, sort = T)

# Evaluating contribution to sentiment
sent_counts2 |> group_by(sentiment) |> slice_max(n, n = 10) |> ungroup() |> mutate(word = reorder(word, n)) |> ggplot(aes(x=n, y=word, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales= "free_y") + labs(x = "Contribution to sentiment", y = NULL)
```

**"Loss" seems to be associated with strongly negative emotions. Conservation also seems to elicit a strong emotional response.**

**Soil, wind, and diverse are associated with more negative emotions, which is misleading. Since these terms are pretty neutral in this context, we can reclassify their associated sentiments.**

```{r, warning=FALSE, message=FALSE}
## Reclassifying
sent_counts2 |> filter(!word %in% c("soil", "wind", "diverse")) |> group_by(sentiment) |> slice_max(n, n = 10) |> ungroup() |> mutate(word = reorder(word, n)) |> ggplot(aes(x=n, y=word, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales= "free_y") + labs(x = "Contribution to sentiment", y = NULL)
```

**Thats better, harm and crisis are more appropriately associated with negative sentiment than soil and wind**

Now we can plot the amount of nrc emotion words as a percentage of all the emotion words used each day. Then we can analyze the distribution of emotion words change over time.

```{r, warning=FALSE, message=FALSE}

nrc_emotion_counts <- text %>%
  inner_join(nrc_sent) %>%
  count(Date, sentiment)

# Aggregate the text from articles published on the same day
total_emotions_by_day <- nrc_emotion_counts %>%
  group_by(Date) %>%
  summarise(total = sum(n))

# Calculate the percentage of NRC emotion words per day
nrc_emotion_percentage <- nrc_emotion_counts %>%
  left_join(total_emotions_by_day, by = "Date") %>%
  mutate(percentage = n / total * 100)

# Plot the distribution of emotion words over time
ggplot(nrc_emotion_percentage, aes(x = Date, y = percentage, color = sentiment)) +
  geom_line() +
  theme_minimal() +
  labs(x = "Date", y = "Percentage of Emotion Words", title = "Distribution of Emotion Words Over Time") +
  theme(legend.title = element_blank(), legend.position = "bottom", legend.box = "horizontal")

```

**The sentiment around the biodiversity term is overwhelmingly positive over the given time period. Trust was the second most frequent sentiment. This could be because most of the articles I downloaded were related to conservation efforts and achievements. The only time negative sentiment surpasses positive sentiment was at the end of February, when the only article published within a 6 day period was titled "Majorda locals object to alleged destruction of biodiversity, natural flow of water by RVNL"**

## Topic Analysis

Now for topic analysis. We'll first load the data and clean it as appropriate:

```{r, message=FALSE, warning=FALSE}
# Making a corpus of the articles
corpus_bio <- corpus(x = articles, text_field = "Article")
stories_stats <- summary(corpus_bio)
head(stories_stats)
```


```{r, message=FALSE, warning=FALSE}
toks2 <- tokens(corpus_bio, remove_punct = T, remove_numbers = T)
add_stops <- stopwords("en")
toks3 <- tokens_select(toks2, pattern = add_stops, selection = "remove")

dfm_bio <- dfm(toks3, tolower = T)
dfm <- dfm_trim(dfm_bio, min_docfreq = 2)

head(dfm)

sel_idx <- slam::row_sums(dfm)>0
dfm <- dfm[sel_idx,]
```


3.  Run three models (i.e. with 3 values of k) and select the overall best value for k (the number of topics) - include some justification for your selection: theory, FindTopicsNumber() optimization metrics, interpretability, LDAvis. select the best single value for k.

### K = 10
```{r, message=FALSE, warning=FALSE}
k <- 10

topicModel_k10 <- LDA(dfm, 
                     k, 
                     method= "Gibbs", 
                     control = list(iter = 500,
                             verbose = 25))

result <- posterior(topicModel_k10)
attributes(result)

beta <- result$terms
theta <- result$topics
vocab <- colnames(beta)

dim(beta)
dim(theta)
terms(topicModel_k10, 10)
```

```{r, message=FALSE, warning=FALSE}
result <- FindTopicsNumber(dfm, 
                           topics = seq(from = 2, to = 20, by = 1), metrics = c("CaoJuan2009", "Deveaud2014"),
                           method = "Gibbs",
                           verbose = T)


FindTopicsNumber_plot(result)
```
### K = 5
```{r, message=FALSE, warning=FALSE}
k <- 5

topicModel_k5 <- LDA(dfm, 
                     k, 
                     method= "Gibbs", 
                     control = list(iter = 500,
                             verbose = 25))

result <- posterior(topicModel_k10)
attributes(result)

beta <- result$terms
theta <- result$topics
vocab <- colnames(beta)

dim(beta)
dim(theta)
terms(topicModel_k5, 10)
```
### K = 7
```{r, message=FALSE, warning=FALSE}
k <- 16

topicModel_k7 <- LDA(dfm, 
                     k, 
                     method= "Gibbs", 
                     control = list(iter = 500,
                             verbose = 25))

result <- posterior(topicModel_k10)
attributes(result)

beta <- result$terms
theta <- result$topics
vocab <- colnames(beta)

dim(beta)
dim(theta)
terms(topicModel_k7, 10)
```

**Although the Findtopicsnumber() optimization metrics didn't suggest a consistent value for K, I decided to go with k =5 for interpretability. Running more topics resulted in more low-value words and worse interpretability between topics. ** 

4. Plot the top terms in each topic and the distribution of topics across a sample of the documents (constrained by what looks good in the plot).
```{r, message=FALSE, warning=FALSE}
bio_topics <- tidy(topicModel_k5, matrix = "beta")

top_terms <- bio_topics |> group_by(topic) |> top_n(10, beta) |> ungroup() |> arrange(topic, -beta)
top_terms %>%
  mutate(term = reorder_within(term, beta, topic, sep = "")) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free_y") +
  scale_x_reordered()+
  coord_flip()
```

```{r, message=FALSE, warning=FALSE}
topic_words <- terms(topicModel_k10, 5)
topic_names <- apply(topic_words, 2, paste, collapse = "")
```

```{r, message=FALSE, warning=FALSE}
example_ids <- c(5:10)
n <- length(example_ids)
example_props <- theta[example_ids,]
colnames(example_props) <- topic_names
viz_df <- melt(cbind(data.frame(example_props),
                     document = factor(1:n),
                     variable.name = 'topic',
                     id.vars = 'document'))
viz_df
ggplot(data = viz_df, aes(variable, value, fill = document), ylab = "proportion") +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  coord_flip() +
  facet_wrap(~ document, ncol = n)
```


5.  Take a stab at interpreting the resulting topics. What are the key themes discussed in the articles in your data base?
**Based off the top terms in each topic, topic 1 seems to be most closely associated with different levels of government and their efforts to work on biodiversity projects. Topic 2 seems most closely associated with management and conservation, topic 3 seems to be associated with the theme of international climate change, topic 4 looks to be associated with risk and impact assessment for companies, and topic 5 seems to be associated with the theme of climate change's effect on natural ecosystems. K = 5 seems to have been a reasonable choice. **


